# PIP-322: Pulsar Rate Limiting Refactoring

# Motivation

The current rate limiting implementation in Apache Pulsar has several
known issues that impact performance and accuracy when operating Pulsar
clusters under load (detailed in [Problems to
Address](#problems-to-address)). This proposal outlines a refactor to
consolidate multiple existing options into a single improved solution.

The refactor aims to resolve numerous user complaints regarding default
Pulsar rate limiting being unusable. In addition, inconsistencies and
thread contention with existing rate limiters cause unpredictable
throttling and added latency.

Overhauling the built-in implementation will improve multi-tenancy,
allow Pulsar to scale to demanding workloads, and address longstanding
issues.

Rate limiters act as a conduit to more extensive capacity management and
Quality of Service (QoS) controls in Pulsar. They are integral to
Pulsar's core multi-tenancy features. This refactoring will pave the way
for future enhancements.

# Background Reading

To understand what we're trying to achieve, you should first read the
blog post "Proposal for Pulsar Rate Limiting Enhancements"
[here](https://codingthestreams.com/pulsar/2023/11/22/pulsar-slos-and-rate-limiting.html#proposal-for-pulsar-rate-limiting-enhancements).
Please also make sure to read the [Pulsar Community Meeting minutes
  2023/11/23](https://lists.apache.org/thread/y1sqpyv37fo0k4bm1ox28wggvkb7pbtw).

# Goals

## In Scope

- Preserve current functionality without breaking changes
- Consolidate the multiple existing rate limiting options into a single,
  configurable rate limiting solution
- Remove the separate “precise” rate limiter

### Problems to Address  

- High CPU load with default rate limiter
- High lock contention that impacts shared Netty IO threads and adds
  latency to unrelated Pulsar topic producers ([Bug] RateLimiter lock
  contention when use precise publish rate limiter #21442.)
- Multiple limiting implementations (default, precise) which
  unnecessarily expose implementation details to users of Pulsar and
  make the code harder to maintain and improve
- Inability to limit throughput consistently when using default rate
  limiter
- Inconsistent behavior across multiple levels of throttling (broker,
  namespace, connection)
- Code maintainability
  - Improve understandability of code

## Out of Scope

- Custom/pluggable rate limiters
- Additional rate limiting features
- Cluster-wide capacity management
- Addressing the shared connection multiplexing problem where throttling
  multiple independent streams multiplexed on the same connection cannot
  be consistently throttled by pausing reads on the server side.

# Current solution

## Rate limiters in Pulsar

In Pulsar, rate limiters are used for a few cases:
  - publisher rate limiting
    - topic level (configured at namespace level or topic level policy)
    - broker level
    - resource group level
  - dispatcher rate limiting
  - subscribe rate limiting
  - namespace bundle unloading rate limiting

For producers ("publishers"), there are addition conditions to throttle, besides the rate limiters:
  - limiting pending publish requests per connection, configured with maxPendingPublishRequestsPerConnection
    in broker configuration
  - limiting memory for publishing messages, configured with maxMessagePublishBufferSizeInMB in broker configuration

### Current publisher rate limiters in Pulsar

Pulsar contains two implementations for publisher rate limiters: "default" and "precise". "precise" is the rate limiter implementation which is used when the broker is configured with preciseTopicPublishRateLimiterEnable=true.

#### Default publisher rate limiter

In this approach, a sub-second scheduler runs (configured with brokerPublisherThrottlingTickTimeMillis, defaults to 50ms), iterating every topic in the broker and checking if the topic has exceeded its threshold. If so, it will toggle the autoread state of the connection for the client's producer. Concurrently, a separate one-second scheduler resets the counters and re-enables throttled connections. This method results in inaccurate rate limiting. Additionally, this approach can result in increased CPU usage due to the operation of two schedulers which are constantly iterating all topics and toggling autoread states.

#### Precise publisher rate limiter

In this approach, the rate limit check is done on every send messages request and thus the rate limiting is enforced more accurately. This fixes the main issues of the default rate limiters. However, it introduces a lock contention problem since the rate limiter implementation extensively uses synchronous methods. Since this lock content happens on Netty IO threads, it impacts also unrelated 
topics on the same broker and causes unnecessary slowdowns as reported by bug #21442.

### Publisher throttling approach

In the Pulsar binary protocol, the only way to signal backpressure to the client from the broker is to pause reads and let the buffers to fill up. There is no explicit protocol level permit based flow control such that there exists for consumers. 

When the broker throttles a producer, it will need to pause reading on the connection that the client's producer is using. This is handled by setting the Netty channel's autoread state to false.

# High Level Design

The proposed refactor will overhaul rate limiting internals while
preserving existing APIs and user-facing behavior. A token bucket
algorithm will provide efficient and accurate calculations to throttle
throughput. 

Multiple built-in options such as "precise" rate limiter will be
consolidated under a single solution. Performance issues caused by
contention and CPU overhead will be addressed.

# Detailed Design

## Proposed Solution

- Use asynchronous token bucket algorithm
  - Efficient for concurrent updates
    - Can be implemented in a lockless, non-blocking way using CAS operations on
    volatile fields. 
  - Conceptually simple
  - Token bucket algorithms are a common industry practice for handling traffic shaping
- Unify rate limiting implementations
  - Consolidate the multiple built-in rate limiting options like
    "default" and "precise" into a single unified implementation.  
- Improve understandability, code quality and maintainability. 
- Use more non-blocking asynchronous code and get rid of synchronized
  methods which are causing issues such as #21442.
- Fix the inconsistency of multiple levels of throttling by coordinating
  toggling of the Netty autoread state. Use a reference count type of
  pattern to solve this.

### Preserve Public Contracts

- Avoid breaking existing configs, APIs or client functionality.
- Handle through internal refactoring.  

## Publisher rate limiter throttling

The refactored design ensures that a publish rate limiter operates in a
non-blocking fashion. It consumes the quota seamlessly without
disrupting ongoing operations. When the quota is exhausted, the
connection is throttled by pausing reads.

Subsequently, the publisher rate limiter adds the throttled producer to
a queue for unthrottling and schedules a task to process this queue
after a certain delay, assuming there isn't a task already scheduled.
When this task is executed, it unthrottles all the publishers in the
queue, provided there is available quota. This queuing mechanism
guarantees fairness in the solution.

In terms of implementation, the rate limiter necessitates a token bucket
algorithm that is asynchronous, lockless, and non-blocking. This
algorithm should be capable of calculating the duration of throttling. A
proof-of-concept implementation that fulfills these requirements can be
found at https://github.com/lhotari/async-tokenbucket.

## Public-facing Changes

There are no changes to existing configs, CLI options, monitoring etc.
This PIP is about a large change which includes a major refactoring and
multiple improvements and bug fixes to rate limiting.

## More Detailed Level Design

Please refer directly to the pull request with the proposed changes for the proposed detailed level changes.
The implementation level detail questions can be handled in the pull request review. The goal is to document low level details directly in the Javadoc and comments.

# Links

<!--
Updated afterwards
-->
* [Pulsar Community Meeting minutes
  2023/11/23](https://lists.apache.org/thread/y1sqpyv37fo0k4bm1ox28wggvkb7pbtw)
* [Apache Pulsar service level objectives and rate
  limiting](https://codingthestreams.com/pulsar/2023/11/22/pulsar-slos-and-rate-limiting.html)
* Mailing List discussion thread:
  https://lists.apache.org/thread/xzrp2ypggp1oql437tvmkqgfw2b4ft33
* Mailing List voting thread: https://lists.apache.org/thread/bbfncm0hdpx42hrj0b2xnzb5oqm1pwyl
* Proposed changes for Pulsar Rate limiting refactoring:
  https://github.com/apache/pulsar/pull/21681
* Proof-of-concept asynchronous token bucket implementation: https://github.com/lhotari/async-tokenbucket