# Background knowledge

The cursor metadata contains the following three data:
- Subscription properties(Usually, this is small)
- The last sequence ID of each producer. It only exists for the cursor `pulsar.dedup`. If a topic has many, many producers, this part of the data will be large. See [PIP-6:-Guaranteed-Message-Deduplication](https://github.com/apache/pulsar/wiki/PIP-6:-Guaranteed-Message-Deduplication) for more details.
- Individual Deleted Messages(including the acknowledgment of batched messages). This part of the data occupies most of the cursor metadata's space, which is the focus of this proposal.

Differ with Kafka: Pulsar supports [individual acknowledgment](https://pulsar.apache.org/docs/2.11.x/concepts-messaging/#acknowledgment) (just like ack `{pos-1, pos-3, pos-5}`), so instead of a pointer(acknowledged on the left and un-acknowledged on the right), Pulsar needs to persist the acknowledgment state of each message, we call these records `Individual Deleted Messages.`

The current persistence mechanism of the cursor metadata(including `Individual Deleted Messages`) works like this:
1. Write the data of cursor metadata(including `Individual Deleted Messages`) to BK in one Entry; by default, the maximum size of the Entry is 5MB.
2. Write the data of cursor metadata(optional to include `Individual Deleted Messages`) to the Metadata Store(such as ZK) if BK-Write fails; data of a Metadata Store Node that is less than 10MB is recommended. Since writing large chunks of data to the Metadata Store frequently makes the Metadata Store work unstable, this is only a backstop measure.

Is 5MB enough? `Individual Deleted Messages` consists of Position_Rang(each Position_Rang occupies 32 bytes; the implementation will not be explained in this proposal). This means that the Broker can persist `5m / 32bytes` number of Position_Rang for each Subscription, and there is an additional compression mechanism at work, so it is sufficient for almost all scenarios except the following three scenarios:
- Client Miss Acknowledges: Clients receive many messages, and ack some of them, the rest still need to be acknowledged due to errors or other reasons. As time goes on, more and more records will be staying there.
- Delay Messages: Long-delayed and short-delayed messages are mixed, with only the short-delayed message successfully consumed and the long-delayed message not delivered. As time goes on, more and more records will be staying there.
- Large Number of Consumers: If the number of consumers is large and each has some discrete ack records, all add up to a large number.
- Large Number of Producers: If the number of producers is large, there might be a large data of Last Sequence ID to persist. This scenario only exists on the `pulsar.dedup` cursor.

# Motivation

Since the frequent persistence of `Individual Deleted Messages` will magnify the amount of BK Written and increase the latency of ack-response, the Broker does not immediately persist it when receiving a consumer's acknowledgment but persists it regularly. 

The data of cursor metadata is recommended to be less than 5MB; if a subscription's `Individual Deleted Messages` data is too large to persist, as the program grows for a long time, there will be more and more non-persistent data. Eventually, there will be an unacceptable amount of repeated consumption of messages when the Broker restarts.

# Goal

## In Scope

To avoid repeated consumption due to the cursor metadata being too large to persist.

## Out of Scope

This proposal will not care about this scenario: if so many producers make the metadata of cursor `pulsar.dedup` cannot persist, the task `Take Deduplication Snapshot` will be in vain due to the inability to persist.

# High-Level Design

Cache the size of the cursor metadata in memory when doing persistent data to BK. We call the cache name `cursorMetadataSizeInBytes.` 

Provide a new config named `maxUnPersistAckRecordsBytesPerSubscription,` stuck delivery messages to clients if the size of the cursor metadata reaches the limit.

Note:
- Since we will not update `cursorMetadataSizeInBytes` each time acknowledgment, `cursorMetadataSizeInBytes` is not a real-time value.
- The delayed messages will also not be redelivered after reaching the limitation.

# Detailed Design
### Public API

**broker.conf**
```java
/** 
 * Max bytes size of the cursor metadata. The Dispatcher will block dispatching if it reaches this limitation. The default value is 5M.
 * If the value is less than 1, the feature is turned off.  And this config is limited, and if it is not less than 1, it should be larger or equal to 1K.
 */
long maxPerCursorMetadataSizeInBytes;
```

## Design & Implementation Details

Cache the size of the cursor metadata(including Individual Deleted Messages) in memory when doing persist data to BK. Stuck delivery messages to clients if the size of the cursor metadata reaches the limit(`maxUnPersistAckRecordsBytesPerSubscription`). Since the size of the cursor metadata will not be updated in time(the real size will be increased when delivering messages to clients), it is an estimated value.

# Metrics
**pulsar_cursor_metadata_size_bytes**
- Description: the size of the cursor metadata of the cursor/subscription
- type: Gauge
- labels: `[cluster, namespace, topic, subscription]`
- unit: bytes

# Alert
- If the `pulsar_cursor_metadata_size_bytes` of a cursor exceeds the expected value, we can create an alert to check if something is wrong.
- If the total of `pulsar_cursor_metadata_size_bytes` in a broker is too large, maybe we should increase the max memory usage of JVM

# Links
## Differ with maxUnackedMessagesPerSubscription?
- `totalUnackedMessages`: the number of messages that need to be acknowledged.
- `cursorMetadataSizeInBytes`: the byte size of the cursor metadata, the more messages that need to be acknowledged, the larger.

`totalUnackedMessages` cannot completely replace `cursorMetadataSizeInBytes` because of the following case:
There are 5000 ledgers in the topic: `[1~5000]`, 50,000 entries in each ledger, and 1000 messages in each entry. Only messages `1:0:0`({ledger_id}: {entry_id}: {batch_index}) and message `5000:5,0000:999` were acked, so the value of totalUnackedMessages is `5000 * 50,000 * 1000 - 2` messages, but there only two Position_Rang in the Individual_Deleted_Messages: `[{1:-1~1:0}, {5000:49,998~5000:49,999}]`, it cost 64 bytes.
