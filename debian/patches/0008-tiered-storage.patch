From: HelgiSaga <bglegb@gmail.com>
Date: Tue, 21 Jan 2026 13:00:00 +0300
Subject: [PATCH] debian: Update tiered storage implementations and tests

Description: Update tiered storage implementations and tests
Origin: other, https://github.com/HelgiSaga/pulsar/tree/debian
Bug-Debian: no
Forwarded: no
Author: HelgiSaga <bglegb@gmail.com>
Last-Update: 2026-01-21

---
 tiered-storage/file-system/pom.xml                 |  516 +++---
 .../FileSystemLedgerOffloaderFactory.java          |  100 +-
 .../impl/FileStoreBackedReadHandleImpl.java        |  442 +++---
 .../impl/FileSystemManagedLedgerOffloader.java     |  830 +++++-----
 .../offload/filesystem/impl/package-info.java      |   38 +-
 .../mledger/offload/filesystem/package-info.java   |   38 +-
 .../META-INF/services/pulsar-offloader.yaml        |   44 +-
 .../src/main/resources/findbugsExclude.xml         |   76 +-
 .../offload/filesystem/FileStoreTestBase.java      |  218 +--
 .../impl/FileSystemManagedLedgerOffloaderTest.java |  394 ++---
 .../impl/FileSystemOffloaderLocalFileTest.java     |  292 ++--
 .../resources/filesystem_offload_core_site.xml     |   96 +-
 tiered-storage/jcloud/pom.xml                      |  330 ++--
 .../mledger/offload/jcloud/BackedInputStream.java  |   62 +-
 .../jcloud/BlockAwareSegmentInputStream.java       |  140 +-
 .../mledger/offload/jcloud/DataBlockHeader.java    |  112 +-
 .../jcloud/JCloudLedgerOffloaderFactory.java       |  154 +-
 .../mledger/offload/jcloud/OffloadIndexBlock.java  |  206 +--
 .../offload/jcloud/OffloadIndexBlockBuilder.java   |  164 +-
 .../offload/jcloud/OffloadIndexBlockV2.java        |  144 +-
 .../offload/jcloud/OffloadIndexBlockV2Builder.java |  166 +-
 .../mledger/offload/jcloud/OffloadIndexEntry.java  |  106 +-
 .../impl/BlobStoreBackedInputStreamImpl.java       |  422 ++---
 .../jcloud/impl/BlobStoreBackedReadHandleImpl.java |  910 +++++------
 .../impl/BlobStoreBackedReadHandleImplV2.java      |  684 ++++----
 .../impl/BlobStoreManagedLedgerOffloader.java      | 1514 +++++++++---------
 .../impl/BlockAwareSegmentInputStreamImpl.java     |  698 ++++-----
 .../offload/jcloud/impl/BufferedOffloadStream.java |  270 ++--
 .../offload/jcloud/impl/DataBlockHeaderImpl.java   |  258 +--
 .../offload/jcloud/impl/DataBlockUtils.java        |  218 +--
 .../offload/jcloud/impl/OffloadIndexBlockImpl.java |  724 ++++-----
 .../impl/OffloadIndexBlockV2BuilderImpl.java       |  300 ++--
 .../jcloud/impl/OffloadIndexBlockV2Impl.java       |  758 ++++-----
 .../offload/jcloud/impl/OffloadIndexEntryImpl.java |  168 +-
 .../mledger/offload/jcloud/impl/OffsetsCache.java  |  170 +-
 .../jcloud/impl/StreamingDataBlockHeaderImpl.java  |  278 ++--
 .../mledger/offload/jcloud/impl/package-info.java  |   36 +-
 .../mledger/offload/jcloud/package-info.java       |   36 +-
 .../offload/jcloud/provider/BlobStoreLocation.java |  124 +-
 .../jcloud/provider/JCloudBlobStoreProvider.java   |  904 +++++------
 .../provider/TieredStorageConfiguration.java       |  752 ++++-----
 .../offload/jcloud/provider/package-info.java      |   36 +-
 .../META-INF/services/pulsar-offloader.yaml        |   44 +-
 .../jcloud/src/main/resources/findbugsExclude.xml  |   96 +-
 .../jcloud/BlobStoreBackedInputStreamTest.java     |  612 ++++----
 .../mledger/offload/jcloud/BlobStoreTestBase.java  |  166 +-
 .../impl/BlobStoreBackedInputStreamTest.java       |  104 +-
 .../impl/BlobStoreBackedReadHandleImplTest.java    |  424 ++---
 .../impl/BlobStoreManagedLedgerOffloaderBase.java  |  342 ++--
 ...obStoreManagedLedgerOffloaderStreamingTest.java | 1068 ++++++-------
 .../impl/BlobStoreManagedLedgerOffloaderTest.java  | 1394 ++++++++---------
 .../impl/BlockAwareSegmentInputStreamTest.java     | 1640 ++++++++++----------
 .../jcloud/impl/BufferedOffloadStreamTest.java     |  386 ++---
 .../offload/jcloud/impl/DataBlockHeaderTest.java   |  166 +-
 .../offload/jcloud/impl/DataBlockHeaderV2Test.java |  174 +--
 .../offload/jcloud/impl/DataBlockUtilsTest.java    |  128 +-
 .../offload/jcloud/impl/MockManagedLedger.java     |  926 +++++------
 .../offload/jcloud/impl/OffloadIndexTest.java      |  514 +++---
 .../offload/jcloud/impl/OffloadIndexV2Test.java    |  654 ++++----
 .../offload/jcloud/impl/OffsetsCacheTest.java      |   90 +-
 .../AbstractJCloudBlobStoreFactoryTest.java        |  228 +--
 .../provider/JCloudBlobStoreProviderTest.java      |  266 ++--
 .../provider/TieredStorageConfigurationTest.java   |  466 +++---
 .../provider/TransientBlobStoreFactoryTest.java    |  118 +-
 .../jcloud/src/test/resources/log4j2-test.yml      |  122 +-
 tiered-storage/pom.xml                             |  108 +-
 66 files changed, 12082 insertions(+), 12082 deletions(-)

diff --git a/tiered-storage/file-system/pom.xml b/tiered-storage/file-system/pom.xml
index 274833ee02..13da23f480 100644
--- a/tiered-storage/file-system/pom.xml
+++ b/tiered-storage/file-system/pom.xml
@@ -1,258 +1,258 @@
-<!--
-
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
-    <modelVersion>4.0.0</modelVersion>
-
-    <parent>
-        <groupId>org.apache.pulsar</groupId>
-        <artifactId>tiered-storage-parent</artifactId>
-        <version>4.1.2</version>
-    </parent>
-
-    <artifactId>tiered-storage-file-system</artifactId>
-    <name>Apache Pulsar :: Tiered Storage :: File System</name>
-    <dependencies>
-      <dependency>
-          <groupId>${project.groupId}</groupId>
-          <artifactId>managed-ledger</artifactId>
-          <version>${project.version}</version>
-          <scope>provided</scope>
-      </dependency>
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-common</artifactId>
-        <version>${hdfs-offload-version3}</version>
-        <exclusions>
-          <exclusion>
-            <groupId>log4j</groupId>
-            <artifactId>log4j</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.slf4j</groupId>
-            <artifactId>*</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>dnsjava</groupId>
-            <artifactId>dnsjava</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.bouncycastle</groupId>
-            <artifactId>bcprov-jdk15on</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>io.netty</groupId>
-            <artifactId>*</artifactId>
-          </exclusion>
-        </exclusions>
-      </dependency>
-      <dependency>
-        <groupId>org.bouncycastle</groupId>
-        <artifactId>bcprov-jdk18on</artifactId>
-      </dependency>
-      <dependency>
-            <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-hdfs-client</artifactId>
-            <version>${hdfs-offload-version3}</version>
-            <exclusions>
-                <exclusion>
-                    <groupId>org.apache.avro</groupId>
-                    <artifactId>avro</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>org.mortbay.jetty</groupId>
-                    <artifactId>jetty</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>com.sun.jersey</groupId>
-                    <artifactId>jersey-core</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>com.sun.jersey</groupId>
-                    <artifactId>jersey-server</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>javax.servlet</groupId>
-                    <artifactId>servlet-api</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>dnsjava</groupId>
-                    <artifactId>dnsjava</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>org.bouncycastle</groupId>
-                    <artifactId>bcprov-jdk15on</artifactId>
-                </exclusion>
-            </exclusions>
-        </dependency>
-        <!-- fix hadoop-commons vulnerable dependencies -->
-        <dependency>
-            <groupId>org.apache.avro</groupId>
-            <artifactId>avro</artifactId>
-            <version>${avro.version}</version>
-        </dependency>
-        <!-- fix hadoop-commons vulnerable dependencies -->
-        <dependency>
-            <groupId>net.minidev</groupId>
-            <artifactId>json-smart</artifactId>
-            <version>${json-smart.version}</version>
-        </dependency>
-        <dependency>
-            <groupId>com.google.protobuf</groupId>
-            <artifactId>protobuf-java</artifactId>
-        </dependency>
-
-        <dependency>
-            <groupId>${project.groupId}</groupId>
-            <artifactId>testmocks</artifactId>
-            <version>${project.version}</version>
-            <scope>test</scope>
-        </dependency>
-
-        <dependency>
-            <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-minicluster</artifactId>
-            <version>${hdfs-offload-version3}</version>
-            <scope>test</scope>
-            <exclusions>
-                <exclusion>
-                    <groupId>io.netty</groupId>
-                    <artifactId>netty-all</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>org.bouncycastle</groupId>
-                    <artifactId>*</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>org.slf4j</groupId>
-                    <artifactId>*</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>dnsjava</groupId>
-                    <artifactId>dnsjava</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>org.bouncycastle</groupId>
-                    <artifactId>bcprov-jdk15on</artifactId>
-                </exclusion>
-            </exclusions>
-        </dependency>
-
-        <dependency>
-            <groupId>org.bouncycastle</groupId>
-            <artifactId>bcpkix-jdk18on</artifactId>
-            <scope>test</scope>
-        </dependency>
-
-
-        <dependency>
-          <groupId>io.netty</groupId>
-          <artifactId>netty-codec-http</artifactId>
-          <scope>test</scope>
-        </dependency>
-
-        <dependency>
-            <groupId>org.eclipse.jetty</groupId>
-            <artifactId>jetty-server</artifactId>
-            <scope>test</scope>
-        </dependency>
-        <dependency>
-            <groupId>org.eclipse.jetty</groupId>
-            <artifactId>jetty-alpn-conscrypt-server</artifactId>
-            <scope>test</scope>
-        </dependency>
-        <dependency>
-            <groupId>org.eclipse.jetty</groupId>
-            <artifactId>jetty-servlet</artifactId>
-            <scope>test</scope>
-        </dependency>
-        <dependency>
-            <groupId>org.eclipse.jetty</groupId>
-            <artifactId>jetty-util</artifactId>
-            <scope>test</scope>
-        </dependency>
-    </dependencies>
-
-    <build>
-        <plugins>
-            <plugin>
-                <groupId>org.apache.nifi</groupId>
-                <artifactId>nifi-nar-maven-plugin</artifactId>
-            </plugin>
-            <plugin>
-                <groupId>com.github.spotbugs</groupId>
-                <artifactId>spotbugs-maven-plugin</artifactId>
-                <version>${spotbugs-maven-plugin.version}</version>
-                <configuration>
-                    <excludeFilterFile>${basedir}/src/main/resources/findbugsExclude.xml</excludeFilterFile>
-                </configuration>
-                <executions>
-                    <execution>
-                        <id>spotbugs</id>
-                        <phase>verify</phase>
-                        <goals>
-                            <goal>check</goal>
-                        </goals>
-                    </execution>
-                </executions>
-            </plugin>
-            <plugin>
-                <groupId>org.apache.maven.plugins</groupId>
-                <artifactId>maven-checkstyle-plugin</artifactId>
-                <executions>
-                    <execution>
-                        <id>checkstyle</id>
-                        <phase>verify</phase>
-                        <goals>
-                            <goal>check</goal>
-                        </goals>
-                    </execution>
-                </executions>
-            </plugin>
-        </plugins>
-    </build>
-    <profiles>
-        <!--
-        The only working way for OWASP dependency checker plugin
-        to exclude module when failBuildOnCVSS is used
-        in the root pom's plugin.
-        -->
-        <profile>
-            <id>owasp-dependency-check</id>
-            <build>
-                <plugins>
-                    <plugin>
-                        <groupId>org.owasp</groupId>
-                        <artifactId>dependency-check-maven</artifactId>
-                        <executions>
-                            <execution>
-                                <goals>
-                                    <goal>aggregate</goal>
-                                </goals>
-                                <phase>none</phase>
-                            </execution>
-                        </executions>
-                    </plugin>
-                </plugins>
-            </build>
-        </profile>
-    </profiles>
-</project>
+<!--
+
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.apache.pulsar</groupId>
+        <artifactId>tiered-storage-parent</artifactId>
+        <version>4.1.2</version>
+    </parent>
+
+    <artifactId>tiered-storage-file-system</artifactId>
+    <name>Apache Pulsar :: Tiered Storage :: File System</name>
+    <dependencies>
+      <dependency>
+          <groupId>${project.groupId}</groupId>
+          <artifactId>managed-ledger</artifactId>
+          <version>${project.version}</version>
+          <scope>provided</scope>
+      </dependency>
+      <dependency>
+        <groupId>org.apache.hadoop</groupId>
+        <artifactId>hadoop-common</artifactId>
+        <version>${hdfs-offload-version3}</version>
+        <exclusions>
+          <exclusion>
+            <groupId>log4j</groupId>
+            <artifactId>log4j</artifactId>
+          </exclusion>
+          <exclusion>
+            <groupId>org.slf4j</groupId>
+            <artifactId>*</artifactId>
+          </exclusion>
+          <exclusion>
+            <groupId>dnsjava</groupId>
+            <artifactId>dnsjava</artifactId>
+          </exclusion>
+          <exclusion>
+            <groupId>org.bouncycastle</groupId>
+            <artifactId>bcprov-jdk15on</artifactId>
+          </exclusion>
+          <exclusion>
+            <groupId>io.netty</groupId>
+            <artifactId>*</artifactId>
+          </exclusion>
+        </exclusions>
+      </dependency>
+      <dependency>
+        <groupId>org.bouncycastle</groupId>
+        <artifactId>bcprov-jdk18on</artifactId>
+      </dependency>
+      <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-hdfs-client</artifactId>
+            <version>${hdfs-offload-version3}</version>
+            <exclusions>
+                <exclusion>
+                    <groupId>org.apache.avro</groupId>
+                    <artifactId>avro</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>jetty</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>com.sun.jersey</groupId>
+                    <artifactId>jersey-core</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>com.sun.jersey</groupId>
+                    <artifactId>jersey-server</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>javax.servlet</groupId>
+                    <artifactId>servlet-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>dnsjava</groupId>
+                    <artifactId>dnsjava</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.bouncycastle</groupId>
+                    <artifactId>bcprov-jdk15on</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+        <!-- fix hadoop-commons vulnerable dependencies -->
+        <dependency>
+            <groupId>org.apache.avro</groupId>
+            <artifactId>avro</artifactId>
+            <version>${avro.version}</version>
+        </dependency>
+        <!-- fix hadoop-commons vulnerable dependencies -->
+        <dependency>
+            <groupId>net.minidev</groupId>
+            <artifactId>json-smart</artifactId>
+            <version>${json-smart.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>com.google.protobuf</groupId>
+            <artifactId>protobuf-java</artifactId>
+        </dependency>
+
+        <dependency>
+            <groupId>${project.groupId}</groupId>
+            <artifactId>testmocks</artifactId>
+            <version>${project.version}</version>
+            <scope>test</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-minicluster</artifactId>
+            <version>${hdfs-offload-version3}</version>
+            <scope>test</scope>
+            <exclusions>
+                <exclusion>
+                    <groupId>io.netty</groupId>
+                    <artifactId>netty-all</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.bouncycastle</groupId>
+                    <artifactId>*</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.slf4j</groupId>
+                    <artifactId>*</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>dnsjava</groupId>
+                    <artifactId>dnsjava</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.bouncycastle</groupId>
+                    <artifactId>bcprov-jdk15on</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+
+        <dependency>
+            <groupId>org.bouncycastle</groupId>
+            <artifactId>bcpkix-jdk18on</artifactId>
+            <scope>test</scope>
+        </dependency>
+
+
+        <dependency>
+          <groupId>io.netty</groupId>
+          <artifactId>netty-codec-http</artifactId>
+          <scope>test</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.eclipse.jetty</groupId>
+            <artifactId>jetty-server</artifactId>
+            <scope>test</scope>
+        </dependency>
+        <dependency>
+            <groupId>org.eclipse.jetty</groupId>
+            <artifactId>jetty-alpn-conscrypt-server</artifactId>
+            <scope>test</scope>
+        </dependency>
+        <dependency>
+            <groupId>org.eclipse.jetty</groupId>
+            <artifactId>jetty-servlet</artifactId>
+            <scope>test</scope>
+        </dependency>
+        <dependency>
+            <groupId>org.eclipse.jetty</groupId>
+            <artifactId>jetty-util</artifactId>
+            <scope>test</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.nifi</groupId>
+                <artifactId>nifi-nar-maven-plugin</artifactId>
+            </plugin>
+            <plugin>
+                <groupId>com.github.spotbugs</groupId>
+                <artifactId>spotbugs-maven-plugin</artifactId>
+                <version>${spotbugs-maven-plugin.version}</version>
+                <configuration>
+                    <excludeFilterFile>${basedir}/src/main/resources/findbugsExclude.xml</excludeFilterFile>
+                </configuration>
+                <executions>
+                    <execution>
+                        <id>spotbugs</id>
+                        <phase>verify</phase>
+                        <goals>
+                            <goal>check</goal>
+                        </goals>
+                    </execution>
+                </executions>
+            </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-checkstyle-plugin</artifactId>
+                <executions>
+                    <execution>
+                        <id>checkstyle</id>
+                        <phase>verify</phase>
+                        <goals>
+                            <goal>check</goal>
+                        </goals>
+                    </execution>
+                </executions>
+            </plugin>
+        </plugins>
+    </build>
+    <profiles>
+        <!--
+        The only working way for OWASP dependency checker plugin
+        to exclude module when failBuildOnCVSS is used
+        in the root pom's plugin.
+        -->
+        <profile>
+            <id>owasp-dependency-check</id>
+            <build>
+                <plugins>
+                    <plugin>
+                        <groupId>org.owasp</groupId>
+                        <artifactId>dependency-check-maven</artifactId>
+                        <executions>
+                            <execution>
+                                <goals>
+                                    <goal>aggregate</goal>
+                                </goals>
+                                <phase>none</phase>
+                            </execution>
+                        </executions>
+                    </plugin>
+                </plugins>
+            </build>
+        </profile>
+    </profiles>
+</project>
diff --git a/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/FileSystemLedgerOffloaderFactory.java b/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/FileSystemLedgerOffloaderFactory.java
index 5649d264f5..750c5c7819 100644
--- a/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/FileSystemLedgerOffloaderFactory.java
+++ b/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/FileSystemLedgerOffloaderFactory.java
@@ -1,50 +1,50 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.filesystem;
-
-import java.io.IOException;
-import java.util.Map;
-import org.apache.bookkeeper.common.util.OrderedScheduler;
-import org.apache.bookkeeper.mledger.LedgerOffloaderFactory;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStatsDisable;
-import org.apache.bookkeeper.mledger.offload.filesystem.impl.FileSystemManagedLedgerOffloader;
-import org.apache.pulsar.common.policies.data.OffloadPoliciesImpl;
-
-public class FileSystemLedgerOffloaderFactory implements LedgerOffloaderFactory<FileSystemManagedLedgerOffloader> {
-    @Override
-    public boolean isDriverSupported(String driverName) {
-        return FileSystemManagedLedgerOffloader.driverSupported(driverName);
-    }
-
-    @Override
-    public FileSystemManagedLedgerOffloader create(OffloadPoliciesImpl offloadPolicies,
-                                                   Map<String, String> userMetadata, OrderedScheduler scheduler)
-            throws IOException {
-        return create(offloadPolicies, userMetadata, scheduler, LedgerOffloaderStatsDisable.INSTANCE);
-    }
-
-    @Override
-    public FileSystemManagedLedgerOffloader create(OffloadPoliciesImpl offloadPolicies,
-                                                   Map<String, String> userMetadata,
-                                                   OrderedScheduler scheduler,
-                                                   LedgerOffloaderStats offloaderStats) throws IOException {
-        return FileSystemManagedLedgerOffloader.create(offloadPolicies, scheduler, offloaderStats);
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.filesystem;
+
+import java.io.IOException;
+import java.util.Map;
+import org.apache.bookkeeper.common.util.OrderedScheduler;
+import org.apache.bookkeeper.mledger.LedgerOffloaderFactory;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStatsDisable;
+import org.apache.bookkeeper.mledger.offload.filesystem.impl.FileSystemManagedLedgerOffloader;
+import org.apache.pulsar.common.policies.data.OffloadPoliciesImpl;
+
+public class FileSystemLedgerOffloaderFactory implements LedgerOffloaderFactory<FileSystemManagedLedgerOffloader> {
+    @Override
+    public boolean isDriverSupported(String driverName) {
+        return FileSystemManagedLedgerOffloader.driverSupported(driverName);
+    }
+
+    @Override
+    public FileSystemManagedLedgerOffloader create(OffloadPoliciesImpl offloadPolicies,
+                                                   Map<String, String> userMetadata, OrderedScheduler scheduler)
+            throws IOException {
+        return create(offloadPolicies, userMetadata, scheduler, LedgerOffloaderStatsDisable.INSTANCE);
+    }
+
+    @Override
+    public FileSystemManagedLedgerOffloader create(OffloadPoliciesImpl offloadPolicies,
+                                                   Map<String, String> userMetadata,
+                                                   OrderedScheduler scheduler,
+                                                   LedgerOffloaderStats offloaderStats) throws IOException {
+        return FileSystemManagedLedgerOffloader.create(offloadPolicies, scheduler, offloaderStats);
+    }
+}
diff --git a/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileStoreBackedReadHandleImpl.java b/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileStoreBackedReadHandleImpl.java
index abdd9d4b11..4f00df00b2 100644
--- a/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileStoreBackedReadHandleImpl.java
+++ b/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileStoreBackedReadHandleImpl.java
@@ -1,221 +1,221 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.filesystem.impl;
-
-import static org.apache.bookkeeper.mledger.offload.OffloadUtils.parseLedgerMetadata;
-import io.netty.buffer.ByteBuf;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.ScheduledExecutorService;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicReference;
-import org.apache.bookkeeper.client.BKException;
-import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;
-import org.apache.bookkeeper.client.api.LedgerEntries;
-import org.apache.bookkeeper.client.api.LedgerEntry;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.client.api.ReadHandle;
-import org.apache.bookkeeper.client.impl.LedgerEntriesImpl;
-import org.apache.bookkeeper.client.impl.LedgerEntryImpl;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.bookkeeper.mledger.ManagedLedgerException;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.MapFile;
-import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
-import org.apache.pulsar.common.naming.TopicName;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class FileStoreBackedReadHandleImpl implements ReadHandle {
-    private static final Logger log = LoggerFactory.getLogger(FileStoreBackedReadHandleImpl.class);
-    private final ExecutorService executor;
-    private final MapFile.Reader reader;
-    private final long ledgerId;
-    private final LedgerMetadata ledgerMetadata;
-    private final LedgerOffloaderStats offloaderStats;
-    private final String managedLedgerName;
-    private final String topicName;
-    enum State {
-        Opened,
-        Closed
-    }
-    private volatile State state;
-    private final AtomicReference<CompletableFuture<Void>> closeFuture = new AtomicReference<>();
-
-    private FileStoreBackedReadHandleImpl(ExecutorService executor, MapFile.Reader reader, long ledgerId,
-                                          LedgerOffloaderStats offloaderStats,
-                                          String managedLedgerName) throws IOException {
-        this.ledgerId = ledgerId;
-        this.executor = executor;
-        this.reader = reader;
-        this.offloaderStats = offloaderStats;
-        this.managedLedgerName = managedLedgerName;
-        this.topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
-        LongWritable key = new LongWritable();
-        BytesWritable value = new BytesWritable();
-        try {
-            key.set(FileSystemManagedLedgerOffloader.METADATA_KEY_INDEX);
-            long startReadIndexTime = System.nanoTime();
-            reader.get(key, value);
-            offloaderStats.recordReadOffloadIndexLatency(topicName,
-                    System.nanoTime() - startReadIndexTime, TimeUnit.NANOSECONDS);
-            this.ledgerMetadata = parseLedgerMetadata(ledgerId, value.copyBytes());
-            state = State.Opened;
-        } catch (IOException e) {
-            log.error("Fail to read LedgerMetadata for ledgerId {}",
-                    ledgerId);
-            throw new IOException("Fail to read LedgerMetadata for ledgerId " + key.get());
-        }
-    }
-
-    @Override
-    public long getId() {
-        return ledgerId;
-    }
-
-    @Override
-    public LedgerMetadata getLedgerMetadata() {
-        return ledgerMetadata;
-
-    }
-
-    @Override
-    public CompletableFuture<Void> closeAsync() {
-        if (closeFuture.get() != null || !closeFuture.compareAndSet(null, new CompletableFuture<>())) {
-            return closeFuture.get();
-        }
-
-        CompletableFuture<Void> promise = closeFuture.get();
-        executor.execute(() -> {
-            try {
-                reader.close();
-                state = State.Closed;
-                promise.complete(null);
-            } catch (IOException t) {
-                promise.completeExceptionally(t);
-            }
-        });
-        return promise;
-    }
-
-    @Override
-    public CompletableFuture<LedgerEntries> readAsync(long firstEntry, long lastEntry) {
-        if (log.isDebugEnabled()) {
-            log.debug("Ledger {}: reading {} - {}", getId(), firstEntry, lastEntry);
-        }
-        CompletableFuture<LedgerEntries> promise = new CompletableFuture<>();
-        executor.execute(() -> {
-            if (state == State.Closed) {
-                log.warn("Reading a closed read handler. Ledger ID: {}, Read range: {}-{}",
-                        ledgerId, firstEntry, lastEntry);
-                promise.completeExceptionally(new ManagedLedgerException.OffloadReadHandleClosedException());
-                return;
-            }
-            if (firstEntry > lastEntry
-                    || firstEntry < 0
-                    || lastEntry > getLastAddConfirmed()) {
-                promise.completeExceptionally(new BKException.BKIncorrectParameterException());
-                return;
-            }
-            long entriesToRead = (lastEntry - firstEntry) + 1;
-            List<LedgerEntry> entries = new ArrayList<LedgerEntry>();
-            long nextExpectedId = firstEntry;
-            LongWritable key = new LongWritable();
-            BytesWritable value = new BytesWritable();
-            try {
-                key.set(nextExpectedId - 1);
-                reader.seek(key);
-                while (entriesToRead > 0) {
-                    long startReadTime = System.nanoTime();
-                    reader.next(key, value);
-                    this.offloaderStats.recordReadOffloadDataLatency(topicName,
-                            System.nanoTime() - startReadTime, TimeUnit.NANOSECONDS);
-                    int length = value.getLength();
-                    long entryId = key.get();
-                    if (entryId == nextExpectedId) {
-                        ByteBuf buf = PulsarByteBufAllocator.DEFAULT.buffer(length, length);
-                        entries.add(LedgerEntryImpl.create(ledgerId, entryId, length, buf));
-                        buf.writeBytes(value.copyBytes());
-                        entriesToRead--;
-                        nextExpectedId++;
-                        this.offloaderStats.recordReadOffloadBytes(topicName, length);
-                    } else if (entryId > lastEntry) {
-                        log.info("Expected to read {}, but read {}, which is greater than last entry {}",
-                                nextExpectedId, entryId, lastEntry);
-                        throw new BKException.BKUnexpectedConditionException();
-                    }
-                }
-                promise.complete(LedgerEntriesImpl.create(entries));
-            } catch (Throwable t) {
-                this.offloaderStats.recordReadOffloadError(topicName);
-                promise.completeExceptionally(t);
-                entries.forEach(LedgerEntry::close);
-            }
-        });
-        return promise;
-    }
-
-    @Override
-    public CompletableFuture<LedgerEntries> readUnconfirmedAsync(long firstEntry, long lastEntry) {
-        return readAsync(firstEntry, lastEntry);
-    }
-
-    @Override
-    public CompletableFuture<Long> readLastAddConfirmedAsync() {
-        return CompletableFuture.completedFuture(getLastAddConfirmed());
-    }
-
-    @Override
-    public CompletableFuture<Long> tryReadLastAddConfirmedAsync() {
-        return CompletableFuture.completedFuture(getLastAddConfirmed());
-    }
-
-    @Override
-    public long getLastAddConfirmed() {
-        return getLedgerMetadata().getLastEntryId();
-    }
-
-    @Override
-    public long getLength() {
-        return getLedgerMetadata().getLength();
-    }
-
-    @Override
-    public boolean isClosed() {
-        return getLedgerMetadata().isClosed();
-    }
-
-    @Override
-    public CompletableFuture<LastConfirmedAndEntry> readLastAddConfirmedAndEntryAsync(long entryId,
-                                                                                      long timeOutInMillis,
-                                                                                      boolean parallel) {
-        CompletableFuture<LastConfirmedAndEntry> promise = new CompletableFuture<>();
-        promise.completeExceptionally(new UnsupportedOperationException());
-        return promise;
-    }
-
-    public static ReadHandle open(ScheduledExecutorService executor, MapFile.Reader reader, long ledgerId,
-                                  LedgerOffloaderStats offloaderStats, String managedLedgerName) throws IOException {
-        return new FileStoreBackedReadHandleImpl(executor, reader, ledgerId, offloaderStats, managedLedgerName);
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.filesystem.impl;
+
+import static org.apache.bookkeeper.mledger.offload.OffloadUtils.parseLedgerMetadata;
+import io.netty.buffer.ByteBuf;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicReference;
+import org.apache.bookkeeper.client.BKException;
+import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;
+import org.apache.bookkeeper.client.api.LedgerEntries;
+import org.apache.bookkeeper.client.api.LedgerEntry;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.client.api.ReadHandle;
+import org.apache.bookkeeper.client.impl.LedgerEntriesImpl;
+import org.apache.bookkeeper.client.impl.LedgerEntryImpl;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.bookkeeper.mledger.ManagedLedgerException;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.MapFile;
+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
+import org.apache.pulsar.common.naming.TopicName;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class FileStoreBackedReadHandleImpl implements ReadHandle {
+    private static final Logger log = LoggerFactory.getLogger(FileStoreBackedReadHandleImpl.class);
+    private final ExecutorService executor;
+    private final MapFile.Reader reader;
+    private final long ledgerId;
+    private final LedgerMetadata ledgerMetadata;
+    private final LedgerOffloaderStats offloaderStats;
+    private final String managedLedgerName;
+    private final String topicName;
+    enum State {
+        Opened,
+        Closed
+    }
+    private volatile State state;
+    private final AtomicReference<CompletableFuture<Void>> closeFuture = new AtomicReference<>();
+
+    private FileStoreBackedReadHandleImpl(ExecutorService executor, MapFile.Reader reader, long ledgerId,
+                                          LedgerOffloaderStats offloaderStats,
+                                          String managedLedgerName) throws IOException {
+        this.ledgerId = ledgerId;
+        this.executor = executor;
+        this.reader = reader;
+        this.offloaderStats = offloaderStats;
+        this.managedLedgerName = managedLedgerName;
+        this.topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
+        LongWritable key = new LongWritable();
+        BytesWritable value = new BytesWritable();
+        try {
+            key.set(FileSystemManagedLedgerOffloader.METADATA_KEY_INDEX);
+            long startReadIndexTime = System.nanoTime();
+            reader.get(key, value);
+            offloaderStats.recordReadOffloadIndexLatency(topicName,
+                    System.nanoTime() - startReadIndexTime, TimeUnit.NANOSECONDS);
+            this.ledgerMetadata = parseLedgerMetadata(ledgerId, value.copyBytes());
+            state = State.Opened;
+        } catch (IOException e) {
+            log.error("Fail to read LedgerMetadata for ledgerId {}",
+                    ledgerId);
+            throw new IOException("Fail to read LedgerMetadata for ledgerId " + key.get());
+        }
+    }
+
+    @Override
+    public long getId() {
+        return ledgerId;
+    }
+
+    @Override
+    public LedgerMetadata getLedgerMetadata() {
+        return ledgerMetadata;
+
+    }
+
+    @Override
+    public CompletableFuture<Void> closeAsync() {
+        if (closeFuture.get() != null || !closeFuture.compareAndSet(null, new CompletableFuture<>())) {
+            return closeFuture.get();
+        }
+
+        CompletableFuture<Void> promise = closeFuture.get();
+        executor.execute(() -> {
+            try {
+                reader.close();
+                state = State.Closed;
+                promise.complete(null);
+            } catch (IOException t) {
+                promise.completeExceptionally(t);
+            }
+        });
+        return promise;
+    }
+
+    @Override
+    public CompletableFuture<LedgerEntries> readAsync(long firstEntry, long lastEntry) {
+        if (log.isDebugEnabled()) {
+            log.debug("Ledger {}: reading {} - {}", getId(), firstEntry, lastEntry);
+        }
+        CompletableFuture<LedgerEntries> promise = new CompletableFuture<>();
+        executor.execute(() -> {
+            if (state == State.Closed) {
+                log.warn("Reading a closed read handler. Ledger ID: {}, Read range: {}-{}",
+                        ledgerId, firstEntry, lastEntry);
+                promise.completeExceptionally(new ManagedLedgerException.OffloadReadHandleClosedException());
+                return;
+            }
+            if (firstEntry > lastEntry
+                    || firstEntry < 0
+                    || lastEntry > getLastAddConfirmed()) {
+                promise.completeExceptionally(new BKException.BKIncorrectParameterException());
+                return;
+            }
+            long entriesToRead = (lastEntry - firstEntry) + 1;
+            List<LedgerEntry> entries = new ArrayList<LedgerEntry>();
+            long nextExpectedId = firstEntry;
+            LongWritable key = new LongWritable();
+            BytesWritable value = new BytesWritable();
+            try {
+                key.set(nextExpectedId - 1);
+                reader.seek(key);
+                while (entriesToRead > 0) {
+                    long startReadTime = System.nanoTime();
+                    reader.next(key, value);
+                    this.offloaderStats.recordReadOffloadDataLatency(topicName,
+                            System.nanoTime() - startReadTime, TimeUnit.NANOSECONDS);
+                    int length = value.getLength();
+                    long entryId = key.get();
+                    if (entryId == nextExpectedId) {
+                        ByteBuf buf = PulsarByteBufAllocator.DEFAULT.buffer(length, length);
+                        entries.add(LedgerEntryImpl.create(ledgerId, entryId, length, buf));
+                        buf.writeBytes(value.copyBytes());
+                        entriesToRead--;
+                        nextExpectedId++;
+                        this.offloaderStats.recordReadOffloadBytes(topicName, length);
+                    } else if (entryId > lastEntry) {
+                        log.info("Expected to read {}, but read {}, which is greater than last entry {}",
+                                nextExpectedId, entryId, lastEntry);
+                        throw new BKException.BKUnexpectedConditionException();
+                    }
+                }
+                promise.complete(LedgerEntriesImpl.create(entries));
+            } catch (Throwable t) {
+                this.offloaderStats.recordReadOffloadError(topicName);
+                promise.completeExceptionally(t);
+                entries.forEach(LedgerEntry::close);
+            }
+        });
+        return promise;
+    }
+
+    @Override
+    public CompletableFuture<LedgerEntries> readUnconfirmedAsync(long firstEntry, long lastEntry) {
+        return readAsync(firstEntry, lastEntry);
+    }
+
+    @Override
+    public CompletableFuture<Long> readLastAddConfirmedAsync() {
+        return CompletableFuture.completedFuture(getLastAddConfirmed());
+    }
+
+    @Override
+    public CompletableFuture<Long> tryReadLastAddConfirmedAsync() {
+        return CompletableFuture.completedFuture(getLastAddConfirmed());
+    }
+
+    @Override
+    public long getLastAddConfirmed() {
+        return getLedgerMetadata().getLastEntryId();
+    }
+
+    @Override
+    public long getLength() {
+        return getLedgerMetadata().getLength();
+    }
+
+    @Override
+    public boolean isClosed() {
+        return getLedgerMetadata().isClosed();
+    }
+
+    @Override
+    public CompletableFuture<LastConfirmedAndEntry> readLastAddConfirmedAndEntryAsync(long entryId,
+                                                                                      long timeOutInMillis,
+                                                                                      boolean parallel) {
+        CompletableFuture<LastConfirmedAndEntry> promise = new CompletableFuture<>();
+        promise.completeExceptionally(new UnsupportedOperationException());
+        return promise;
+    }
+
+    public static ReadHandle open(ScheduledExecutorService executor, MapFile.Reader reader, long ledgerId,
+                                  LedgerOffloaderStats offloaderStats, String managedLedgerName) throws IOException {
+        return new FileStoreBackedReadHandleImpl(executor, reader, ledgerId, offloaderStats, managedLedgerName);
+    }
+}
diff --git a/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileSystemManagedLedgerOffloader.java b/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileSystemManagedLedgerOffloader.java
index 40ae52d7b7..d031b0c6f5 100644
--- a/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileSystemManagedLedgerOffloader.java
+++ b/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileSystemManagedLedgerOffloader.java
@@ -1,415 +1,415 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.filesystem.impl;
-
-import static org.apache.bookkeeper.mledger.offload.OffloadUtils.buildLedgerMetadataFormat;
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.collect.ImmutableMap;
-import com.google.common.util.concurrent.MoreExecutors;
-import io.netty.util.Recycler;
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.UUID;
-import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.Semaphore;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicLong;
-import org.apache.bookkeeper.client.api.LedgerEntries;
-import org.apache.bookkeeper.client.api.LedgerEntry;
-import org.apache.bookkeeper.client.api.ReadHandle;
-import org.apache.bookkeeper.common.util.OrderedScheduler;
-import org.apache.bookkeeper.mledger.LedgerOffloader;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.bookkeeper.mledger.offload.filesystem.FileSystemLedgerOffloaderFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.MapFile;
-import org.apache.pulsar.common.naming.TopicName;
-import org.apache.pulsar.common.policies.data.OffloadPolicies;
-import org.apache.pulsar.common.policies.data.OffloadPoliciesImpl;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class FileSystemManagedLedgerOffloader implements LedgerOffloader {
-
-    private static final Logger log = LoggerFactory.getLogger(FileSystemManagedLedgerOffloader.class);
-    private static final String STORAGE_BASE_PATH = "storageBasePath";
-    private static final String DRIVER_NAMES = "filesystem";
-    private static final String MANAGED_LEDGER_NAME = "ManagedLedgerName";
-    static final long METADATA_KEY_INDEX = -1;
-    private final Configuration configuration;
-    private final String driverName;
-    private final String storageBasePath;
-    private final FileSystem fileSystem;
-    private OrderedScheduler scheduler;
-    private static final long ENTRIES_PER_READ = 100;
-    private OrderedScheduler assignmentScheduler;
-    private OffloadPolicies offloadPolicies;
-    private final LedgerOffloaderStats offloaderStats;
-
-    public static boolean driverSupported(String driver) {
-        return DRIVER_NAMES.equals(driver);
-    }
-
-    @Override
-    public String getOffloadDriverName() {
-        return driverName;
-    }
-
-    public static FileSystemManagedLedgerOffloader create(OffloadPoliciesImpl conf,
-                                                          OrderedScheduler scheduler,
-                                                          LedgerOffloaderStats offloaderStats) throws IOException {
-        return new FileSystemManagedLedgerOffloader(conf, scheduler, offloaderStats);
-    }
-
-    private FileSystemManagedLedgerOffloader(OffloadPoliciesImpl conf, OrderedScheduler scheduler,
-                                             LedgerOffloaderStats offloaderStats) throws IOException {
-        this.offloadPolicies = conf;
-        this.configuration = new Configuration();
-        if (conf.getFileSystemProfilePath() != null) {
-            String[] paths = conf.getFileSystemProfilePath().split(",");
-            for (int i = 0; i < paths.length; i++) {
-                configuration.addResource(new Path(paths[i]));
-            }
-        }
-        if (!"".equals(conf.getFileSystemURI()) && conf.getFileSystemURI() != null) {
-            configuration.set("fs.defaultFS", conf.getFileSystemURI());
-        }
-        if (configuration.get("fs.hdfs.impl") == null) {
-            this.configuration.set("fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem");
-        }
-
-        if (configuration.get("fs.file.impl") == null) {
-            this.configuration.set("fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem");
-        }
-
-        this.configuration.setClassLoader(FileSystemLedgerOffloaderFactory.class.getClassLoader());
-        this.driverName = conf.getManagedLedgerOffloadDriver();
-        this.storageBasePath = configuration.get("fs.defaultFS");
-        this.scheduler = scheduler;
-        this.fileSystem = FileSystem.get(configuration);
-        this.assignmentScheduler = OrderedScheduler.newSchedulerBuilder()
-                .numThreads(conf.getManagedLedgerOffloadMaxThreads())
-                .name("offload-assignment").build();
-        this.offloaderStats = offloaderStats;
-    }
-
-    @VisibleForTesting
-    public FileSystemManagedLedgerOffloader(OffloadPoliciesImpl conf,
-                                            OrderedScheduler scheduler,
-                                            String testHDFSPath,
-                                            String baseDir,
-                                            LedgerOffloaderStats offloaderStats) throws IOException {
-        this.offloadPolicies = conf;
-        this.configuration = new Configuration();
-        this.configuration.set("fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem");
-        this.configuration.set("fs.defaultFS", testHDFSPath);
-        this.configuration.setClassLoader(FileSystemLedgerOffloaderFactory.class.getClassLoader());
-        this.driverName = conf.getManagedLedgerOffloadDriver();
-        this.configuration.set("hadoop.tmp.dir", baseDir);
-        this.storageBasePath = baseDir;
-        this.scheduler = scheduler;
-        this.fileSystem = FileSystem.get(configuration);
-        this.assignmentScheduler = OrderedScheduler.newSchedulerBuilder()
-                .numThreads(conf.getManagedLedgerOffloadMaxThreads())
-                .name("offload-assignment").build();
-        this.offloaderStats = offloaderStats;
-    }
-
-    @Override
-    public Map<String, String> getOffloadDriverMetadata() {
-        String path = storageBasePath == null ? "null" : storageBasePath;
-        return ImmutableMap.of(
-                STORAGE_BASE_PATH, path
-        );
-    }
-
-    /*
-    * ledgerMetadata stored in an index of -1
-    * */
-    @Override
-    public CompletableFuture<Void> offload(ReadHandle readHandle, UUID uuid, Map<String, String> extraMetadata) {
-        CompletableFuture<Void> promise = new CompletableFuture<>();
-        scheduler.chooseThread(readHandle.getId()).execute(
-                new LedgerReader(readHandle, uuid, extraMetadata, promise, storageBasePath, configuration,
-                        assignmentScheduler, offloadPolicies.getManagedLedgerOffloadPrefetchRounds(),
-                        this.offloaderStats));
-        return promise;
-    }
-
-    private static class LedgerReader implements Runnable {
-
-        private final ReadHandle readHandle;
-        private final UUID uuid;
-        private final Map<String, String> extraMetadata;
-        private final CompletableFuture<Void> promise;
-        private final String storageBasePath;
-        private final Configuration configuration;
-        volatile Exception fileSystemWriteException = null;
-        private OrderedScheduler assignmentScheduler;
-        private int managedLedgerOffloadPrefetchRounds = 1;
-        private final LedgerOffloaderStats offloaderStats;
-
-        private LedgerReader(ReadHandle readHandle,
-                             UUID uuid,
-                             Map<String, String> extraMetadata,
-                             CompletableFuture<Void> promise,
-                             String storageBasePath,
-                             Configuration configuration,
-                             OrderedScheduler assignmentScheduler,
-                             int managedLedgerOffloadPrefetchRounds,
-                             LedgerOffloaderStats offloaderStats) {
-            this.readHandle = readHandle;
-            this.uuid = uuid;
-            this.extraMetadata = extraMetadata;
-            this.promise = promise;
-            this.storageBasePath = storageBasePath;
-            this.configuration = configuration;
-            this.assignmentScheduler = assignmentScheduler;
-            this.managedLedgerOffloadPrefetchRounds = managedLedgerOffloadPrefetchRounds;
-            this.offloaderStats = offloaderStats;
-        }
-
-        @Override
-        public void run() {
-            if (!readHandle.isClosed() || readHandle.getLastAddConfirmed() < 0) {
-                promise.completeExceptionally(
-                        new IllegalArgumentException("An empty or open ledger should never be offloaded"));
-                return;
-            }
-            if (readHandle.getLength() <= 0) {
-                log.warn("Ledger [{}] has zero length, but it contains {} entries. "
-                    + " Attempting to offload ledger since it contains entries.", readHandle.getId(),
-                    readHandle.getLastAddConfirmed() + 1);
-            }
-            long ledgerId = readHandle.getId();
-            final String managedLedgerName = extraMetadata.get(MANAGED_LEDGER_NAME);
-            String storagePath = getStoragePath(storageBasePath, managedLedgerName);
-            String dataFilePath = getDataFilePath(storagePath, ledgerId, uuid);
-            final String topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
-            LongWritable key = new LongWritable();
-            BytesWritable value = new BytesWritable();
-            try {
-                MapFile.Writer dataWriter = new MapFile.Writer(configuration,
-                        new Path(dataFilePath),
-                        MapFile.Writer.keyClass(LongWritable.class),
-                        MapFile.Writer.valueClass(BytesWritable.class));
-                //store the ledgerMetadata in -1 index
-                key.set(METADATA_KEY_INDEX);
-                byte[] ledgerMetadata = buildLedgerMetadataFormat(readHandle.getLedgerMetadata());
-                value.set(ledgerMetadata, 0, ledgerMetadata.length);
-                dataWriter.append(key, value);
-                AtomicLong haveOffloadEntryNumber = new AtomicLong(0);
-                long needToOffloadFirstEntryNumber = 0;
-                CountDownLatch countDownLatch;
-                //avoid prefetch too much data into memory
-                Semaphore semaphore = new Semaphore(managedLedgerOffloadPrefetchRounds);
-                do {
-                    long end = Math.min(needToOffloadFirstEntryNumber + ENTRIES_PER_READ - 1,
-                            readHandle.getLastAddConfirmed());
-                    log.debug("read ledger entries. start: {}, end: {}", needToOffloadFirstEntryNumber, end);
-                    long startReadTime = System.nanoTime();
-                    LedgerEntries ledgerEntriesOnce = readHandle.readAsync(needToOffloadFirstEntryNumber, end).get();
-                    long cost = System.nanoTime() - startReadTime;
-                    this.offloaderStats.recordReadLedgerLatency(topicName, cost, TimeUnit.NANOSECONDS);
-                    semaphore.acquire();
-                    countDownLatch = new CountDownLatch(1);
-                    assignmentScheduler.chooseThread(ledgerId)
-                            .execute(FileSystemWriter.create(ledgerEntriesOnce,
-                                    dataWriter, semaphore, countDownLatch, haveOffloadEntryNumber, this));
-                    needToOffloadFirstEntryNumber = end + 1;
-                } while (needToOffloadFirstEntryNumber - 1 != readHandle.getLastAddConfirmed()
-                        && fileSystemWriteException == null);
-                countDownLatch.await();
-                if (fileSystemWriteException != null) {
-                    throw fileSystemWriteException;
-                }
-                IOUtils.closeStream(dataWriter);
-                promise.complete(null);
-            } catch (Exception e) {
-                log.error("Exception when get CompletableFuture<LedgerEntries> : ManagerLedgerName: {}, "
-                        + "LedgerId: {}, UUID: {} ", managedLedgerName, ledgerId, uuid, e);
-                if (e instanceof InterruptedException) {
-                    Thread.currentThread().interrupt();
-                }
-                this.offloaderStats.recordOffloadError(topicName);
-                promise.completeExceptionally(e);
-            }
-        }
-    }
-
-    private static class FileSystemWriter implements Runnable {
-
-        private LedgerEntries ledgerEntriesOnce;
-
-        private final LongWritable key = new LongWritable();
-        private final BytesWritable value = new BytesWritable();
-
-        private MapFile.Writer dataWriter;
-        private CountDownLatch countDownLatch;
-        private AtomicLong haveOffloadEntryNumber;
-        private LedgerReader ledgerReader;
-        private Semaphore semaphore;
-        private Recycler.Handle<FileSystemWriter> recyclerHandle;
-
-        private FileSystemWriter(Recycler.Handle<FileSystemWriter> recyclerHandle) {
-            this.recyclerHandle = recyclerHandle;
-        }
-
-        private static final Recycler<FileSystemWriter> RECYCLER = new Recycler<FileSystemWriter>() {
-            @Override
-            protected FileSystemWriter newObject(Recycler.Handle<FileSystemWriter> handle) {
-                return new FileSystemWriter(handle);
-            }
-        };
-
-        private void recycle() {
-            this.dataWriter = null;
-            this.countDownLatch = null;
-            this.haveOffloadEntryNumber = null;
-            this.ledgerReader = null;
-            this.ledgerEntriesOnce = null;
-            this.semaphore = null;
-            recyclerHandle.recycle(this);
-        }
-
-
-        public static FileSystemWriter create(LedgerEntries ledgerEntriesOnce,
-                                              MapFile.Writer dataWriter,
-                                              Semaphore semaphore,
-                                              CountDownLatch countDownLatch,
-                                              AtomicLong haveOffloadEntryNumber,
-                                              LedgerReader ledgerReader) {
-            FileSystemWriter writer = RECYCLER.get();
-            writer.ledgerReader = ledgerReader;
-            writer.dataWriter = dataWriter;
-            writer.countDownLatch = countDownLatch;
-            writer.haveOffloadEntryNumber = haveOffloadEntryNumber;
-            writer.ledgerEntriesOnce = ledgerEntriesOnce;
-            writer.semaphore = semaphore;
-            return writer;
-        }
-
-        @Override
-        public void run() {
-            String managedLedgerName = ledgerReader.extraMetadata.get(MANAGED_LEDGER_NAME);
-            String topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
-            if (ledgerReader.fileSystemWriteException == null) {
-                Iterator<LedgerEntry> iterator = ledgerEntriesOnce.iterator();
-                while (iterator.hasNext()) {
-                    LedgerEntry entry = iterator.next();
-                    long entryId = entry.getEntryId();
-                    key.set(entryId);
-                    byte[] currentEntryBytes;
-                    int currentEntrySize;
-                    try {
-                        currentEntryBytes = entry.getEntryBytes();
-                        currentEntrySize = currentEntryBytes.length;
-                        value.set(currentEntryBytes, 0, currentEntrySize);
-                        dataWriter.append(key, value);
-                    } catch (IOException e) {
-                        ledgerReader.fileSystemWriteException = e;
-                        ledgerReader.offloaderStats.recordWriteToStorageError(topicName);
-                        break;
-                    }
-                    haveOffloadEntryNumber.incrementAndGet();
-                    ledgerReader.offloaderStats.recordOffloadBytes(topicName, currentEntrySize);
-                }
-            }
-            countDownLatch.countDown();
-            ledgerEntriesOnce.close();
-            semaphore.release();
-            this.recycle();
-        }
-    }
-
-    @Override
-    public CompletableFuture<ReadHandle> readOffloaded(long ledgerId, UUID uuid,
-                                                       Map<String, String> offloadDriverMetadata) {
-
-        final String ledgerName = offloadDriverMetadata.get(MANAGED_LEDGER_NAME);
-        CompletableFuture<ReadHandle> promise = new CompletableFuture<>();
-        String storagePath = getStoragePath(storageBasePath, ledgerName);
-        String dataFilePath = getDataFilePath(storagePath, ledgerId, uuid);
-        scheduler.chooseThread(ledgerId).execute(() -> {
-            try {
-                MapFile.Reader reader = new MapFile.Reader(new Path(dataFilePath),
-                        configuration);
-                promise.complete(FileStoreBackedReadHandleImpl.open(
-                        scheduler.chooseThread(ledgerId), reader, ledgerId, this.offloaderStats, ledgerName));
-            } catch (Throwable t) {
-                log.error("Failed to open FileStoreBackedReadHandleImpl: ManagerLedgerName: {}, "
-                        + "LegerId: {}, UUID: {}", ledgerName, ledgerId, uuid, t);
-                promise.completeExceptionally(t);
-            }
-        });
-        return promise;
-    }
-
-    private static String getStoragePath(String storageBasePath, String managedLedgerName) {
-        return storageBasePath == null ? managedLedgerName + "/" : storageBasePath + "/" + managedLedgerName + "/";
-    }
-
-    private static String getDataFilePath(String storagePath, long ledgerId, UUID uuid) {
-        return storagePath + ledgerId + "-" + uuid.toString();
-    }
-
-    @Override
-    public CompletableFuture<Void> deleteOffloaded(long ledgerId, UUID uid, Map<String, String> offloadDriverMetadata) {
-        String ledgerName = offloadDriverMetadata.get(MANAGED_LEDGER_NAME);
-        String storagePath = getStoragePath(storageBasePath, ledgerName);
-        String dataFilePath = getDataFilePath(storagePath, ledgerId, uid);
-        String topicName = TopicName.fromPersistenceNamingEncoding(ledgerName);
-        CompletableFuture<Void> promise = new CompletableFuture<>();
-        try {
-            fileSystem.delete(new Path(dataFilePath), true);
-            promise.complete(null);
-        } catch (IOException e) {
-            log.error("Failed to delete Offloaded: ", e);
-            promise.completeExceptionally(e);
-        }
-        return promise.whenComplete((__, t) ->
-                this.offloaderStats.recordDeleteOffloadOps(topicName, t == null));
-    }
-
-    @Override
-    public OffloadPolicies getOffloadPolicies() {
-        return offloadPolicies;
-    }
-
-    @Override
-    public void close() {
-        if (fileSystem != null) {
-            try {
-                fileSystem.close();
-            } catch (Exception e) {
-                log.error("FileSystemManagedLedgerOffloader close failed!", e);
-            }
-        }
-        if (assignmentScheduler != null) {
-            MoreExecutors.shutdownAndAwaitTermination(assignmentScheduler, 5, TimeUnit.SECONDS);
-        }
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.filesystem.impl;
+
+import static org.apache.bookkeeper.mledger.offload.OffloadUtils.buildLedgerMetadataFormat;
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.util.concurrent.MoreExecutors;
+import io.netty.util.Recycler;
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicLong;
+import org.apache.bookkeeper.client.api.LedgerEntries;
+import org.apache.bookkeeper.client.api.LedgerEntry;
+import org.apache.bookkeeper.client.api.ReadHandle;
+import org.apache.bookkeeper.common.util.OrderedScheduler;
+import org.apache.bookkeeper.mledger.LedgerOffloader;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.bookkeeper.mledger.offload.filesystem.FileSystemLedgerOffloaderFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.MapFile;
+import org.apache.pulsar.common.naming.TopicName;
+import org.apache.pulsar.common.policies.data.OffloadPolicies;
+import org.apache.pulsar.common.policies.data.OffloadPoliciesImpl;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class FileSystemManagedLedgerOffloader implements LedgerOffloader {
+
+    private static final Logger log = LoggerFactory.getLogger(FileSystemManagedLedgerOffloader.class);
+    private static final String STORAGE_BASE_PATH = "storageBasePath";
+    private static final String DRIVER_NAMES = "filesystem";
+    private static final String MANAGED_LEDGER_NAME = "ManagedLedgerName";
+    static final long METADATA_KEY_INDEX = -1;
+    private final Configuration configuration;
+    private final String driverName;
+    private final String storageBasePath;
+    private final FileSystem fileSystem;
+    private OrderedScheduler scheduler;
+    private static final long ENTRIES_PER_READ = 100;
+    private OrderedScheduler assignmentScheduler;
+    private OffloadPolicies offloadPolicies;
+    private final LedgerOffloaderStats offloaderStats;
+
+    public static boolean driverSupported(String driver) {
+        return DRIVER_NAMES.equals(driver);
+    }
+
+    @Override
+    public String getOffloadDriverName() {
+        return driverName;
+    }
+
+    public static FileSystemManagedLedgerOffloader create(OffloadPoliciesImpl conf,
+                                                          OrderedScheduler scheduler,
+                                                          LedgerOffloaderStats offloaderStats) throws IOException {
+        return new FileSystemManagedLedgerOffloader(conf, scheduler, offloaderStats);
+    }
+
+    private FileSystemManagedLedgerOffloader(OffloadPoliciesImpl conf, OrderedScheduler scheduler,
+                                             LedgerOffloaderStats offloaderStats) throws IOException {
+        this.offloadPolicies = conf;
+        this.configuration = new Configuration();
+        if (conf.getFileSystemProfilePath() != null) {
+            String[] paths = conf.getFileSystemProfilePath().split(",");
+            for (int i = 0; i < paths.length; i++) {
+                configuration.addResource(new Path(paths[i]));
+            }
+        }
+        if (!"".equals(conf.getFileSystemURI()) && conf.getFileSystemURI() != null) {
+            configuration.set("fs.defaultFS", conf.getFileSystemURI());
+        }
+        if (configuration.get("fs.hdfs.impl") == null) {
+            this.configuration.set("fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem");
+        }
+
+        if (configuration.get("fs.file.impl") == null) {
+            this.configuration.set("fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem");
+        }
+
+        this.configuration.setClassLoader(FileSystemLedgerOffloaderFactory.class.getClassLoader());
+        this.driverName = conf.getManagedLedgerOffloadDriver();
+        this.storageBasePath = configuration.get("fs.defaultFS");
+        this.scheduler = scheduler;
+        this.fileSystem = FileSystem.get(configuration);
+        this.assignmentScheduler = OrderedScheduler.newSchedulerBuilder()
+                .numThreads(conf.getManagedLedgerOffloadMaxThreads())
+                .name("offload-assignment").build();
+        this.offloaderStats = offloaderStats;
+    }
+
+    @VisibleForTesting
+    public FileSystemManagedLedgerOffloader(OffloadPoliciesImpl conf,
+                                            OrderedScheduler scheduler,
+                                            String testHDFSPath,
+                                            String baseDir,
+                                            LedgerOffloaderStats offloaderStats) throws IOException {
+        this.offloadPolicies = conf;
+        this.configuration = new Configuration();
+        this.configuration.set("fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem");
+        this.configuration.set("fs.defaultFS", testHDFSPath);
+        this.configuration.setClassLoader(FileSystemLedgerOffloaderFactory.class.getClassLoader());
+        this.driverName = conf.getManagedLedgerOffloadDriver();
+        this.configuration.set("hadoop.tmp.dir", baseDir);
+        this.storageBasePath = baseDir;
+        this.scheduler = scheduler;
+        this.fileSystem = FileSystem.get(configuration);
+        this.assignmentScheduler = OrderedScheduler.newSchedulerBuilder()
+                .numThreads(conf.getManagedLedgerOffloadMaxThreads())
+                .name("offload-assignment").build();
+        this.offloaderStats = offloaderStats;
+    }
+
+    @Override
+    public Map<String, String> getOffloadDriverMetadata() {
+        String path = storageBasePath == null ? "null" : storageBasePath;
+        return ImmutableMap.of(
+                STORAGE_BASE_PATH, path
+        );
+    }
+
+    /*
+    * ledgerMetadata stored in an index of -1
+    * */
+    @Override
+    public CompletableFuture<Void> offload(ReadHandle readHandle, UUID uuid, Map<String, String> extraMetadata) {
+        CompletableFuture<Void> promise = new CompletableFuture<>();
+        scheduler.chooseThread(readHandle.getId()).execute(
+                new LedgerReader(readHandle, uuid, extraMetadata, promise, storageBasePath, configuration,
+                        assignmentScheduler, offloadPolicies.getManagedLedgerOffloadPrefetchRounds(),
+                        this.offloaderStats));
+        return promise;
+    }
+
+    private static class LedgerReader implements Runnable {
+
+        private final ReadHandle readHandle;
+        private final UUID uuid;
+        private final Map<String, String> extraMetadata;
+        private final CompletableFuture<Void> promise;
+        private final String storageBasePath;
+        private final Configuration configuration;
+        volatile Exception fileSystemWriteException = null;
+        private OrderedScheduler assignmentScheduler;
+        private int managedLedgerOffloadPrefetchRounds = 1;
+        private final LedgerOffloaderStats offloaderStats;
+
+        private LedgerReader(ReadHandle readHandle,
+                             UUID uuid,
+                             Map<String, String> extraMetadata,
+                             CompletableFuture<Void> promise,
+                             String storageBasePath,
+                             Configuration configuration,
+                             OrderedScheduler assignmentScheduler,
+                             int managedLedgerOffloadPrefetchRounds,
+                             LedgerOffloaderStats offloaderStats) {
+            this.readHandle = readHandle;
+            this.uuid = uuid;
+            this.extraMetadata = extraMetadata;
+            this.promise = promise;
+            this.storageBasePath = storageBasePath;
+            this.configuration = configuration;
+            this.assignmentScheduler = assignmentScheduler;
+            this.managedLedgerOffloadPrefetchRounds = managedLedgerOffloadPrefetchRounds;
+            this.offloaderStats = offloaderStats;
+        }
+
+        @Override
+        public void run() {
+            if (!readHandle.isClosed() || readHandle.getLastAddConfirmed() < 0) {
+                promise.completeExceptionally(
+                        new IllegalArgumentException("An empty or open ledger should never be offloaded"));
+                return;
+            }
+            if (readHandle.getLength() <= 0) {
+                log.warn("Ledger [{}] has zero length, but it contains {} entries. "
+                    + " Attempting to offload ledger since it contains entries.", readHandle.getId(),
+                    readHandle.getLastAddConfirmed() + 1);
+            }
+            long ledgerId = readHandle.getId();
+            final String managedLedgerName = extraMetadata.get(MANAGED_LEDGER_NAME);
+            String storagePath = getStoragePath(storageBasePath, managedLedgerName);
+            String dataFilePath = getDataFilePath(storagePath, ledgerId, uuid);
+            final String topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
+            LongWritable key = new LongWritable();
+            BytesWritable value = new BytesWritable();
+            try {
+                MapFile.Writer dataWriter = new MapFile.Writer(configuration,
+                        new Path(dataFilePath),
+                        MapFile.Writer.keyClass(LongWritable.class),
+                        MapFile.Writer.valueClass(BytesWritable.class));
+                //store the ledgerMetadata in -1 index
+                key.set(METADATA_KEY_INDEX);
+                byte[] ledgerMetadata = buildLedgerMetadataFormat(readHandle.getLedgerMetadata());
+                value.set(ledgerMetadata, 0, ledgerMetadata.length);
+                dataWriter.append(key, value);
+                AtomicLong haveOffloadEntryNumber = new AtomicLong(0);
+                long needToOffloadFirstEntryNumber = 0;
+                CountDownLatch countDownLatch;
+                //avoid prefetch too much data into memory
+                Semaphore semaphore = new Semaphore(managedLedgerOffloadPrefetchRounds);
+                do {
+                    long end = Math.min(needToOffloadFirstEntryNumber + ENTRIES_PER_READ - 1,
+                            readHandle.getLastAddConfirmed());
+                    log.debug("read ledger entries. start: {}, end: {}", needToOffloadFirstEntryNumber, end);
+                    long startReadTime = System.nanoTime();
+                    LedgerEntries ledgerEntriesOnce = readHandle.readAsync(needToOffloadFirstEntryNumber, end).get();
+                    long cost = System.nanoTime() - startReadTime;
+                    this.offloaderStats.recordReadLedgerLatency(topicName, cost, TimeUnit.NANOSECONDS);
+                    semaphore.acquire();
+                    countDownLatch = new CountDownLatch(1);
+                    assignmentScheduler.chooseThread(ledgerId)
+                            .execute(FileSystemWriter.create(ledgerEntriesOnce,
+                                    dataWriter, semaphore, countDownLatch, haveOffloadEntryNumber, this));
+                    needToOffloadFirstEntryNumber = end + 1;
+                } while (needToOffloadFirstEntryNumber - 1 != readHandle.getLastAddConfirmed()
+                        && fileSystemWriteException == null);
+                countDownLatch.await();
+                if (fileSystemWriteException != null) {
+                    throw fileSystemWriteException;
+                }
+                IOUtils.closeStream(dataWriter);
+                promise.complete(null);
+            } catch (Exception e) {
+                log.error("Exception when get CompletableFuture<LedgerEntries> : ManagerLedgerName: {}, "
+                        + "LedgerId: {}, UUID: {} ", managedLedgerName, ledgerId, uuid, e);
+                if (e instanceof InterruptedException) {
+                    Thread.currentThread().interrupt();
+                }
+                this.offloaderStats.recordOffloadError(topicName);
+                promise.completeExceptionally(e);
+            }
+        }
+    }
+
+    private static class FileSystemWriter implements Runnable {
+
+        private LedgerEntries ledgerEntriesOnce;
+
+        private final LongWritable key = new LongWritable();
+        private final BytesWritable value = new BytesWritable();
+
+        private MapFile.Writer dataWriter;
+        private CountDownLatch countDownLatch;
+        private AtomicLong haveOffloadEntryNumber;
+        private LedgerReader ledgerReader;
+        private Semaphore semaphore;
+        private Recycler.Handle<FileSystemWriter> recyclerHandle;
+
+        private FileSystemWriter(Recycler.Handle<FileSystemWriter> recyclerHandle) {
+            this.recyclerHandle = recyclerHandle;
+        }
+
+        private static final Recycler<FileSystemWriter> RECYCLER = new Recycler<FileSystemWriter>() {
+            @Override
+            protected FileSystemWriter newObject(Recycler.Handle<FileSystemWriter> handle) {
+                return new FileSystemWriter(handle);
+            }
+        };
+
+        private void recycle() {
+            this.dataWriter = null;
+            this.countDownLatch = null;
+            this.haveOffloadEntryNumber = null;
+            this.ledgerReader = null;
+            this.ledgerEntriesOnce = null;
+            this.semaphore = null;
+            recyclerHandle.recycle(this);
+        }
+
+
+        public static FileSystemWriter create(LedgerEntries ledgerEntriesOnce,
+                                              MapFile.Writer dataWriter,
+                                              Semaphore semaphore,
+                                              CountDownLatch countDownLatch,
+                                              AtomicLong haveOffloadEntryNumber,
+                                              LedgerReader ledgerReader) {
+            FileSystemWriter writer = RECYCLER.get();
+            writer.ledgerReader = ledgerReader;
+            writer.dataWriter = dataWriter;
+            writer.countDownLatch = countDownLatch;
+            writer.haveOffloadEntryNumber = haveOffloadEntryNumber;
+            writer.ledgerEntriesOnce = ledgerEntriesOnce;
+            writer.semaphore = semaphore;
+            return writer;
+        }
+
+        @Override
+        public void run() {
+            String managedLedgerName = ledgerReader.extraMetadata.get(MANAGED_LEDGER_NAME);
+            String topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
+            if (ledgerReader.fileSystemWriteException == null) {
+                Iterator<LedgerEntry> iterator = ledgerEntriesOnce.iterator();
+                while (iterator.hasNext()) {
+                    LedgerEntry entry = iterator.next();
+                    long entryId = entry.getEntryId();
+                    key.set(entryId);
+                    byte[] currentEntryBytes;
+                    int currentEntrySize;
+                    try {
+                        currentEntryBytes = entry.getEntryBytes();
+                        currentEntrySize = currentEntryBytes.length;
+                        value.set(currentEntryBytes, 0, currentEntrySize);
+                        dataWriter.append(key, value);
+                    } catch (IOException e) {
+                        ledgerReader.fileSystemWriteException = e;
+                        ledgerReader.offloaderStats.recordWriteToStorageError(topicName);
+                        break;
+                    }
+                    haveOffloadEntryNumber.incrementAndGet();
+                    ledgerReader.offloaderStats.recordOffloadBytes(topicName, currentEntrySize);
+                }
+            }
+            countDownLatch.countDown();
+            ledgerEntriesOnce.close();
+            semaphore.release();
+            this.recycle();
+        }
+    }
+
+    @Override
+    public CompletableFuture<ReadHandle> readOffloaded(long ledgerId, UUID uuid,
+                                                       Map<String, String> offloadDriverMetadata) {
+
+        final String ledgerName = offloadDriverMetadata.get(MANAGED_LEDGER_NAME);
+        CompletableFuture<ReadHandle> promise = new CompletableFuture<>();
+        String storagePath = getStoragePath(storageBasePath, ledgerName);
+        String dataFilePath = getDataFilePath(storagePath, ledgerId, uuid);
+        scheduler.chooseThread(ledgerId).execute(() -> {
+            try {
+                MapFile.Reader reader = new MapFile.Reader(new Path(dataFilePath),
+                        configuration);
+                promise.complete(FileStoreBackedReadHandleImpl.open(
+                        scheduler.chooseThread(ledgerId), reader, ledgerId, this.offloaderStats, ledgerName));
+            } catch (Throwable t) {
+                log.error("Failed to open FileStoreBackedReadHandleImpl: ManagerLedgerName: {}, "
+                        + "LegerId: {}, UUID: {}", ledgerName, ledgerId, uuid, t);
+                promise.completeExceptionally(t);
+            }
+        });
+        return promise;
+    }
+
+    private static String getStoragePath(String storageBasePath, String managedLedgerName) {
+        return storageBasePath == null ? managedLedgerName + "/" : storageBasePath + "/" + managedLedgerName + "/";
+    }
+
+    private static String getDataFilePath(String storagePath, long ledgerId, UUID uuid) {
+        return storagePath + ledgerId + "-" + uuid.toString();
+    }
+
+    @Override
+    public CompletableFuture<Void> deleteOffloaded(long ledgerId, UUID uid, Map<String, String> offloadDriverMetadata) {
+        String ledgerName = offloadDriverMetadata.get(MANAGED_LEDGER_NAME);
+        String storagePath = getStoragePath(storageBasePath, ledgerName);
+        String dataFilePath = getDataFilePath(storagePath, ledgerId, uid);
+        String topicName = TopicName.fromPersistenceNamingEncoding(ledgerName);
+        CompletableFuture<Void> promise = new CompletableFuture<>();
+        try {
+            fileSystem.delete(new Path(dataFilePath), true);
+            promise.complete(null);
+        } catch (IOException e) {
+            log.error("Failed to delete Offloaded: ", e);
+            promise.completeExceptionally(e);
+        }
+        return promise.whenComplete((__, t) ->
+                this.offloaderStats.recordDeleteOffloadOps(topicName, t == null));
+    }
+
+    @Override
+    public OffloadPolicies getOffloadPolicies() {
+        return offloadPolicies;
+    }
+
+    @Override
+    public void close() {
+        if (fileSystem != null) {
+            try {
+                fileSystem.close();
+            } catch (Exception e) {
+                log.error("FileSystemManagedLedgerOffloader close failed!", e);
+            }
+        }
+        if (assignmentScheduler != null) {
+            MoreExecutors.shutdownAndAwaitTermination(assignmentScheduler, 5, TimeUnit.SECONDS);
+        }
+    }
+}
diff --git a/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/package-info.java b/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/package-info.java
index 14f75f5635..8faaa2656b 100644
--- a/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/package-info.java
+++ b/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/package-info.java
@@ -1,19 +1,19 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.filesystem.impl;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.filesystem.impl;
diff --git a/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/package-info.java b/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/package-info.java
index a41f5206f7..a43fd34f5f 100644
--- a/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/package-info.java
+++ b/tiered-storage/file-system/src/main/java/org/apache/bookkeeper/mledger/offload/filesystem/package-info.java
@@ -1,19 +1,19 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.filesystem;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.filesystem;
diff --git a/tiered-storage/file-system/src/main/resources/META-INF/services/pulsar-offloader.yaml b/tiered-storage/file-system/src/main/resources/META-INF/services/pulsar-offloader.yaml
index d7f7ed2d26..cf0e0bdafe 100644
--- a/tiered-storage/file-system/src/main/resources/META-INF/services/pulsar-offloader.yaml
+++ b/tiered-storage/file-system/src/main/resources/META-INF/services/pulsar-offloader.yaml
@@ -1,22 +1,22 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-#
-
-name: filesystem
-description: fileSystem based offloader implementation
-offloaderFactoryClass: org.apache.bookkeeper.mledger.offload.filesystem.FileSystemLedgerOffloaderFactory
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+name: filesystem
+description: fileSystem based offloader implementation
+offloaderFactoryClass: org.apache.bookkeeper.mledger.offload.filesystem.FileSystemLedgerOffloaderFactory
diff --git a/tiered-storage/file-system/src/main/resources/findbugsExclude.xml b/tiered-storage/file-system/src/main/resources/findbugsExclude.xml
index 051f0e68e2..0d6bda166e 100644
--- a/tiered-storage/file-system/src/main/resources/findbugsExclude.xml
+++ b/tiered-storage/file-system/src/main/resources/findbugsExclude.xml
@@ -1,38 +1,38 @@
-<!--
-
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-
--->
-<FindBugsFilter>
-  <!-- Ignore violations that were present when the rule was enabled -->
-  <Match>
-    <Class name="org.apache.bookkeeper.mledger.offload.filesystem.impl.FileSystemManagedLedgerOffloader"/>
-    <Method name="getOffloadPolicies"/>
-    <Bug pattern="EI_EXPOSE_REP"/>
-  </Match>
-  <Match>
-    <Class name="org.apache.bookkeeper.mledger.offload.filesystem.impl.FileSystemManagedLedgerOffloader"/>
-    <Method name="&lt;init&gt;"/>
-    <Bug pattern="EI_EXPOSE_REP2"/>
-  </Match>
-  <Match>
-    <Class name="org.apache.bookkeeper.mledger.offload.filesystem.impl.FileSystemManagedLedgerOffloader"/>
-    <Method name="&lt;init&gt;"/>
-    <Bug pattern="EI_EXPOSE_REP2"/>
-  </Match>
-</FindBugsFilter>
+<!--
+
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+
+-->
+<FindBugsFilter>
+  <!-- Ignore violations that were present when the rule was enabled -->
+  <Match>
+    <Class name="org.apache.bookkeeper.mledger.offload.filesystem.impl.FileSystemManagedLedgerOffloader"/>
+    <Method name="getOffloadPolicies"/>
+    <Bug pattern="EI_EXPOSE_REP"/>
+  </Match>
+  <Match>
+    <Class name="org.apache.bookkeeper.mledger.offload.filesystem.impl.FileSystemManagedLedgerOffloader"/>
+    <Method name="&lt;init&gt;"/>
+    <Bug pattern="EI_EXPOSE_REP2"/>
+  </Match>
+  <Match>
+    <Class name="org.apache.bookkeeper.mledger.offload.filesystem.impl.FileSystemManagedLedgerOffloader"/>
+    <Method name="&lt;init&gt;"/>
+    <Bug pattern="EI_EXPOSE_REP2"/>
+  </Match>
+</FindBugsFilter>
diff --git a/tiered-storage/file-system/src/test/java/org/apache/bookkeeper/mledger/offload/filesystem/FileStoreTestBase.java b/tiered-storage/file-system/src/test/java/org/apache/bookkeeper/mledger/offload/filesystem/FileStoreTestBase.java
index d9d7b8cf5e..543a8576b7 100644
--- a/tiered-storage/file-system/src/test/java/org/apache/bookkeeper/mledger/offload/filesystem/FileStoreTestBase.java
+++ b/tiered-storage/file-system/src/test/java/org/apache/bookkeeper/mledger/offload/filesystem/FileStoreTestBase.java
@@ -1,109 +1,109 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.filesystem;
-
-import java.io.File;
-import java.io.IOException;
-import java.nio.file.Files;
-import java.util.Properties;
-import java.util.concurrent.Executors;
-import java.util.concurrent.ScheduledExecutorService;
-import org.apache.bookkeeper.common.util.OrderedScheduler;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.bookkeeper.mledger.offload.filesystem.impl.FileSystemManagedLedgerOffloader;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.pulsar.common.policies.data.OffloadPoliciesImpl;
-import org.testng.annotations.AfterClass;
-import org.testng.annotations.AfterMethod;
-import org.testng.annotations.BeforeClass;
-import org.testng.annotations.BeforeMethod;
-
-public abstract class FileStoreTestBase {
-    protected FileSystemManagedLedgerOffloader fileSystemManagedLedgerOffloader;
-    protected OrderedScheduler scheduler;
-    protected final String basePath = "pulsar";
-    private MiniDFSCluster hdfsCluster;
-    private String hdfsURI;
-    protected LedgerOffloaderStats offloaderStats;
-    private ScheduledExecutorService scheduledExecutorService;
-
-    @BeforeClass(alwaysRun = true)
-    public final void beforeClass() throws Exception {
-        init();
-    }
-
-    public void init() throws Exception {
-        scheduler = OrderedScheduler.newSchedulerBuilder().numThreads(1).name("offloader").build();
-    }
-
-    @AfterClass(alwaysRun = true)
-    public final void afterClass() throws IOException {
-        cleanup();
-    }
-
-    public void cleanup() throws IOException {
-        if (scheduler != null) {
-            scheduler.shutdownNow();
-            scheduler = null;
-        }
-    }
-
-    @BeforeMethod(alwaysRun = true)
-    public void start() throws Exception {
-        File baseDir = Files.createTempDirectory(basePath).toFile().getAbsoluteFile();
-        Configuration conf = new Configuration();
-        conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());
-        conf.set("dfs.namenode.gc.time.monitor.enable", "false");
-        MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(conf);
-        hdfsCluster = builder.build();
-
-        hdfsURI = "hdfs://localhost:" + hdfsCluster.getNameNodePort() + "/";
-        Properties properties = new Properties();
-        scheduledExecutorService = Executors.newScheduledThreadPool(1);
-        this.offloaderStats = LedgerOffloaderStats.create(true, true, scheduledExecutorService, 60);
-        fileSystemManagedLedgerOffloader = new FileSystemManagedLedgerOffloader(
-                OffloadPoliciesImpl.create(properties),
-                scheduler, hdfsURI, basePath, offloaderStats);
-    }
-
-    @AfterMethod(alwaysRun = true)
-    public void tearDown() throws Exception {
-        if (fileSystemManagedLedgerOffloader != null) {
-            fileSystemManagedLedgerOffloader.close();
-            fileSystemManagedLedgerOffloader = null;
-        }
-        if (offloaderStats != null) {
-            offloaderStats.close();
-            offloaderStats = null;
-        }
-        if (hdfsCluster != null) {
-            hdfsCluster.shutdown(true, true);
-            hdfsCluster = null;
-        }
-        if (scheduledExecutorService != null) {
-            scheduledExecutorService.shutdownNow();
-            scheduledExecutorService = null;
-        }
-    }
-
-    public String getURI() {
-        return hdfsURI;
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.filesystem;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.util.Properties;
+import java.util.concurrent.Executors;
+import java.util.concurrent.ScheduledExecutorService;
+import org.apache.bookkeeper.common.util.OrderedScheduler;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.bookkeeper.mledger.offload.filesystem.impl.FileSystemManagedLedgerOffloader;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.pulsar.common.policies.data.OffloadPoliciesImpl;
+import org.testng.annotations.AfterClass;
+import org.testng.annotations.AfterMethod;
+import org.testng.annotations.BeforeClass;
+import org.testng.annotations.BeforeMethod;
+
+public abstract class FileStoreTestBase {
+    protected FileSystemManagedLedgerOffloader fileSystemManagedLedgerOffloader;
+    protected OrderedScheduler scheduler;
+    protected final String basePath = "pulsar";
+    private MiniDFSCluster hdfsCluster;
+    private String hdfsURI;
+    protected LedgerOffloaderStats offloaderStats;
+    private ScheduledExecutorService scheduledExecutorService;
+
+    @BeforeClass(alwaysRun = true)
+    public final void beforeClass() throws Exception {
+        init();
+    }
+
+    public void init() throws Exception {
+        scheduler = OrderedScheduler.newSchedulerBuilder().numThreads(1).name("offloader").build();
+    }
+
+    @AfterClass(alwaysRun = true)
+    public final void afterClass() throws IOException {
+        cleanup();
+    }
+
+    public void cleanup() throws IOException {
+        if (scheduler != null) {
+            scheduler.shutdownNow();
+            scheduler = null;
+        }
+    }
+
+    @BeforeMethod(alwaysRun = true)
+    public void start() throws Exception {
+        File baseDir = Files.createTempDirectory(basePath).toFile().getAbsoluteFile();
+        Configuration conf = new Configuration();
+        conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());
+        conf.set("dfs.namenode.gc.time.monitor.enable", "false");
+        MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(conf);
+        hdfsCluster = builder.build();
+
+        hdfsURI = "hdfs://localhost:" + hdfsCluster.getNameNodePort() + "/";
+        Properties properties = new Properties();
+        scheduledExecutorService = Executors.newScheduledThreadPool(1);
+        this.offloaderStats = LedgerOffloaderStats.create(true, true, scheduledExecutorService, 60);
+        fileSystemManagedLedgerOffloader = new FileSystemManagedLedgerOffloader(
+                OffloadPoliciesImpl.create(properties),
+                scheduler, hdfsURI, basePath, offloaderStats);
+    }
+
+    @AfterMethod(alwaysRun = true)
+    public void tearDown() throws Exception {
+        if (fileSystemManagedLedgerOffloader != null) {
+            fileSystemManagedLedgerOffloader.close();
+            fileSystemManagedLedgerOffloader = null;
+        }
+        if (offloaderStats != null) {
+            offloaderStats.close();
+            offloaderStats = null;
+        }
+        if (hdfsCluster != null) {
+            hdfsCluster.shutdown(true, true);
+            hdfsCluster = null;
+        }
+        if (scheduledExecutorService != null) {
+            scheduledExecutorService.shutdownNow();
+            scheduledExecutorService = null;
+        }
+    }
+
+    public String getURI() {
+        return hdfsURI;
+    }
+}
diff --git a/tiered-storage/file-system/src/test/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileSystemManagedLedgerOffloaderTest.java b/tiered-storage/file-system/src/test/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileSystemManagedLedgerOffloaderTest.java
index 210b577ae9..b4b9c02f30 100644
--- a/tiered-storage/file-system/src/test/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileSystemManagedLedgerOffloaderTest.java
+++ b/tiered-storage/file-system/src/test/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileSystemManagedLedgerOffloaderTest.java
@@ -1,197 +1,197 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.filesystem.impl;
-
-
-import static org.testng.Assert.assertEquals;
-import static org.testng.Assert.assertFalse;
-import static org.testng.Assert.assertTrue;
-import java.io.IOException;
-import java.net.URI;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.UUID;
-import lombok.Cleanup;
-import org.apache.bookkeeper.client.BookKeeper;
-import org.apache.bookkeeper.client.LedgerHandle;
-import org.apache.bookkeeper.client.PulsarMockBookKeeper;
-import org.apache.bookkeeper.client.api.DigestType;
-import org.apache.bookkeeper.client.api.LedgerEntries;
-import org.apache.bookkeeper.client.api.LedgerEntry;
-import org.apache.bookkeeper.client.api.ReadHandle;
-import org.apache.bookkeeper.mledger.LedgerOffloader;
-import org.apache.bookkeeper.mledger.impl.LedgerOffloaderStatsImpl;
-import org.apache.bookkeeper.mledger.offload.filesystem.FileStoreTestBase;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.pulsar.common.naming.TopicName;
-import org.testng.annotations.AfterMethod;
-import org.testng.annotations.BeforeMethod;
-import org.testng.annotations.Test;
-
-public class FileSystemManagedLedgerOffloaderTest extends FileStoreTestBase {
-    private PulsarMockBookKeeper bk;
-    private String managedLedgerName = "public/default/persistent/testOffload";
-    private String topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
-    private String storagePath = createStoragePath(managedLedgerName);
-    private LedgerHandle lh;
-    private ReadHandle toWrite;
-    private final int numberOfEntries = 601;
-    private  Map<String, String> map = new HashMap<>();
-
-    @Override
-    public void init() throws Exception {
-        super.init();
-        this.bk = new PulsarMockBookKeeper(scheduler);
-        this.toWrite = buildReadHandle();
-        map.put("ManagedLedgerName", managedLedgerName);
-    }
-
-    @Override
-    public void cleanup() throws IOException {
-        if (bk != null) {
-            bk.shutdown();
-        }
-        super.cleanup();
-    }
-
-    private ReadHandle buildReadHandle() throws Exception {
-
-        lh = bk.createLedger(1, 1, 1, BookKeeper.DigestType.CRC32, "foobar".getBytes());
-
-        int i = 0;
-        int blocksWritten = 1;
-        while (blocksWritten <= numberOfEntries) {
-            byte[] entry = ("foobar" + i).getBytes();
-            blocksWritten++;
-            lh.addEntry(entry);
-            i++;
-        }
-        lh.close();
-
-        return bk.newOpenLedgerOp().withLedgerId(lh.getId())
-                .withPassword("foobar".getBytes()).withDigestType(DigestType.CRC32).execute().get();
-    }
-
-    @BeforeMethod(alwaysRun = true)
-    @Override
-    public void start() throws Exception {
-        super.start();
-    }
-
-    @AfterMethod(alwaysRun = true)
-    @Override
-    public void tearDown() throws Exception {
-        super.tearDown();
-    }
-
-    @Test
-    public void testOffloadAndRead() throws Exception {
-        LedgerOffloader offloader = fileSystemManagedLedgerOffloader;
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, map).get();
-        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, map).get();
-        assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
-        LedgerEntries toTestEntries = toTest.read(0, numberOfEntries - 1);
-        LedgerEntries toWriteEntries = toWrite.read(0, numberOfEntries - 1);
-        Iterator<LedgerEntry> toTestIter = toTestEntries.iterator();
-        Iterator<LedgerEntry> toWriteIter = toWriteEntries.iterator();
-        while (toTestIter.hasNext()) {
-            LedgerEntry toWriteEntry = toWriteIter.next();
-            LedgerEntry toTestEntry = toTestIter.next();
-
-            assertEquals(toWriteEntry.getLedgerId(), toTestEntry.getLedgerId());
-            assertEquals(toWriteEntry.getEntryId(), toTestEntry.getEntryId());
-            assertEquals(toWriteEntry.getLength(), toTestEntry.getLength());
-            assertEquals(toWriteEntry.getEntryBuffer(), toTestEntry.getEntryBuffer());
-        }
-        toTestEntries.close();
-        toWriteEntries.close();
-        toTestEntries = toTest.read(1, numberOfEntries - 1);
-        toWriteEntries = toWrite.read(1, numberOfEntries - 1);
-        toTestIter = toTestEntries.iterator();
-        toWriteIter = toWriteEntries.iterator();
-        while (toTestIter.hasNext()) {
-            LedgerEntry toWriteEntry = toWriteIter.next();
-            LedgerEntry toTestEntry = toTestIter.next();
-
-            assertEquals(toWriteEntry.getLedgerId(), toTestEntry.getLedgerId());
-            assertEquals(toWriteEntry.getEntryId(), toTestEntry.getEntryId());
-            assertEquals(toWriteEntry.getLength(), toTestEntry.getLength());
-            assertEquals(toWriteEntry.getEntryBuffer(), toTestEntry.getEntryBuffer());
-        }
-        toTestEntries.close();
-        toWriteEntries.close();
-    }
-
-    @Test
-    public void testOffloadAndReadMetrics() throws Exception {
-        LedgerOffloader offloader = fileSystemManagedLedgerOffloader;
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, map).get();
-
-        LedgerOffloaderStatsImpl offloaderStats = (LedgerOffloaderStatsImpl) this.offloaderStats;
-        assertTrue(offloaderStats.getOffloadError(topicName) == 0);
-        assertTrue(offloaderStats.getOffloadBytes(topicName) > 0);
-        assertTrue(offloaderStats.getReadLedgerLatency(topicName).count > 0);
-        assertTrue(offloaderStats.getWriteStorageError(topicName) == 0);
-
-        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, map).get();
-        LedgerEntries toTestEntries = toTest.read(0, numberOfEntries - 1);
-        Iterator<LedgerEntry> toTestIter = toTestEntries.iterator();
-        while (toTestIter.hasNext()) {
-            LedgerEntry toTestEntry = toTestIter.next();
-        }
-        toTestEntries.close();
-
-        assertTrue(offloaderStats.getReadOffloadError(topicName) == 0);
-        assertTrue(offloaderStats.getReadOffloadBytes(topicName) > 0);
-        assertTrue(offloaderStats.getReadOffloadDataLatency(topicName).count > 0);
-        assertTrue(offloaderStats.getReadOffloadIndexLatency(topicName).count > 0);
-    }
-
-    @Test
-    public void testDeleteOffload() throws Exception {
-        LedgerOffloader offloader = fileSystemManagedLedgerOffloader;
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, map).get();
-        Configuration configuration = new Configuration();
-        @Cleanup
-        FileSystem fileSystem = FileSystem.get(new URI(getURI()), configuration);
-        assertTrue(fileSystem.exists(new Path(createDataFilePath(storagePath, lh.getId(), uuid))));
-        assertTrue(fileSystem.exists(new Path(createIndexFilePath(storagePath, lh.getId(), uuid))));
-        offloader.deleteOffloaded(lh.getId(), uuid, map).get();
-        assertFalse(fileSystem.exists(new Path(createDataFilePath(storagePath, lh.getId(), uuid))));
-        assertFalse(fileSystem.exists(new Path(createIndexFilePath(storagePath, lh.getId(), uuid))));
-    }
-
-    private String createStoragePath(String managedLedgerName) {
-        return basePath == null ? managedLedgerName + "/" : basePath + "/" +  managedLedgerName + "/";
-    }
-
-    private String createIndexFilePath(String storagePath, long ledgerId, UUID uuid) {
-        return storagePath + ledgerId + "-" + uuid + "/index";
-    }
-
-    private String createDataFilePath(String storagePath, long ledgerId, UUID uuid) {
-        return storagePath + ledgerId + "-" + uuid + "/data";
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.filesystem.impl;
+
+
+import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertFalse;
+import static org.testng.Assert.assertTrue;
+import java.io.IOException;
+import java.net.URI;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.UUID;
+import lombok.Cleanup;
+import org.apache.bookkeeper.client.BookKeeper;
+import org.apache.bookkeeper.client.LedgerHandle;
+import org.apache.bookkeeper.client.PulsarMockBookKeeper;
+import org.apache.bookkeeper.client.api.DigestType;
+import org.apache.bookkeeper.client.api.LedgerEntries;
+import org.apache.bookkeeper.client.api.LedgerEntry;
+import org.apache.bookkeeper.client.api.ReadHandle;
+import org.apache.bookkeeper.mledger.LedgerOffloader;
+import org.apache.bookkeeper.mledger.impl.LedgerOffloaderStatsImpl;
+import org.apache.bookkeeper.mledger.offload.filesystem.FileStoreTestBase;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.pulsar.common.naming.TopicName;
+import org.testng.annotations.AfterMethod;
+import org.testng.annotations.BeforeMethod;
+import org.testng.annotations.Test;
+
+public class FileSystemManagedLedgerOffloaderTest extends FileStoreTestBase {
+    private PulsarMockBookKeeper bk;
+    private String managedLedgerName = "public/default/persistent/testOffload";
+    private String topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
+    private String storagePath = createStoragePath(managedLedgerName);
+    private LedgerHandle lh;
+    private ReadHandle toWrite;
+    private final int numberOfEntries = 601;
+    private  Map<String, String> map = new HashMap<>();
+
+    @Override
+    public void init() throws Exception {
+        super.init();
+        this.bk = new PulsarMockBookKeeper(scheduler);
+        this.toWrite = buildReadHandle();
+        map.put("ManagedLedgerName", managedLedgerName);
+    }
+
+    @Override
+    public void cleanup() throws IOException {
+        if (bk != null) {
+            bk.shutdown();
+        }
+        super.cleanup();
+    }
+
+    private ReadHandle buildReadHandle() throws Exception {
+
+        lh = bk.createLedger(1, 1, 1, BookKeeper.DigestType.CRC32, "foobar".getBytes());
+
+        int i = 0;
+        int blocksWritten = 1;
+        while (blocksWritten <= numberOfEntries) {
+            byte[] entry = ("foobar" + i).getBytes();
+            blocksWritten++;
+            lh.addEntry(entry);
+            i++;
+        }
+        lh.close();
+
+        return bk.newOpenLedgerOp().withLedgerId(lh.getId())
+                .withPassword("foobar".getBytes()).withDigestType(DigestType.CRC32).execute().get();
+    }
+
+    @BeforeMethod(alwaysRun = true)
+    @Override
+    public void start() throws Exception {
+        super.start();
+    }
+
+    @AfterMethod(alwaysRun = true)
+    @Override
+    public void tearDown() throws Exception {
+        super.tearDown();
+    }
+
+    @Test
+    public void testOffloadAndRead() throws Exception {
+        LedgerOffloader offloader = fileSystemManagedLedgerOffloader;
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, map).get();
+        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, map).get();
+        assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
+        LedgerEntries toTestEntries = toTest.read(0, numberOfEntries - 1);
+        LedgerEntries toWriteEntries = toWrite.read(0, numberOfEntries - 1);
+        Iterator<LedgerEntry> toTestIter = toTestEntries.iterator();
+        Iterator<LedgerEntry> toWriteIter = toWriteEntries.iterator();
+        while (toTestIter.hasNext()) {
+            LedgerEntry toWriteEntry = toWriteIter.next();
+            LedgerEntry toTestEntry = toTestIter.next();
+
+            assertEquals(toWriteEntry.getLedgerId(), toTestEntry.getLedgerId());
+            assertEquals(toWriteEntry.getEntryId(), toTestEntry.getEntryId());
+            assertEquals(toWriteEntry.getLength(), toTestEntry.getLength());
+            assertEquals(toWriteEntry.getEntryBuffer(), toTestEntry.getEntryBuffer());
+        }
+        toTestEntries.close();
+        toWriteEntries.close();
+        toTestEntries = toTest.read(1, numberOfEntries - 1);
+        toWriteEntries = toWrite.read(1, numberOfEntries - 1);
+        toTestIter = toTestEntries.iterator();
+        toWriteIter = toWriteEntries.iterator();
+        while (toTestIter.hasNext()) {
+            LedgerEntry toWriteEntry = toWriteIter.next();
+            LedgerEntry toTestEntry = toTestIter.next();
+
+            assertEquals(toWriteEntry.getLedgerId(), toTestEntry.getLedgerId());
+            assertEquals(toWriteEntry.getEntryId(), toTestEntry.getEntryId());
+            assertEquals(toWriteEntry.getLength(), toTestEntry.getLength());
+            assertEquals(toWriteEntry.getEntryBuffer(), toTestEntry.getEntryBuffer());
+        }
+        toTestEntries.close();
+        toWriteEntries.close();
+    }
+
+    @Test
+    public void testOffloadAndReadMetrics() throws Exception {
+        LedgerOffloader offloader = fileSystemManagedLedgerOffloader;
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, map).get();
+
+        LedgerOffloaderStatsImpl offloaderStats = (LedgerOffloaderStatsImpl) this.offloaderStats;
+        assertTrue(offloaderStats.getOffloadError(topicName) == 0);
+        assertTrue(offloaderStats.getOffloadBytes(topicName) > 0);
+        assertTrue(offloaderStats.getReadLedgerLatency(topicName).count > 0);
+        assertTrue(offloaderStats.getWriteStorageError(topicName) == 0);
+
+        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, map).get();
+        LedgerEntries toTestEntries = toTest.read(0, numberOfEntries - 1);
+        Iterator<LedgerEntry> toTestIter = toTestEntries.iterator();
+        while (toTestIter.hasNext()) {
+            LedgerEntry toTestEntry = toTestIter.next();
+        }
+        toTestEntries.close();
+
+        assertTrue(offloaderStats.getReadOffloadError(topicName) == 0);
+        assertTrue(offloaderStats.getReadOffloadBytes(topicName) > 0);
+        assertTrue(offloaderStats.getReadOffloadDataLatency(topicName).count > 0);
+        assertTrue(offloaderStats.getReadOffloadIndexLatency(topicName).count > 0);
+    }
+
+    @Test
+    public void testDeleteOffload() throws Exception {
+        LedgerOffloader offloader = fileSystemManagedLedgerOffloader;
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, map).get();
+        Configuration configuration = new Configuration();
+        @Cleanup
+        FileSystem fileSystem = FileSystem.get(new URI(getURI()), configuration);
+        assertTrue(fileSystem.exists(new Path(createDataFilePath(storagePath, lh.getId(), uuid))));
+        assertTrue(fileSystem.exists(new Path(createIndexFilePath(storagePath, lh.getId(), uuid))));
+        offloader.deleteOffloaded(lh.getId(), uuid, map).get();
+        assertFalse(fileSystem.exists(new Path(createDataFilePath(storagePath, lh.getId(), uuid))));
+        assertFalse(fileSystem.exists(new Path(createIndexFilePath(storagePath, lh.getId(), uuid))));
+    }
+
+    private String createStoragePath(String managedLedgerName) {
+        return basePath == null ? managedLedgerName + "/" : basePath + "/" +  managedLedgerName + "/";
+    }
+
+    private String createIndexFilePath(String storagePath, long ledgerId, UUID uuid) {
+        return storagePath + ledgerId + "-" + uuid + "/index";
+    }
+
+    private String createDataFilePath(String storagePath, long ledgerId, UUID uuid) {
+        return storagePath + ledgerId + "-" + uuid + "/data";
+    }
+}
diff --git a/tiered-storage/file-system/src/test/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileSystemOffloaderLocalFileTest.java b/tiered-storage/file-system/src/test/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileSystemOffloaderLocalFileTest.java
index 5840224f59..c1d20888f7 100644
--- a/tiered-storage/file-system/src/test/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileSystemOffloaderLocalFileTest.java
+++ b/tiered-storage/file-system/src/test/java/org/apache/bookkeeper/mledger/offload/filesystem/impl/FileSystemOffloaderLocalFileTest.java
@@ -1,146 +1,146 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.filesystem.impl;
-
-import static org.testng.Assert.assertEquals;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.Paths;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.UUID;
-import lombok.Cleanup;
-import org.apache.bookkeeper.client.BookKeeper;
-import org.apache.bookkeeper.client.LedgerHandle;
-import org.apache.bookkeeper.client.PulsarMockBookKeeper;
-import org.apache.bookkeeper.client.api.DigestType;
-import org.apache.bookkeeper.client.api.LedgerEntries;
-import org.apache.bookkeeper.client.api.LedgerEntry;
-import org.apache.bookkeeper.client.api.ReadHandle;
-import org.apache.bookkeeper.common.util.OrderedScheduler;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.pulsar.common.naming.TopicName;
-import org.apache.pulsar.common.policies.data.OffloadPoliciesImpl;
-import org.testng.annotations.AfterClass;
-import org.testng.annotations.BeforeClass;
-import org.testng.annotations.Test;
-
-public class FileSystemOffloaderLocalFileTest {
-    private OrderedScheduler scheduler;
-    private LedgerOffloaderStats offloaderStats;
-
-    @BeforeClass
-    public void setup() throws Exception {
-        scheduler = OrderedScheduler.newSchedulerBuilder().numThreads(1).name("offloader").build();
-        offloaderStats = LedgerOffloaderStats.create(true, true, scheduler, 60);
-    }
-
-    @AfterClass(alwaysRun = true)
-    public void cleanup() throws Exception {
-        if (scheduler != null) {
-            scheduler.shutdown();
-        }
-        if (offloaderStats != null) {
-            offloaderStats.close();
-        }
-    }
-
-    private String getResourceFilePath(String name) {
-        return getClass().getClassLoader().getResource(name).getPath();
-    }
-
-    @Test
-    public void testReadWriteWithLocalFileUsingFileSystemURI() throws Exception {
-        // prepare the offload policies
-        final String basePath = "/tmp";
-        OffloadPoliciesImpl offloadPolicies = new OffloadPoliciesImpl();
-        offloadPolicies.setFileSystemURI("file://" + basePath);
-        offloadPolicies.setManagedLedgerOffloadDriver("filesystem");
-        offloadPolicies.setFileSystemProfilePath(getResourceFilePath("filesystem_offload_core_site.xml"));
-
-        // initialize the offloader with the offload policies
-        @Cleanup
-        var offloader = FileSystemManagedLedgerOffloader.create(offloadPolicies, scheduler, offloaderStats);
-
-        int numberOfEntries = 100;
-
-        // prepare the data in bookkeeper
-        @Cleanup
-        BookKeeper bk = new PulsarMockBookKeeper(scheduler);
-        LedgerHandle lh = bk.createLedger(1, 1, 1, BookKeeper.DigestType.CRC32, "".getBytes());
-        for (int i = 0; i <  numberOfEntries; i++) {
-            byte[] entry = ("foobar" + i).getBytes();
-            lh.addEntry(entry);
-        }
-        lh.close();
-
-        @Cleanup
-        ReadHandle read = bk.newOpenLedgerOp()
-            .withLedgerId(lh.getId())
-            .withDigestType(DigestType.CRC32)
-            .withPassword("".getBytes()).execute().get();
-
-        final String mlName = TopicName.get("testWriteLocalFIle").getPersistenceNamingEncoding();
-        Map<String, String> offloadDriverMetadata = new HashMap<>();
-        offloadDriverMetadata.put("ManagedLedgerName", mlName);
-
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(read, uuid, offloadDriverMetadata).get();
-        @Cleanup
-        ReadHandle toTest = offloader.readOffloaded(read.getId(), uuid, offloadDriverMetadata).get();
-        assertEquals(toTest.getLastAddConfirmed(), read.getLastAddConfirmed());
-        LedgerEntries toTestEntries = toTest.read(0, numberOfEntries - 1);
-        LedgerEntries toWriteEntries = read.read(0, numberOfEntries - 1);
-        Iterator<LedgerEntry> toTestIter = toTestEntries.iterator();
-        Iterator<LedgerEntry> toWriteIter = toWriteEntries.iterator();
-        while (toTestIter.hasNext()) {
-            LedgerEntry toWriteEntry = toWriteIter.next();
-            LedgerEntry toTestEntry = toTestIter.next();
-
-            assertEquals(toWriteEntry.getLedgerId(), toTestEntry.getLedgerId());
-            assertEquals(toWriteEntry.getEntryId(), toTestEntry.getEntryId());
-            assertEquals(toWriteEntry.getLength(), toTestEntry.getLength());
-            assertEquals(toWriteEntry.getEntryBuffer(), toTestEntry.getEntryBuffer());
-        }
-        toTestEntries.close();
-        toWriteEntries.close();
-
-        toTestEntries = toTest.read(1, numberOfEntries - 1);
-        toWriteEntries = read.read(1, numberOfEntries - 1);
-        toTestIter = toTestEntries.iterator();
-        toWriteIter = toWriteEntries.iterator();
-        while (toTestIter.hasNext()) {
-            LedgerEntry toWriteEntry = toWriteIter.next();
-            LedgerEntry toTestEntry = toTestIter.next();
-
-            assertEquals(toWriteEntry.getLedgerId(), toTestEntry.getLedgerId());
-            assertEquals(toWriteEntry.getEntryId(), toTestEntry.getEntryId());
-            assertEquals(toWriteEntry.getLength(), toTestEntry.getLength());
-            assertEquals(toWriteEntry.getEntryBuffer(), toTestEntry.getEntryBuffer());
-        }
-
-        toTestEntries.close();
-        toWriteEntries.close();
-
-        // check the file located in the local file system
-        Path offloadedFilePath = Paths.get(basePath, mlName);
-        assertEquals(Files.exists(offloadedFilePath), true);
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.filesystem.impl;
+
+import static org.testng.Assert.assertEquals;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.UUID;
+import lombok.Cleanup;
+import org.apache.bookkeeper.client.BookKeeper;
+import org.apache.bookkeeper.client.LedgerHandle;
+import org.apache.bookkeeper.client.PulsarMockBookKeeper;
+import org.apache.bookkeeper.client.api.DigestType;
+import org.apache.bookkeeper.client.api.LedgerEntries;
+import org.apache.bookkeeper.client.api.LedgerEntry;
+import org.apache.bookkeeper.client.api.ReadHandle;
+import org.apache.bookkeeper.common.util.OrderedScheduler;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.pulsar.common.naming.TopicName;
+import org.apache.pulsar.common.policies.data.OffloadPoliciesImpl;
+import org.testng.annotations.AfterClass;
+import org.testng.annotations.BeforeClass;
+import org.testng.annotations.Test;
+
+public class FileSystemOffloaderLocalFileTest {
+    private OrderedScheduler scheduler;
+    private LedgerOffloaderStats offloaderStats;
+
+    @BeforeClass
+    public void setup() throws Exception {
+        scheduler = OrderedScheduler.newSchedulerBuilder().numThreads(1).name("offloader").build();
+        offloaderStats = LedgerOffloaderStats.create(true, true, scheduler, 60);
+    }
+
+    @AfterClass(alwaysRun = true)
+    public void cleanup() throws Exception {
+        if (scheduler != null) {
+            scheduler.shutdown();
+        }
+        if (offloaderStats != null) {
+            offloaderStats.close();
+        }
+    }
+
+    private String getResourceFilePath(String name) {
+        return getClass().getClassLoader().getResource(name).getPath();
+    }
+
+    @Test
+    public void testReadWriteWithLocalFileUsingFileSystemURI() throws Exception {
+        // prepare the offload policies
+        final String basePath = "/tmp";
+        OffloadPoliciesImpl offloadPolicies = new OffloadPoliciesImpl();
+        offloadPolicies.setFileSystemURI("file://" + basePath);
+        offloadPolicies.setManagedLedgerOffloadDriver("filesystem");
+        offloadPolicies.setFileSystemProfilePath(getResourceFilePath("filesystem_offload_core_site.xml"));
+
+        // initialize the offloader with the offload policies
+        @Cleanup
+        var offloader = FileSystemManagedLedgerOffloader.create(offloadPolicies, scheduler, offloaderStats);
+
+        int numberOfEntries = 100;
+
+        // prepare the data in bookkeeper
+        @Cleanup
+        BookKeeper bk = new PulsarMockBookKeeper(scheduler);
+        LedgerHandle lh = bk.createLedger(1, 1, 1, BookKeeper.DigestType.CRC32, "".getBytes());
+        for (int i = 0; i <  numberOfEntries; i++) {
+            byte[] entry = ("foobar" + i).getBytes();
+            lh.addEntry(entry);
+        }
+        lh.close();
+
+        @Cleanup
+        ReadHandle read = bk.newOpenLedgerOp()
+            .withLedgerId(lh.getId())
+            .withDigestType(DigestType.CRC32)
+            .withPassword("".getBytes()).execute().get();
+
+        final String mlName = TopicName.get("testWriteLocalFIle").getPersistenceNamingEncoding();
+        Map<String, String> offloadDriverMetadata = new HashMap<>();
+        offloadDriverMetadata.put("ManagedLedgerName", mlName);
+
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(read, uuid, offloadDriverMetadata).get();
+        @Cleanup
+        ReadHandle toTest = offloader.readOffloaded(read.getId(), uuid, offloadDriverMetadata).get();
+        assertEquals(toTest.getLastAddConfirmed(), read.getLastAddConfirmed());
+        LedgerEntries toTestEntries = toTest.read(0, numberOfEntries - 1);
+        LedgerEntries toWriteEntries = read.read(0, numberOfEntries - 1);
+        Iterator<LedgerEntry> toTestIter = toTestEntries.iterator();
+        Iterator<LedgerEntry> toWriteIter = toWriteEntries.iterator();
+        while (toTestIter.hasNext()) {
+            LedgerEntry toWriteEntry = toWriteIter.next();
+            LedgerEntry toTestEntry = toTestIter.next();
+
+            assertEquals(toWriteEntry.getLedgerId(), toTestEntry.getLedgerId());
+            assertEquals(toWriteEntry.getEntryId(), toTestEntry.getEntryId());
+            assertEquals(toWriteEntry.getLength(), toTestEntry.getLength());
+            assertEquals(toWriteEntry.getEntryBuffer(), toTestEntry.getEntryBuffer());
+        }
+        toTestEntries.close();
+        toWriteEntries.close();
+
+        toTestEntries = toTest.read(1, numberOfEntries - 1);
+        toWriteEntries = read.read(1, numberOfEntries - 1);
+        toTestIter = toTestEntries.iterator();
+        toWriteIter = toWriteEntries.iterator();
+        while (toTestIter.hasNext()) {
+            LedgerEntry toWriteEntry = toWriteIter.next();
+            LedgerEntry toTestEntry = toTestIter.next();
+
+            assertEquals(toWriteEntry.getLedgerId(), toTestEntry.getLedgerId());
+            assertEquals(toWriteEntry.getEntryId(), toTestEntry.getEntryId());
+            assertEquals(toWriteEntry.getLength(), toTestEntry.getLength());
+            assertEquals(toWriteEntry.getEntryBuffer(), toTestEntry.getEntryBuffer());
+        }
+
+        toTestEntries.close();
+        toWriteEntries.close();
+
+        // check the file located in the local file system
+        Path offloadedFilePath = Paths.get(basePath, mlName);
+        assertEquals(Files.exists(offloadedFilePath), true);
+    }
+}
diff --git a/tiered-storage/file-system/src/test/resources/filesystem_offload_core_site.xml b/tiered-storage/file-system/src/test/resources/filesystem_offload_core_site.xml
index d26cec2cc6..e0b194fe2f 100644
--- a/tiered-storage/file-system/src/test/resources/filesystem_offload_core_site.xml
+++ b/tiered-storage/file-system/src/test/resources/filesystem_offload_core_site.xml
@@ -1,48 +1,48 @@
-<!--
-
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-
--->
-<configuration>
-    <!--file system uri, necessary-->
-    <property>
-        <name>fs.defaultFS</name>
-        <value></value>
-    </property>
-    <property>
-        <name>hadoop.tmp.dir</name>
-        <value>pulsar</value>
-    </property>
-    <property>
-        <name>io.file.buffer.size</name>
-        <value>4096</value>
-    </property>
-    <property>
-        <name>io.seqfile.compress.blocksize</name>
-        <value>1000000</value>
-    </property>
-    <property>
-        <name>io.seqfile.compression.type</name>
-        <value>BLOCK</value>
-    </property>
-    <property>
-        <name>io.map.index.interval</name>
-        <value>128</value>
-    </property>
-
-</configuration>
+<!--
+
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+
+-->
+<configuration>
+    <!--file system uri, necessary-->
+    <property>
+        <name>fs.defaultFS</name>
+        <value></value>
+    </property>
+    <property>
+        <name>hadoop.tmp.dir</name>
+        <value>pulsar</value>
+    </property>
+    <property>
+        <name>io.file.buffer.size</name>
+        <value>4096</value>
+    </property>
+    <property>
+        <name>io.seqfile.compress.blocksize</name>
+        <value>1000000</value>
+    </property>
+    <property>
+        <name>io.seqfile.compression.type</name>
+        <value>BLOCK</value>
+    </property>
+    <property>
+        <name>io.map.index.interval</name>
+        <value>128</value>
+    </property>
+
+</configuration>
diff --git a/tiered-storage/jcloud/pom.xml b/tiered-storage/jcloud/pom.xml
index 5e1ea5dac9..a257f9cf75 100644
--- a/tiered-storage/jcloud/pom.xml
+++ b/tiered-storage/jcloud/pom.xml
@@ -1,165 +1,165 @@
-<!--
-
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
-  <modelVersion>4.0.0</modelVersion>
-
-  <parent>
-    <groupId>org.apache.pulsar</groupId>
-    <artifactId>tiered-storage-parent</artifactId>
-    <version>4.1.2</version>
-  </parent>
-
-  <artifactId>tiered-storage-jcloud</artifactId>
-  <name>Apache Pulsar :: Tiered Storage :: JCloud</name>
-
-  <dependencies>
-    <dependency>
-      <groupId>${project.groupId}</groupId>
-      <artifactId>managed-ledger</artifactId>
-      <version>${project.version}</version>
-      <scope>provided</scope>
-    </dependency>
-
-    <dependency>
-      <groupId>${project.groupId}</groupId>
-      <artifactId>jclouds-shaded</artifactId>
-      <version>${pulsar.jclouds.shaded.version}</version>
-      <exclusions>
-        <exclusion>
-          <groupId>com.google.code.gson</groupId>
-          <artifactId>gson</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>com.google.guava</groupId>
-          <artifactId>guava</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>org.apache.jclouds</groupId>
-          <artifactId>*</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>org.apache.jclouds.api</groupId>
-          <artifactId>*</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>org.apache.jclouds.common</groupId>
-          <artifactId>*</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>org.apache.jclouds.provider</groupId>
-          <artifactId>*</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>org.apache.jclouds.driver</groupId>
-          <artifactId>*</artifactId>
-        </exclusion>
-      </exclusions>
-    </dependency>
-
-    <dependency>
-      <groupId>org.apache.jclouds</groupId>
-      <artifactId>jclouds-allblobstore</artifactId>
-      <version>${jclouds.version}</version>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>com.amazonaws</groupId>
-      <artifactId>aws-java-sdk-core</artifactId>
-    </dependency>
-
-    <dependency>
-      <groupId>com.amazonaws</groupId>
-      <artifactId>aws-java-sdk-sts</artifactId>
-    </dependency>
-
-    <dependency>
-      <groupId>${project.groupId}</groupId>
-      <artifactId>testmocks</artifactId>
-      <version>${project.version}</version>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.jclouds</groupId>
-      <artifactId>jclouds-blobstore</artifactId>
-      <version>${jclouds.version}</version>
-      <scope>provided</scope>
-    </dependency>
-
-    <dependency>
-      <groupId>jakarta.xml.bind</groupId>
-      <artifactId>jakarta.xml.bind-api</artifactId>
-      <exclusions>
-        <exclusion>
-          <groupId>javax.activation</groupId>
-          <artifactId>javax.activation-api</artifactId>
-        </exclusion>
-      </exclusions>
-      <scope>runtime</scope>
-    </dependency>
-
-    <dependency>
-      <groupId>com.sun.activation</groupId>
-      <artifactId>jakarta.activation</artifactId>
-      <scope>runtime</scope>
-    </dependency>
-
-  </dependencies>
-  <build>
-    <plugins>
-      <plugin>
-        <groupId>org.apache.nifi</groupId>
-        <artifactId>nifi-nar-maven-plugin</artifactId>
-      </plugin>
-
-      <plugin>
-        <groupId>com.github.spotbugs</groupId>
-        <artifactId>spotbugs-maven-plugin</artifactId>
-        <version>${spotbugs-maven-plugin.version}</version>
-        <configuration>
-          <excludeFilterFile>${basedir}/src/main/resources/findbugsExclude.xml</excludeFilterFile>
-        </configuration>
-        <executions>
-          <execution>
-            <id>spotbugs</id>
-            <phase>verify</phase>
-            <goals>
-              <goal>check</goal>
-            </goals>
-          </execution>
-        </executions>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-checkstyle-plugin</artifactId>
-        <executions>
-          <execution>
-            <id>checkstyle</id>
-            <phase>verify</phase>
-            <goals>
-              <goal>check</goal>
-            </goals>
-          </execution>
-        </executions>
-      </plugin>
-    </plugins>
-  </build>
-</project>
+<!--
+
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+  <modelVersion>4.0.0</modelVersion>
+
+  <parent>
+    <groupId>org.apache.pulsar</groupId>
+    <artifactId>tiered-storage-parent</artifactId>
+    <version>4.1.2</version>
+  </parent>
+
+  <artifactId>tiered-storage-jcloud</artifactId>
+  <name>Apache Pulsar :: Tiered Storage :: JCloud</name>
+
+  <dependencies>
+    <dependency>
+      <groupId>${project.groupId}</groupId>
+      <artifactId>managed-ledger</artifactId>
+      <version>${project.version}</version>
+      <scope>provided</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>${project.groupId}</groupId>
+      <artifactId>jclouds-shaded</artifactId>
+      <version>${pulsar.jclouds.shaded.version}</version>
+      <exclusions>
+        <exclusion>
+          <groupId>com.google.code.gson</groupId>
+          <artifactId>gson</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>com.google.guava</groupId>
+          <artifactId>guava</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>org.apache.jclouds</groupId>
+          <artifactId>*</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>org.apache.jclouds.api</groupId>
+          <artifactId>*</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>org.apache.jclouds.common</groupId>
+          <artifactId>*</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>org.apache.jclouds.provider</groupId>
+          <artifactId>*</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>org.apache.jclouds.driver</groupId>
+          <artifactId>*</artifactId>
+        </exclusion>
+      </exclusions>
+    </dependency>
+
+    <dependency>
+      <groupId>org.apache.jclouds</groupId>
+      <artifactId>jclouds-allblobstore</artifactId>
+      <version>${jclouds.version}</version>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>com.amazonaws</groupId>
+      <artifactId>aws-java-sdk-core</artifactId>
+    </dependency>
+
+    <dependency>
+      <groupId>com.amazonaws</groupId>
+      <artifactId>aws-java-sdk-sts</artifactId>
+    </dependency>
+
+    <dependency>
+      <groupId>${project.groupId}</groupId>
+      <artifactId>testmocks</artifactId>
+      <version>${project.version}</version>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.jclouds</groupId>
+      <artifactId>jclouds-blobstore</artifactId>
+      <version>${jclouds.version}</version>
+      <scope>provided</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>jakarta.xml.bind</groupId>
+      <artifactId>jakarta.xml.bind-api</artifactId>
+      <exclusions>
+        <exclusion>
+          <groupId>javax.activation</groupId>
+          <artifactId>javax.activation-api</artifactId>
+        </exclusion>
+      </exclusions>
+      <scope>runtime</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>com.sun.activation</groupId>
+      <artifactId>jakarta.activation</artifactId>
+      <scope>runtime</scope>
+    </dependency>
+
+  </dependencies>
+  <build>
+    <plugins>
+      <plugin>
+        <groupId>org.apache.nifi</groupId>
+        <artifactId>nifi-nar-maven-plugin</artifactId>
+      </plugin>
+
+      <plugin>
+        <groupId>com.github.spotbugs</groupId>
+        <artifactId>spotbugs-maven-plugin</artifactId>
+        <version>${spotbugs-maven-plugin.version}</version>
+        <configuration>
+          <excludeFilterFile>${basedir}/src/main/resources/findbugsExclude.xml</excludeFilterFile>
+        </configuration>
+        <executions>
+          <execution>
+            <id>spotbugs</id>
+            <phase>verify</phase>
+            <goals>
+              <goal>check</goal>
+            </goals>
+          </execution>
+        </executions>
+      </plugin>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-checkstyle-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>checkstyle</id>
+            <phase>verify</phase>
+            <goals>
+              <goal>check</goal>
+            </goals>
+          </execution>
+        </executions>
+      </plugin>
+    </plugins>
+  </build>
+</project>
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/BackedInputStream.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/BackedInputStream.java
index 34ee4ea0f1..32bc566751 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/BackedInputStream.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/BackedInputStream.java
@@ -1,31 +1,31 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud;
-
-import java.io.IOException;
-import java.io.InputStream;
-
-/**
- * Abstract input stream class.
- */
-public abstract class BackedInputStream extends InputStream {
-    public abstract void seek(long position);
-    public abstract void seekForward(long position) throws IOException;
-    public abstract long getCurrentPosition();
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud;
+
+import java.io.IOException;
+import java.io.InputStream;
+
+/**
+ * Abstract input stream class.
+ */
+public abstract class BackedInputStream extends InputStream {
+    public abstract void seek(long position);
+    public abstract void seekForward(long position) throws IOException;
+    public abstract long getCurrentPosition();
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/BlockAwareSegmentInputStream.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/BlockAwareSegmentInputStream.java
index c25f162566..9c1cbae1e4 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/BlockAwareSegmentInputStream.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/BlockAwareSegmentInputStream.java
@@ -1,70 +1,70 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud;
-
-import java.io.InputStream;
-import org.apache.bookkeeper.client.api.ReadHandle;
-
-/**
- * The BlockAwareSegmentInputStream for each cold storage data block.
- * This interface should be implemented while extends InputStream.
- * It gets data from ledger, and will be read out the content for a data block.
- * DataBlockHeader + entries(each with format[[entry_size -- int][entry_id -- long][entry_data]]) + padding
- */
-public abstract class BlockAwareSegmentInputStream extends InputStream {
-    /**
-     * Get the ledger, from which this InputStream read data.
-     */
-    public abstract ReadHandle getLedger();
-
-    /**
-     * Get start entry id contained in this InputStream.
-     *
-     * @return the start entry id
-     */
-    public abstract long getStartEntryId();
-
-    /**
-     * Get block size that could read out from this InputStream.
-     *
-     * @return the block size
-     */
-    public abstract int getBlockSize();
-
-    /**
-     * Get entry count that read out from this InputStream.
-     *
-     * @return the block entry count
-     */
-    public abstract int getBlockEntryCount();
-
-    /**
-     * Get end entry id contained in this InputStream.
-     *
-     * @return the end entry id
-     */
-    public abstract long getEndEntryId();
-
-    /**
-     * Get sum of entries data size read from the this InputStream.
-     *
-     * @return the block entry bytes count
-     */
-    public abstract int getBlockEntryBytesCount();
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud;
+
+import java.io.InputStream;
+import org.apache.bookkeeper.client.api.ReadHandle;
+
+/**
+ * The BlockAwareSegmentInputStream for each cold storage data block.
+ * This interface should be implemented while extends InputStream.
+ * It gets data from ledger, and will be read out the content for a data block.
+ * DataBlockHeader + entries(each with format[[entry_size -- int][entry_id -- long][entry_data]]) + padding
+ */
+public abstract class BlockAwareSegmentInputStream extends InputStream {
+    /**
+     * Get the ledger, from which this InputStream read data.
+     */
+    public abstract ReadHandle getLedger();
+
+    /**
+     * Get start entry id contained in this InputStream.
+     *
+     * @return the start entry id
+     */
+    public abstract long getStartEntryId();
+
+    /**
+     * Get block size that could read out from this InputStream.
+     *
+     * @return the block size
+     */
+    public abstract int getBlockSize();
+
+    /**
+     * Get entry count that read out from this InputStream.
+     *
+     * @return the block entry count
+     */
+    public abstract int getBlockEntryCount();
+
+    /**
+     * Get end entry id contained in this InputStream.
+     *
+     * @return the end entry id
+     */
+    public abstract long getEndEntryId();
+
+    /**
+     * Get sum of entries data size read from the this InputStream.
+     *
+     * @return the block entry bytes count
+     */
+    public abstract int getBlockEntryBytesCount();
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/DataBlockHeader.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/DataBlockHeader.java
index ce0d6222c1..0576813537 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/DataBlockHeader.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/DataBlockHeader.java
@@ -1,56 +1,56 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud;
-
-import java.io.InputStream;
-import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;
-
-/**
- * The data block header in tiered storage for each data block.
- *
- * <p>Currently, It is in format:
- * [ magic_word -- int ][ block_len -- int ][ first_entry_id  -- long][padding]
- *
- * with the size: 4 + 4 + 8 + padding = 128 Bytes</p>
- */
-@Unstable
-public interface DataBlockHeader {
-
-    /**
-     * Get the length of the block in bytes, including the header.
-     */
-    long getBlockLength();
-
-    /**
-     * Get the message entry Id for the first message that stored in this data block.
-     */
-    long getFirstEntryId();
-
-    /**
-     * Get the size of this DataBlockHeader.
-     */
-    long getHeaderLength();
-
-    /**
-     * Get the content of the data block header as InputStream.
-     * Read out in current format.
-     */
-    InputStream toStream();
-}
-
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud;
+
+import java.io.InputStream;
+import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;
+
+/**
+ * The data block header in tiered storage for each data block.
+ *
+ * <p>Currently, It is in format:
+ * [ magic_word -- int ][ block_len -- int ][ first_entry_id  -- long][padding]
+ *
+ * with the size: 4 + 4 + 8 + padding = 128 Bytes</p>
+ */
+@Unstable
+public interface DataBlockHeader {
+
+    /**
+     * Get the length of the block in bytes, including the header.
+     */
+    long getBlockLength();
+
+    /**
+     * Get the message entry Id for the first message that stored in this data block.
+     */
+    long getFirstEntryId();
+
+    /**
+     * Get the size of this DataBlockHeader.
+     */
+    long getHeaderLength();
+
+    /**
+     * Get the content of the data block header as InputStream.
+     * Read out in current format.
+     */
+    InputStream toStream();
+}
+
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/JCloudLedgerOffloaderFactory.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/JCloudLedgerOffloaderFactory.java
index ef2db046e3..59a6a091c8 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/JCloudLedgerOffloaderFactory.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/JCloudLedgerOffloaderFactory.java
@@ -1,77 +1,77 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud;
-
-import java.io.IOException;
-import java.util.Map;
-import org.apache.bookkeeper.common.util.OrderedScheduler;
-import org.apache.bookkeeper.mledger.LedgerOffloaderFactory;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStatsDisable;
-import org.apache.bookkeeper.mledger.offload.jcloud.impl.BlobStoreManagedLedgerOffloader;
-import org.apache.bookkeeper.mledger.offload.jcloud.impl.OffsetsCache;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.JCloudBlobStoreProvider;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration;
-import org.apache.pulsar.common.policies.data.OffloadPoliciesImpl;
-
-/**
- * A jcloud based offloader factory.
- */
-public class JCloudLedgerOffloaderFactory implements LedgerOffloaderFactory<BlobStoreManagedLedgerOffloader> {
-    private final OffsetsCache entryOffsetsCache = new OffsetsCache();
-
-    @Override
-    public boolean isDriverSupported(String driverName) {
-        return JCloudBlobStoreProvider.driverSupported(driverName);
-    }
-
-    @Override
-    public BlobStoreManagedLedgerOffloader create(OffloadPoliciesImpl offloadPolicies, Map<String, String> userMetadata,
-                                                  OrderedScheduler scheduler) throws IOException {
-        return create(offloadPolicies, userMetadata, scheduler, LedgerOffloaderStatsDisable.INSTANCE);
-    }
-
-    @Override
-    public BlobStoreManagedLedgerOffloader create(OffloadPoliciesImpl offloadPolicies, Map<String, String> userMetadata,
-                                                  OrderedScheduler scheduler,
-                                                  LedgerOffloaderStats offloaderStats) throws IOException {
-
-        TieredStorageConfiguration config =
-                TieredStorageConfiguration.create(offloadPolicies.toProperties());
-        return BlobStoreManagedLedgerOffloader.create(config, userMetadata, scheduler, scheduler, offloaderStats,
-                entryOffsetsCache);
-    }
-
-    @Override
-    public BlobStoreManagedLedgerOffloader create(OffloadPoliciesImpl offloadPolicies, Map<String, String> userMetadata,
-                                                  OrderedScheduler scheduler,
-                                                  OrderedScheduler readExecutor,
-                                                  LedgerOffloaderStats offloaderStats) throws IOException {
-
-        TieredStorageConfiguration config =
-                TieredStorageConfiguration.create(offloadPolicies.toProperties());
-        return BlobStoreManagedLedgerOffloader.create(config, userMetadata, scheduler, readExecutor, offloaderStats,
-                entryOffsetsCache);
-    }
-
-    @Override
-    public void close() throws Exception {
-        entryOffsetsCache.close();
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud;
+
+import java.io.IOException;
+import java.util.Map;
+import org.apache.bookkeeper.common.util.OrderedScheduler;
+import org.apache.bookkeeper.mledger.LedgerOffloaderFactory;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStatsDisable;
+import org.apache.bookkeeper.mledger.offload.jcloud.impl.BlobStoreManagedLedgerOffloader;
+import org.apache.bookkeeper.mledger.offload.jcloud.impl.OffsetsCache;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.JCloudBlobStoreProvider;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration;
+import org.apache.pulsar.common.policies.data.OffloadPoliciesImpl;
+
+/**
+ * A jcloud based offloader factory.
+ */
+public class JCloudLedgerOffloaderFactory implements LedgerOffloaderFactory<BlobStoreManagedLedgerOffloader> {
+    private final OffsetsCache entryOffsetsCache = new OffsetsCache();
+
+    @Override
+    public boolean isDriverSupported(String driverName) {
+        return JCloudBlobStoreProvider.driverSupported(driverName);
+    }
+
+    @Override
+    public BlobStoreManagedLedgerOffloader create(OffloadPoliciesImpl offloadPolicies, Map<String, String> userMetadata,
+                                                  OrderedScheduler scheduler) throws IOException {
+        return create(offloadPolicies, userMetadata, scheduler, LedgerOffloaderStatsDisable.INSTANCE);
+    }
+
+    @Override
+    public BlobStoreManagedLedgerOffloader create(OffloadPoliciesImpl offloadPolicies, Map<String, String> userMetadata,
+                                                  OrderedScheduler scheduler,
+                                                  LedgerOffloaderStats offloaderStats) throws IOException {
+
+        TieredStorageConfiguration config =
+                TieredStorageConfiguration.create(offloadPolicies.toProperties());
+        return BlobStoreManagedLedgerOffloader.create(config, userMetadata, scheduler, scheduler, offloaderStats,
+                entryOffsetsCache);
+    }
+
+    @Override
+    public BlobStoreManagedLedgerOffloader create(OffloadPoliciesImpl offloadPolicies, Map<String, String> userMetadata,
+                                                  OrderedScheduler scheduler,
+                                                  OrderedScheduler readExecutor,
+                                                  LedgerOffloaderStats offloaderStats) throws IOException {
+
+        TieredStorageConfiguration config =
+                TieredStorageConfiguration.create(offloadPolicies.toProperties());
+        return BlobStoreManagedLedgerOffloader.create(config, userMetadata, scheduler, readExecutor, offloaderStats,
+                entryOffsetsCache);
+    }
+
+    @Override
+    public void close() throws Exception {
+        entryOffsetsCache.close();
+    }
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlock.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlock.java
index 077f16a2be..df778df2bc 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlock.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlock.java
@@ -1,103 +1,103 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud;
-
-import java.io.Closeable;
-import java.io.FilterInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;
-
-/**
- * The Index block abstraction used for offload a ledger to long term storage.
- */
-@Unstable
-public interface OffloadIndexBlock extends Closeable, OffloadIndexBlockV2 {
-
-    /**
-     * Get the content of the index block as InputStream.
-     * Read out in format:
-     *   | index_magic_header | index_block_len | index_entry_count |
-     *   | data_object_size | segment_metadata_length | segment metadata | index entries ... |
-     */
-    IndexInputStream toStream() throws IOException;
-
-    /**
-     * Get the related OffloadIndexEntry that contains the given messageEntryId.
-     *
-     * @param messageEntryId
-     *                      the entry id of message
-     * @return the offload index entry
-     */
-    OffloadIndexEntry getIndexEntryForEntry(long messageEntryId) throws IOException;
-
-    /**
-     * Get the entry count that contained in this index Block.
-     */
-    int getEntryCount();
-
-    /**
-     * Get LedgerMetadata.
-     */
-    LedgerMetadata getLedgerMetadata();
-
-    /**
-     * Get the total size of the data object.
-     */
-    long getDataObjectLength();
-
-    /**
-     * Get the length of the header in the blocks in the data object.
-     */
-    long getDataBlockHeaderLength();
-
-    /**
-     * An input stream which knows the size of the stream upfront.
-     */
-    class IndexInputStream extends FilterInputStream {
-        final long streamSize;
-
-        public IndexInputStream(InputStream in, long streamSize) {
-            super(in);
-            this.streamSize = streamSize;
-        }
-
-        /**
-         * @return the number of bytes in the stream.
-         */
-        public long getStreamSize() {
-            return streamSize;
-        }
-    }
-
-    default OffloadIndexEntry getIndexEntryForEntry(long ledgerId, long messageEntryId) throws IOException {
-        return getIndexEntryForEntry(messageEntryId);
-    }
-
-    default long getStartEntryId(long ledgerId) {
-        return 0; //Offload index block v1 always start with 0;
-    }
-
-    default LedgerMetadata getLedgerMetadata(long ledgerId) {
-        return getLedgerMetadata();
-    }
-
-}
-
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud;
+
+import java.io.Closeable;
+import java.io.FilterInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;
+
+/**
+ * The Index block abstraction used for offload a ledger to long term storage.
+ */
+@Unstable
+public interface OffloadIndexBlock extends Closeable, OffloadIndexBlockV2 {
+
+    /**
+     * Get the content of the index block as InputStream.
+     * Read out in format:
+     *   | index_magic_header | index_block_len | index_entry_count |
+     *   | data_object_size | segment_metadata_length | segment metadata | index entries ... |
+     */
+    IndexInputStream toStream() throws IOException;
+
+    /**
+     * Get the related OffloadIndexEntry that contains the given messageEntryId.
+     *
+     * @param messageEntryId
+     *                      the entry id of message
+     * @return the offload index entry
+     */
+    OffloadIndexEntry getIndexEntryForEntry(long messageEntryId) throws IOException;
+
+    /**
+     * Get the entry count that contained in this index Block.
+     */
+    int getEntryCount();
+
+    /**
+     * Get LedgerMetadata.
+     */
+    LedgerMetadata getLedgerMetadata();
+
+    /**
+     * Get the total size of the data object.
+     */
+    long getDataObjectLength();
+
+    /**
+     * Get the length of the header in the blocks in the data object.
+     */
+    long getDataBlockHeaderLength();
+
+    /**
+     * An input stream which knows the size of the stream upfront.
+     */
+    class IndexInputStream extends FilterInputStream {
+        final long streamSize;
+
+        public IndexInputStream(InputStream in, long streamSize) {
+            super(in);
+            this.streamSize = streamSize;
+        }
+
+        /**
+         * @return the number of bytes in the stream.
+         */
+        public long getStreamSize() {
+            return streamSize;
+        }
+    }
+
+    default OffloadIndexEntry getIndexEntryForEntry(long ledgerId, long messageEntryId) throws IOException {
+        return getIndexEntryForEntry(messageEntryId);
+    }
+
+    default long getStartEntryId(long ledgerId) {
+        return 0; //Offload index block v1 always start with 0;
+    }
+
+    default LedgerMetadata getLedgerMetadata(long ledgerId) {
+        return getLedgerMetadata();
+    }
+
+}
+
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockBuilder.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockBuilder.java
index cf73cf7f0b..5446b01ac8 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockBuilder.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockBuilder.java
@@ -1,82 +1,82 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud;
-
-import java.io.IOException;
-import java.io.InputStream;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.common.annotation.InterfaceAudience.LimitedPrivate;
-import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;
-import org.apache.bookkeeper.mledger.offload.jcloud.impl.OffloadIndexBlockV2BuilderImpl;
-
-/**
- * Interface for builder of index block used for offload a ledger to long term storage.
- */
-@Unstable
-@LimitedPrivate
-public interface OffloadIndexBlockBuilder {
-
-    /**
-     * Build index block with the passed in ledger metadata.
-     *
-     * @param metadata the ledger metadata
-     */
-    OffloadIndexBlockBuilder withLedgerMetadata(LedgerMetadata metadata);
-
-    /**
-     * Add one payload block related information into index block.
-     * It contains the first entryId in payload block, the payload block Id,
-     * and payload block size.
-     * This information will be used to consist one index entry in OffloadIndexBlock.
-     *
-     * @param firstEntryId the first entryId in payload block
-     * @param partId the payload block Id
-     * @param blockSize the payload block size
-     */
-    OffloadIndexBlockBuilder addBlock(long firstEntryId, int partId, int blockSize);
-
-    /**
-     * Specify the length of data object this index is associated with.
-     * @param dataObjectLength the length of the data object
-     */
-    OffloadIndexBlockBuilder withDataObjectLength(long dataObjectLength);
-
-    /**
-     * Specify the length of the block headers in the data object.
-     * @param dataHeaderLength the length of the headers
-     */
-    OffloadIndexBlockBuilder withDataBlockHeaderLength(long dataHeaderLength);
-
-    /**
-     * Finalize the immutable OffloadIndexBlock.
-     */
-    OffloadIndexBlock build();
-
-    /**
-     * Construct OffloadIndex from an InputStream.
-     */
-    OffloadIndexBlockV2 fromStream(InputStream is) throws IOException;
-
-    /**
-     * create an OffloadIndexBlockBuilder.
-     */
-    static OffloadIndexBlockBuilder create() {
-        return new OffloadIndexBlockV2BuilderImpl();
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud;
+
+import java.io.IOException;
+import java.io.InputStream;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.common.annotation.InterfaceAudience.LimitedPrivate;
+import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;
+import org.apache.bookkeeper.mledger.offload.jcloud.impl.OffloadIndexBlockV2BuilderImpl;
+
+/**
+ * Interface for builder of index block used for offload a ledger to long term storage.
+ */
+@Unstable
+@LimitedPrivate
+public interface OffloadIndexBlockBuilder {
+
+    /**
+     * Build index block with the passed in ledger metadata.
+     *
+     * @param metadata the ledger metadata
+     */
+    OffloadIndexBlockBuilder withLedgerMetadata(LedgerMetadata metadata);
+
+    /**
+     * Add one payload block related information into index block.
+     * It contains the first entryId in payload block, the payload block Id,
+     * and payload block size.
+     * This information will be used to consist one index entry in OffloadIndexBlock.
+     *
+     * @param firstEntryId the first entryId in payload block
+     * @param partId the payload block Id
+     * @param blockSize the payload block size
+     */
+    OffloadIndexBlockBuilder addBlock(long firstEntryId, int partId, int blockSize);
+
+    /**
+     * Specify the length of data object this index is associated with.
+     * @param dataObjectLength the length of the data object
+     */
+    OffloadIndexBlockBuilder withDataObjectLength(long dataObjectLength);
+
+    /**
+     * Specify the length of the block headers in the data object.
+     * @param dataHeaderLength the length of the headers
+     */
+    OffloadIndexBlockBuilder withDataBlockHeaderLength(long dataHeaderLength);
+
+    /**
+     * Finalize the immutable OffloadIndexBlock.
+     */
+    OffloadIndexBlock build();
+
+    /**
+     * Construct OffloadIndex from an InputStream.
+     */
+    OffloadIndexBlockV2 fromStream(InputStream is) throws IOException;
+
+    /**
+     * create an OffloadIndexBlockBuilder.
+     */
+    static OffloadIndexBlockBuilder create() {
+        return new OffloadIndexBlockV2BuilderImpl();
+    }
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockV2.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockV2.java
index 9dc3f00771..ff1e49724e 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockV2.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockV2.java
@@ -1,72 +1,72 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud;
-
-import java.io.Closeable;
-import java.io.IOException;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;
-
-/**
- * The Index block abstraction used for offload a ledger to long term storage.
- */
-@Unstable
-public interface OffloadIndexBlockV2 extends Closeable {
-
-    /**
-     * Get the content of the index block as InputStream.
-     * Read out in format:
-     *   | index_magic_header | index_block_len | index_entry_count |
-     *   | data_object_size | segment_metadata_length | segment metadata | index entries ... |
-     */
-    OffloadIndexBlock.IndexInputStream toStream() throws IOException;
-
-    /**
-     * Get the related OffloadIndexEntry that contains the given messageEntryId.
-     *
-     * @param messageEntryId
-     *                      the entry id of message
-     * @return the offload index entry
-     */
-    OffloadIndexEntry getIndexEntryForEntry(long ledgerId, long messageEntryId) throws IOException;
-
-    long getStartEntryId(long ledgerId);
-
-    /**
-     * Get the entry count that contained in this index Block.
-     */
-    int getEntryCount();
-
-    /**
-     * Get LedgerMetadata.
-     * @return
-     */
-    LedgerMetadata getLedgerMetadata(long ledgerId);
-
-    /**
-     * Get the total size of the data object.
-     */
-    long getDataObjectLength();
-
-    /**
-     * Get the length of the header in the blocks in the data object.
-     */
-    long getDataBlockHeaderLength();
-}
-
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud;
+
+import java.io.Closeable;
+import java.io.IOException;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;
+
+/**
+ * The Index block abstraction used for offload a ledger to long term storage.
+ */
+@Unstable
+public interface OffloadIndexBlockV2 extends Closeable {
+
+    /**
+     * Get the content of the index block as InputStream.
+     * Read out in format:
+     *   | index_magic_header | index_block_len | index_entry_count |
+     *   | data_object_size | segment_metadata_length | segment metadata | index entries ... |
+     */
+    OffloadIndexBlock.IndexInputStream toStream() throws IOException;
+
+    /**
+     * Get the related OffloadIndexEntry that contains the given messageEntryId.
+     *
+     * @param messageEntryId
+     *                      the entry id of message
+     * @return the offload index entry
+     */
+    OffloadIndexEntry getIndexEntryForEntry(long ledgerId, long messageEntryId) throws IOException;
+
+    long getStartEntryId(long ledgerId);
+
+    /**
+     * Get the entry count that contained in this index Block.
+     */
+    int getEntryCount();
+
+    /**
+     * Get LedgerMetadata.
+     * @return
+     */
+    LedgerMetadata getLedgerMetadata(long ledgerId);
+
+    /**
+     * Get the total size of the data object.
+     */
+    long getDataObjectLength();
+
+    /**
+     * Get the length of the header in the blocks in the data object.
+     */
+    long getDataBlockHeaderLength();
+}
+
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockV2Builder.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockV2Builder.java
index 8373b7eb54..0b3700ad86 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockV2Builder.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockV2Builder.java
@@ -1,83 +1,83 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud;
-
-import java.io.IOException;
-import java.io.InputStream;
-import org.apache.bookkeeper.common.annotation.InterfaceAudience.LimitedPrivate;
-import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;
-import org.apache.bookkeeper.mledger.offload.jcloud.impl.OffloadIndexBlockV2BuilderImpl;
-import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;
-
-/**
- * Interface for builder of index block used for offload a ledger to long term storage.
- */
-@Unstable
-@LimitedPrivate
-public interface OffloadIndexBlockV2Builder {
-
-    /**
-     * Build index block with the passed in ledger metadata.
-     *
-     * @param ledgerId
-     * @param metadata the ledger metadata
-     */
-    OffloadIndexBlockV2Builder addLedgerMeta(Long ledgerId, LedgerInfo metadata);
-
-    /**
-     * Add one payload block related information into index block.
-     * It contains the first entryId in payload block, the payload block Id,
-     * and payload block size.
-     * This information will be used to consist one index entry in OffloadIndexBlock.
-     *
-     * @param firstEntryId the first entryId in payload block
-     * @param partId the payload block Id
-     * @param blockSize the payload block size
-     */
-    OffloadIndexBlockV2Builder addBlock(long ledgerId, long firstEntryId, int partId, int blockSize);
-
-    /**
-     * Specify the length of data object this index is associated with.
-     * @param dataObjectLength the length of the data object
-     */
-    OffloadIndexBlockV2Builder withDataObjectLength(long dataObjectLength);
-
-    /**
-     * Specify the length of the block headers in the data object.
-     * @param dataHeaderLength the length of the headers
-     */
-    OffloadIndexBlockV2Builder withDataBlockHeaderLength(long dataHeaderLength);
-
-    /**
-     * Finalize the immutable OffloadIndexBlock.
-     */
-    OffloadIndexBlockV2 buildV2();
-
-    /**
-     * Construct OffloadIndex from an InputStream.
-     */
-    OffloadIndexBlockV2 fromStream(InputStream is) throws IOException;
-
-    /**
-     * create an OffloadIndexBlockBuilder.
-     */
-    static OffloadIndexBlockV2Builder create() {
-        return new OffloadIndexBlockV2BuilderImpl();
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud;
+
+import java.io.IOException;
+import java.io.InputStream;
+import org.apache.bookkeeper.common.annotation.InterfaceAudience.LimitedPrivate;
+import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;
+import org.apache.bookkeeper.mledger.offload.jcloud.impl.OffloadIndexBlockV2BuilderImpl;
+import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;
+
+/**
+ * Interface for builder of index block used for offload a ledger to long term storage.
+ */
+@Unstable
+@LimitedPrivate
+public interface OffloadIndexBlockV2Builder {
+
+    /**
+     * Build index block with the passed in ledger metadata.
+     *
+     * @param ledgerId
+     * @param metadata the ledger metadata
+     */
+    OffloadIndexBlockV2Builder addLedgerMeta(Long ledgerId, LedgerInfo metadata);
+
+    /**
+     * Add one payload block related information into index block.
+     * It contains the first entryId in payload block, the payload block Id,
+     * and payload block size.
+     * This information will be used to consist one index entry in OffloadIndexBlock.
+     *
+     * @param firstEntryId the first entryId in payload block
+     * @param partId the payload block Id
+     * @param blockSize the payload block size
+     */
+    OffloadIndexBlockV2Builder addBlock(long ledgerId, long firstEntryId, int partId, int blockSize);
+
+    /**
+     * Specify the length of data object this index is associated with.
+     * @param dataObjectLength the length of the data object
+     */
+    OffloadIndexBlockV2Builder withDataObjectLength(long dataObjectLength);
+
+    /**
+     * Specify the length of the block headers in the data object.
+     * @param dataHeaderLength the length of the headers
+     */
+    OffloadIndexBlockV2Builder withDataBlockHeaderLength(long dataHeaderLength);
+
+    /**
+     * Finalize the immutable OffloadIndexBlock.
+     */
+    OffloadIndexBlockV2 buildV2();
+
+    /**
+     * Construct OffloadIndex from an InputStream.
+     */
+    OffloadIndexBlockV2 fromStream(InputStream is) throws IOException;
+
+    /**
+     * create an OffloadIndexBlockBuilder.
+     */
+    static OffloadIndexBlockV2Builder create() {
+        return new OffloadIndexBlockV2BuilderImpl();
+    }
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexEntry.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexEntry.java
index 935bb54f60..3ab8066db4 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexEntry.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexEntry.java
@@ -1,53 +1,53 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud;
-
-import org.apache.bookkeeper.common.annotation.InterfaceAudience.LimitedPrivate;
-import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;
-
-/**
- * The Index Entry in OffloadIndexBlock.
- * It consists of the message entry id, the tiered storage block part id for this message entry,
- * and the offset in tiered storage block for this message id.
- */
-@Unstable
-@LimitedPrivate
-public interface OffloadIndexEntry {
-
-    /**
-     * Get the entryId that this entry contains.
-     */
-    long getEntryId();
-
-    /**
-     * Get the block part id of tiered storage.
-     */
-    int getPartId();
-
-    /**
-     * Get the offset of this block within the object.
-     */
-    long getOffset();
-
-    /**
-     * Get the offset of the block's data within the object.
-     */
-    long getDataOffset();
-}
-
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud;
+
+import org.apache.bookkeeper.common.annotation.InterfaceAudience.LimitedPrivate;
+import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;
+
+/**
+ * The Index Entry in OffloadIndexBlock.
+ * It consists of the message entry id, the tiered storage block part id for this message entry,
+ * and the offset in tiered storage block for this message id.
+ */
+@Unstable
+@LimitedPrivate
+public interface OffloadIndexEntry {
+
+    /**
+     * Get the entryId that this entry contains.
+     */
+    long getEntryId();
+
+    /**
+     * Get the block part id of tiered storage.
+     */
+    int getPartId();
+
+    /**
+     * Get the offset of this block within the object.
+     */
+    long getOffset();
+
+    /**
+     * Get the offset of the block's data within the object.
+     */
+    long getDataOffset();
+}
+
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedInputStreamImpl.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedInputStreamImpl.java
index 6ebbe5bce5..5acf4c4ce8 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedInputStreamImpl.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedInputStreamImpl.java
@@ -1,211 +1,211 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import io.netty.buffer.ByteBuf;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.concurrent.TimeUnit;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;
-import org.apache.bookkeeper.mledger.offload.jcloud.impl.DataBlockUtils.VersionCheck;
-import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
-import org.apache.pulsar.common.naming.TopicName;
-import org.jclouds.blobstore.BlobStore;
-import org.jclouds.blobstore.KeyNotFoundException;
-import org.jclouds.blobstore.domain.Blob;
-import org.jclouds.blobstore.options.GetOptions;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class BlobStoreBackedInputStreamImpl extends BackedInputStream {
-    private static final Logger log = LoggerFactory.getLogger(BlobStoreBackedInputStreamImpl.class);
-
-    private final BlobStore blobStore;
-    private final String bucket;
-    private final String key;
-    private final VersionCheck versionCheck;
-    private final ByteBuf buffer;
-    private final long objectLen;
-    private final int bufferSize;
-    private LedgerOffloaderStats offloaderStats;
-    private String managedLedgerName;
-    private String topicName;
-
-    private long cursor;
-    private long bufferOffsetStart;
-    private long bufferOffsetEnd;
-
-    public BlobStoreBackedInputStreamImpl(BlobStore blobStore, String bucket, String key,
-                                          VersionCheck versionCheck,
-                                          long objectLen, int bufferSize) {
-        this.blobStore = blobStore;
-        this.bucket = bucket;
-        this.key = key;
-        this.versionCheck = versionCheck;
-        this.buffer = PulsarByteBufAllocator.DEFAULT.buffer(bufferSize, bufferSize);
-        this.objectLen = objectLen;
-        this.bufferSize = bufferSize;
-        this.cursor = 0;
-        this.bufferOffsetStart = this.bufferOffsetEnd = -1;
-    }
-
-
-    public BlobStoreBackedInputStreamImpl(BlobStore blobStore, String bucket, String key,
-                                          VersionCheck versionCheck,
-                                          long objectLen, int bufferSize,
-                                          LedgerOffloaderStats offloaderStats, String managedLedgerName) {
-        this(blobStore, bucket, key, versionCheck, objectLen, bufferSize);
-        this.offloaderStats = offloaderStats;
-        this.managedLedgerName = managedLedgerName;
-        this.topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
-    }
-
-    /**
-     * Refill the buffered input if it is empty.
-     * @return true if there are bytes to read, false otherwise
-     */
-    private boolean refillBufferIfNeeded() throws IOException {
-        if (buffer.readableBytes() == 0) {
-            if (cursor >= objectLen) {
-                return false;
-            }
-            long startRange = cursor;
-            long endRange = Math.min(cursor + bufferSize - 1,
-                                     objectLen - 1);
-            if (log.isDebugEnabled()) {
-                log.info("refillBufferIfNeeded {} - {} ({} bytes to fill)",
-                        startRange, endRange, (endRange - startRange));
-            }
-            try {
-                long startReadTime = System.nanoTime();
-                Blob blob = blobStore.getBlob(bucket, key, new GetOptions().range(startRange, endRange));
-                if (blob == null) {
-                    throw new KeyNotFoundException(bucket, key, "");
-                }
-                versionCheck.check(key, blob);
-
-                try (InputStream stream = blob.getPayload().openStream()) {
-                    buffer.clear();
-                    bufferOffsetStart = startRange;
-                    bufferOffsetEnd = endRange;
-                    long bytesRead = endRange - startRange + 1;
-                    int bytesToCopy = (int) bytesRead;
-                    fillBuffer(stream, bytesToCopy);
-                    cursor += buffer.readableBytes();
-                }
-
-                // here we can get the metrics
-                // because JClouds streams the content
-                // and actually the HTTP call finishes when the stream is fully read
-                if (this.offloaderStats != null) {
-                    this.offloaderStats.recordReadOffloadDataLatency(topicName,
-                            System.nanoTime() - startReadTime, TimeUnit.NANOSECONDS);
-                    this.offloaderStats.recordReadOffloadBytes(topicName, endRange - startRange + 1);
-                }
-            } catch (Throwable e) {
-                if (null != this.offloaderStats) {
-                    this.offloaderStats.recordReadOffloadError(this.topicName);
-                }
-                // If the blob is not found, the original exception is thrown and handled by the caller.
-                if (e instanceof KeyNotFoundException) {
-                    throw e;
-                }
-                throw new IOException("Error reading from BlobStore", e);
-            }
-        }
-        return true;
-    }
-
-    void fillBuffer(InputStream is, int bytesToCopy) throws IOException {
-        while (bytesToCopy > 0) {
-            int writeBytes = buffer.writeBytes(is, bytesToCopy);
-            if (writeBytes < 0) {
-                break;
-            }
-            bytesToCopy -= writeBytes;
-        }
-    }
-
-    ByteBuf getBuffer() {
-        return buffer;
-    }
-
-    @Override
-    public int read() throws IOException {
-        if (refillBufferIfNeeded()) {
-            return buffer.readUnsignedByte();
-        } else {
-            return -1;
-        }
-    }
-
-    @Override
-    public int read(byte[] b, int off, int len) throws IOException {
-        if (refillBufferIfNeeded()) {
-            int bytesToRead = Math.min(len, buffer.readableBytes());
-            buffer.readBytes(b, off, bytesToRead);
-            return bytesToRead;
-        } else {
-            return -1;
-        }
-    }
-
-    @Override
-    public void seek(long position) {
-        log.debug("Seeking to {} on {}/{}, current position {} (bufStart:{}, bufEnd:{})",
-                position, bucket, key, cursor, bufferOffsetStart, bufferOffsetEnd);
-        if (position >= bufferOffsetStart && position <= bufferOffsetEnd) {
-            long newIndex = position - bufferOffsetStart;
-            buffer.readerIndex((int) newIndex);
-        } else {
-            bufferOffsetStart = bufferOffsetEnd = -1;
-            this.cursor = position;
-            buffer.clear();
-        }
-    }
-
-    @Override
-    public void seekForward(long position) throws IOException {
-        if (position >= cursor) {
-            seek(position);
-        } else {
-            throw new IOException(String.format("Error seeking, new position %d < current position %d",
-                                                position, cursor));
-        }
-    }
-
-    public long getCurrentPosition() {
-        if (bufferOffsetStart != -1) {
-            return bufferOffsetStart + buffer.readerIndex();
-        }
-        return cursor + buffer.readerIndex();
-    }
-
-    @Override
-    public void close() {
-        buffer.release();
-    }
-
-    @Override
-    public int available() throws IOException {
-        long available = objectLen - cursor + buffer.readableBytes();
-        return available > Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) available;
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import io.netty.buffer.ByteBuf;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.concurrent.TimeUnit;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;
+import org.apache.bookkeeper.mledger.offload.jcloud.impl.DataBlockUtils.VersionCheck;
+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
+import org.apache.pulsar.common.naming.TopicName;
+import org.jclouds.blobstore.BlobStore;
+import org.jclouds.blobstore.KeyNotFoundException;
+import org.jclouds.blobstore.domain.Blob;
+import org.jclouds.blobstore.options.GetOptions;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class BlobStoreBackedInputStreamImpl extends BackedInputStream {
+    private static final Logger log = LoggerFactory.getLogger(BlobStoreBackedInputStreamImpl.class);
+
+    private final BlobStore blobStore;
+    private final String bucket;
+    private final String key;
+    private final VersionCheck versionCheck;
+    private final ByteBuf buffer;
+    private final long objectLen;
+    private final int bufferSize;
+    private LedgerOffloaderStats offloaderStats;
+    private String managedLedgerName;
+    private String topicName;
+
+    private long cursor;
+    private long bufferOffsetStart;
+    private long bufferOffsetEnd;
+
+    public BlobStoreBackedInputStreamImpl(BlobStore blobStore, String bucket, String key,
+                                          VersionCheck versionCheck,
+                                          long objectLen, int bufferSize) {
+        this.blobStore = blobStore;
+        this.bucket = bucket;
+        this.key = key;
+        this.versionCheck = versionCheck;
+        this.buffer = PulsarByteBufAllocator.DEFAULT.buffer(bufferSize, bufferSize);
+        this.objectLen = objectLen;
+        this.bufferSize = bufferSize;
+        this.cursor = 0;
+        this.bufferOffsetStart = this.bufferOffsetEnd = -1;
+    }
+
+
+    public BlobStoreBackedInputStreamImpl(BlobStore blobStore, String bucket, String key,
+                                          VersionCheck versionCheck,
+                                          long objectLen, int bufferSize,
+                                          LedgerOffloaderStats offloaderStats, String managedLedgerName) {
+        this(blobStore, bucket, key, versionCheck, objectLen, bufferSize);
+        this.offloaderStats = offloaderStats;
+        this.managedLedgerName = managedLedgerName;
+        this.topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
+    }
+
+    /**
+     * Refill the buffered input if it is empty.
+     * @return true if there are bytes to read, false otherwise
+     */
+    private boolean refillBufferIfNeeded() throws IOException {
+        if (buffer.readableBytes() == 0) {
+            if (cursor >= objectLen) {
+                return false;
+            }
+            long startRange = cursor;
+            long endRange = Math.min(cursor + bufferSize - 1,
+                                     objectLen - 1);
+            if (log.isDebugEnabled()) {
+                log.info("refillBufferIfNeeded {} - {} ({} bytes to fill)",
+                        startRange, endRange, (endRange - startRange));
+            }
+            try {
+                long startReadTime = System.nanoTime();
+                Blob blob = blobStore.getBlob(bucket, key, new GetOptions().range(startRange, endRange));
+                if (blob == null) {
+                    throw new KeyNotFoundException(bucket, key, "");
+                }
+                versionCheck.check(key, blob);
+
+                try (InputStream stream = blob.getPayload().openStream()) {
+                    buffer.clear();
+                    bufferOffsetStart = startRange;
+                    bufferOffsetEnd = endRange;
+                    long bytesRead = endRange - startRange + 1;
+                    int bytesToCopy = (int) bytesRead;
+                    fillBuffer(stream, bytesToCopy);
+                    cursor += buffer.readableBytes();
+                }
+
+                // here we can get the metrics
+                // because JClouds streams the content
+                // and actually the HTTP call finishes when the stream is fully read
+                if (this.offloaderStats != null) {
+                    this.offloaderStats.recordReadOffloadDataLatency(topicName,
+                            System.nanoTime() - startReadTime, TimeUnit.NANOSECONDS);
+                    this.offloaderStats.recordReadOffloadBytes(topicName, endRange - startRange + 1);
+                }
+            } catch (Throwable e) {
+                if (null != this.offloaderStats) {
+                    this.offloaderStats.recordReadOffloadError(this.topicName);
+                }
+                // If the blob is not found, the original exception is thrown and handled by the caller.
+                if (e instanceof KeyNotFoundException) {
+                    throw e;
+                }
+                throw new IOException("Error reading from BlobStore", e);
+            }
+        }
+        return true;
+    }
+
+    void fillBuffer(InputStream is, int bytesToCopy) throws IOException {
+        while (bytesToCopy > 0) {
+            int writeBytes = buffer.writeBytes(is, bytesToCopy);
+            if (writeBytes < 0) {
+                break;
+            }
+            bytesToCopy -= writeBytes;
+        }
+    }
+
+    ByteBuf getBuffer() {
+        return buffer;
+    }
+
+    @Override
+    public int read() throws IOException {
+        if (refillBufferIfNeeded()) {
+            return buffer.readUnsignedByte();
+        } else {
+            return -1;
+        }
+    }
+
+    @Override
+    public int read(byte[] b, int off, int len) throws IOException {
+        if (refillBufferIfNeeded()) {
+            int bytesToRead = Math.min(len, buffer.readableBytes());
+            buffer.readBytes(b, off, bytesToRead);
+            return bytesToRead;
+        } else {
+            return -1;
+        }
+    }
+
+    @Override
+    public void seek(long position) {
+        log.debug("Seeking to {} on {}/{}, current position {} (bufStart:{}, bufEnd:{})",
+                position, bucket, key, cursor, bufferOffsetStart, bufferOffsetEnd);
+        if (position >= bufferOffsetStart && position <= bufferOffsetEnd) {
+            long newIndex = position - bufferOffsetStart;
+            buffer.readerIndex((int) newIndex);
+        } else {
+            bufferOffsetStart = bufferOffsetEnd = -1;
+            this.cursor = position;
+            buffer.clear();
+        }
+    }
+
+    @Override
+    public void seekForward(long position) throws IOException {
+        if (position >= cursor) {
+            seek(position);
+        } else {
+            throw new IOException(String.format("Error seeking, new position %d < current position %d",
+                                                position, cursor));
+        }
+    }
+
+    public long getCurrentPosition() {
+        if (bufferOffsetStart != -1) {
+            return bufferOffsetStart + buffer.readerIndex();
+        }
+        return cursor + buffer.readerIndex();
+    }
+
+    @Override
+    public void close() {
+        buffer.release();
+    }
+
+    @Override
+    public int available() throws IOException {
+        long available = objectLen - cursor + buffer.readableBytes();
+        return available > Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) available;
+    }
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedReadHandleImpl.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedReadHandleImpl.java
index 77bcfa5cdd..05bd027829 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedReadHandleImpl.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedReadHandleImpl.java
@@ -1,455 +1,455 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import com.google.common.annotations.VisibleForTesting;
-import io.netty.buffer.ByteBuf;
-import java.io.DataInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.ScheduledExecutorService;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;
-import java.util.concurrent.atomic.AtomicReference;
-import org.apache.bookkeeper.client.BKException;
-import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;
-import org.apache.bookkeeper.client.api.LedgerEntries;
-import org.apache.bookkeeper.client.api.LedgerEntry;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.client.api.ReadHandle;
-import org.apache.bookkeeper.client.impl.LedgerEntriesImpl;
-import org.apache.bookkeeper.client.impl.LedgerEntryImpl;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.bookkeeper.mledger.ManagedLedgerException;
-import org.apache.bookkeeper.mledger.OffloadedLedgerHandle;
-import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockBuilder;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;
-import org.apache.bookkeeper.mledger.offload.jcloud.impl.DataBlockUtils.VersionCheck;
-import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
-import org.apache.pulsar.common.naming.TopicName;
-import org.jclouds.blobstore.BlobStore;
-import org.jclouds.blobstore.KeyNotFoundException;
-import org.jclouds.blobstore.domain.Blob;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class BlobStoreBackedReadHandleImpl implements ReadHandle, OffloadedLedgerHandle {
-    private static final Logger log = LoggerFactory.getLogger(BlobStoreBackedReadHandleImpl.class);
-
-    protected static final AtomicIntegerFieldUpdater<BlobStoreBackedReadHandleImpl> PENDING_READ_UPDATER =
-            AtomicIntegerFieldUpdater.newUpdater(BlobStoreBackedReadHandleImpl.class, "pendingRead");
-
-    private final long ledgerId;
-    private final OffloadIndexBlock index;
-    private final BackedInputStream inputStream;
-    private final DataInputStream dataStream;
-    private final ExecutorService executor;
-    private final OffsetsCache entryOffsetsCache;
-    private final AtomicReference<CompletableFuture<Void>> closeFuture = new AtomicReference<>();
-
-    enum State {
-        Opened,
-        Closed
-    }
-
-    private volatile State state = null;
-
-    private volatile int pendingRead;
-
-    private volatile long lastAccessTimestamp = System.currentTimeMillis();
-
-    @VisibleForTesting
-    BlobStoreBackedReadHandleImpl(long ledgerId, OffloadIndexBlock index,
-                                          BackedInputStream inputStream, ExecutorService executor,
-                                          OffsetsCache entryOffsetsCache) {
-        this.ledgerId = ledgerId;
-        this.index = index;
-        this.inputStream = inputStream;
-        this.dataStream = new DataInputStream(inputStream);
-        this.executor = executor;
-        this.entryOffsetsCache = entryOffsetsCache;
-        state = State.Opened;
-    }
-
-    @Override
-    public long getId() {
-        return ledgerId;
-    }
-
-    @Override
-    public LedgerMetadata getLedgerMetadata() {
-        return index.getLedgerMetadata();
-    }
-
-    @Override
-    public CompletableFuture<Void> closeAsync() {
-        if (closeFuture.get() != null || !closeFuture.compareAndSet(null, new CompletableFuture<>())) {
-            return closeFuture.get();
-        }
-
-        CompletableFuture<Void> promise = closeFuture.get();
-        executor.execute(() -> {
-            try {
-                index.close();
-                inputStream.close();
-                state = State.Closed;
-                promise.complete(null);
-            } catch (IOException t) {
-                promise.completeExceptionally(t);
-            }
-        });
-        return promise;
-    }
-
-    private class ReadTask implements Runnable {
-        private final long firstEntry;
-        private final long lastEntry;
-        private final CompletableFuture<LedgerEntries> promise;
-        private int seekedAndTryTimes = 0;
-
-        public ReadTask(long firstEntry, long lastEntry, CompletableFuture<LedgerEntries> promise) {
-            this.firstEntry = firstEntry;
-            this.lastEntry = lastEntry;
-            this.promise = promise;
-        }
-
-        @Override
-        public void run() {
-            if (state == State.Closed) {
-                log.warn("Reading a closed read handler. Ledger ID: {}, Read range: {}-{}",
-                        ledgerId, firstEntry, lastEntry);
-                promise.completeExceptionally(new ManagedLedgerException.OffloadReadHandleClosedException());
-                return;
-            }
-
-            List<LedgerEntry> entryCollector = new ArrayList<LedgerEntry>();
-            try {
-                if (firstEntry > lastEntry
-                        || firstEntry < 0
-                        || lastEntry > getLastAddConfirmed()) {
-                    promise.completeExceptionally(new BKException.BKIncorrectParameterException());
-                    return;
-                }
-                long entriesToRead = (lastEntry - firstEntry) + 1;
-                long expectedEntryId = firstEntry;
-                seekToEntryOffset(firstEntry);
-                seekedAndTryTimes++;
-
-                while (entriesToRead > 0) {
-                    long currentPosition = inputStream.getCurrentPosition();
-                    int length = dataStream.readInt();
-                    if (length < 0) { // hit padding or new block
-                        seekToEntryOffset(expectedEntryId);
-                        continue;
-                    }
-                    long entryId = dataStream.readLong();
-                    if (entryId == expectedEntryId) {
-                        entryOffsetsCache.put(ledgerId, entryId, currentPosition);
-                        ByteBuf buf = PulsarByteBufAllocator.DEFAULT.buffer(length, length);
-                        entryCollector.add(LedgerEntryImpl.create(ledgerId, entryId, length, buf));
-                        int toWrite = length;
-                        while (toWrite > 0) {
-                            toWrite -= buf.writeBytes(dataStream, toWrite);
-                        }
-                        entriesToRead--;
-                        expectedEntryId++;
-                    } else {
-                        handleUnexpectedEntryId(expectedEntryId, entryId);
-                    }
-                }
-                promise.complete(LedgerEntriesImpl.create(entryCollector));
-            } catch (Throwable t) {
-                log.error("Failed to read entries {} - {} from the offloader in ledger {}, current position of input"
-                        + " stream is {}", firstEntry, lastEntry, ledgerId, inputStream.getCurrentPosition(), t);
-                if (t instanceof KeyNotFoundException) {
-                    promise.completeExceptionally(new BKException.BKNoSuchLedgerExistsException());
-                } else {
-                    promise.completeExceptionally(t);
-                }
-                entryCollector.forEach(LedgerEntry::close);
-            }
-        }
-
-        // in the normal case, the entry id should increment in order. But if there has random access in
-        // the read method, we should allow to seek to the right position and the entry id should
-        // never over to the last entry again.
-        private void handleUnexpectedEntryId(long expectedId, long actEntryId) throws Exception {
-            LedgerMetadata ledgerMetadata = getLedgerMetadata();
-            OffloadIndexEntry offsetOfExpectedId = index.getIndexEntryForEntry(expectedId);
-            OffloadIndexEntry offsetOfActId = actEntryId <= getLedgerMetadata().getLastEntryId() && actEntryId >= 0
-                    ? index.getIndexEntryForEntry(actEntryId) : null;
-            String logLine = String.format("Failed to read [ %s ~ %s ] of the ledger %s."
-                    + " Because got a incorrect entry id %s, the offset is %s."
-                    + " The expected entry id is %s, the offset is %s."
-                    + " Have seeked and retry read times: %s. LAC is %s.",
-                    firstEntry, lastEntry, ledgerId,
-                    actEntryId, offsetOfActId == null ? "null because it does not exist"
-                            : String.valueOf(offsetOfActId),
-                    expectedId, String.valueOf(offsetOfExpectedId),
-                    seekedAndTryTimes, ledgerMetadata != null ? ledgerMetadata.getLastEntryId() : "unknown");
-            // If it still fails after tried entries count times, throw the exception.
-            long maxTryTimes = Math.max(3, (lastEntry - firstEntry + 1) >> 2);
-            if (seekedAndTryTimes > maxTryTimes) {
-                log.error(logLine);
-                throw new BKException.BKUnexpectedConditionException();
-            } else {
-                log.warn(logLine);
-            }
-            seekToEntryOffset(expectedId);
-            seekedAndTryTimes++;
-        }
-
-        private void skipPreviousEntry(long startEntryId, long expectedEntryId) throws IOException, BKException {
-            long nextExpectedEntryId = startEntryId;
-            while (nextExpectedEntryId < expectedEntryId) {
-                long offset = inputStream.getCurrentPosition();
-                int len = dataStream.readInt();
-                if (len < 0) {
-                    LedgerMetadata ledgerMetadata = getLedgerMetadata();
-                    OffloadIndexEntry offsetOfExpectedId = index.getIndexEntryForEntry(expectedEntryId);
-                    log.error("Failed to read [ {} ~ {} ] of the ledger {}."
-                        + " Because failed to skip a previous entry {}, len: {}, got a negative len."
-                        + " The expected entry id is {}, the offset is {}."
-                        + " Have seeked and retry read times: {}. LAC is {}.",
-                        firstEntry, lastEntry, ledgerId,
-                        nextExpectedEntryId, len,
-                        expectedEntryId, String.valueOf(offsetOfExpectedId),
-                        seekedAndTryTimes, ledgerMetadata != null ? ledgerMetadata.getLastEntryId() : "unknown");
-                    throw new BKException.BKUnexpectedConditionException();
-                }
-                long entryId = dataStream.readLong();
-                if (entryId == nextExpectedEntryId) {
-                    long skipped = inputStream.skip(len);
-                    if (skipped != len) {
-                        LedgerMetadata ledgerMetadata = getLedgerMetadata();
-                        OffloadIndexEntry offsetOfExpectedId = index.getIndexEntryForEntry(expectedEntryId);
-                        log.error("Failed to read [ {} ~ {} ] of the ledger {}."
-                            + " Because failed to skip a previous entry {}, offset: {}, len: {}, there is no more data."
-                            + " The expected entry id is {}, the offset is {}."
-                            + " Have seeked and retry read times: {}. LAC is {}.",
-                            firstEntry, lastEntry, ledgerId,
-                            entryId, offset, len,
-                            expectedEntryId, String.valueOf(offsetOfExpectedId),
-                            seekedAndTryTimes, ledgerMetadata != null ? ledgerMetadata.getLastEntryId() : "unknown");
-                        throw new BKException.BKUnexpectedConditionException();
-                    }
-                    nextExpectedEntryId++;
-                } else {
-                    LedgerMetadata ledgerMetadata = getLedgerMetadata();
-                    OffloadIndexEntry offsetOfExpectedId = index.getIndexEntryForEntry(expectedEntryId);
-                    log.error("Failed to read [ {} ~ {} ] of the ledger {}."
-                        + " Because got a incorrect entry id {},."
-                        + " The expected entry id is {}, the offset is {}."
-                        + " Have seeked and retry read times: {}. LAC is {}.",
-                        firstEntry, lastEntry, ledgerId,
-                        entryId, expectedEntryId, String.valueOf(offsetOfExpectedId),
-                        seekedAndTryTimes, ledgerMetadata != null ? ledgerMetadata.getLastEntryId() : "unknown");
-                    throw new BKException.BKUnexpectedConditionException();
-                }
-            }
-        }
-
-        private void seekToEntryOffset(long expectedEntryId) throws IOException, BKException {
-            // 1. Try to find the precise index.
-            // 1-1. Precise cached indexes.
-            Long cachedPreciseIndex = entryOffsetsCache.getIfPresent(ledgerId, expectedEntryId);
-            if (cachedPreciseIndex != null) {
-                inputStream.seek(cachedPreciseIndex);
-                return;
-            }
-            // 1-2. Precise persistent indexes.
-            OffloadIndexEntry indexOfNearestEntry = index.getIndexEntryForEntry(expectedEntryId);
-            if (indexOfNearestEntry.getEntryId() == expectedEntryId) {
-                inputStream.seek(indexOfNearestEntry.getDataOffset());
-                return;
-            }
-            // 2. Try to use the previous index. Since the entry-0 must have a precise index, we can skip to check
-            //    whether "expectedEntryId" is larger than 0;
-            Long cachedPreviousKnownOffset = entryOffsetsCache.getIfPresent(ledgerId, expectedEntryId - 1);
-            if (cachedPreviousKnownOffset != null) {
-                inputStream.seek(cachedPreviousKnownOffset);
-                skipPreviousEntry(expectedEntryId - 1, expectedEntryId);
-                return;
-            }
-            // 3. Use the persistent index of the nearest entry that is smaller than "expectedEntryId".
-            //    Because it is a sparse index, some entries need to be skipped.
-            if (indexOfNearestEntry.getEntryId() < expectedEntryId) {
-                inputStream.seek(indexOfNearestEntry.getDataOffset());
-                skipPreviousEntry(indexOfNearestEntry.getEntryId(), expectedEntryId);
-            } else {
-                LedgerMetadata ledgerMetadata = getLedgerMetadata();
-                log.error("Failed to read [ {} ~ {} ] of the ledger {}."
-                    + " Because got a incorrect index {} of the entry {}, which is greater than expected."
-                    + " Have seeked and retry read times: {}. LAC is {}.",
-                    firstEntry, lastEntry, ledgerId,
-                    String.valueOf(indexOfNearestEntry), expectedEntryId,
-                    seekedAndTryTimes, ledgerMetadata != null ? ledgerMetadata.getLastEntryId() : "unknown");
-                throw new BKException.BKUnexpectedConditionException();
-            }
-        }
-    }
-
-    @Override
-    public CompletableFuture<LedgerEntries> readAsync(long firstEntry, long lastEntry) {
-        if (log.isDebugEnabled()) {
-            log.debug("Ledger {}: reading {} - {} ({} entries}",
-                    getId(), firstEntry, lastEntry, (1 + lastEntry - firstEntry));
-        }
-        CompletableFuture<LedgerEntries> promise = new CompletableFuture<>();
-
-        // Ledger handles will be only marked idle when "pendingRead" is "0", it is not needed to update
-        // "lastAccessTimestamp" if "pendingRead" is larger than "0".
-        // Rather than update "lastAccessTimestamp" when starts a reading, updating it when a reading task is finished
-        // is better.
-        PENDING_READ_UPDATER.incrementAndGet(this);
-        promise.whenComplete((__, ex) -> {
-            lastAccessTimestamp = System.currentTimeMillis();
-            PENDING_READ_UPDATER.decrementAndGet(BlobStoreBackedReadHandleImpl.this);
-        });
-        executor.execute(new ReadTask(firstEntry, lastEntry, promise));
-        return promise;
-    }
-
-    private void seekToEntry(long nextExpectedId) throws IOException {
-        Long knownOffset = entryOffsetsCache.getIfPresent(ledgerId, nextExpectedId);
-        if (knownOffset != null) {
-            inputStream.seek(knownOffset);
-        } else {
-            // we don't know the exact position
-            // we seek to somewhere before the entry
-            long dataOffset = index.getIndexEntryForEntry(nextExpectedId).getDataOffset();
-            inputStream.seek(dataOffset);
-        }
-    }
-
-    private void seekToEntry(OffloadIndexEntry offloadIndexEntry) throws IOException {
-        long dataOffset = offloadIndexEntry.getDataOffset();
-        inputStream.seek(dataOffset);
-    }
-
-    @Override
-    public CompletableFuture<LedgerEntries> readUnconfirmedAsync(long firstEntry, long lastEntry) {
-        return readAsync(firstEntry, lastEntry);
-    }
-
-    @Override
-    public CompletableFuture<Long> readLastAddConfirmedAsync() {
-        return CompletableFuture.completedFuture(getLastAddConfirmed());
-    }
-
-    @Override
-    public CompletableFuture<Long> tryReadLastAddConfirmedAsync() {
-        return CompletableFuture.completedFuture(getLastAddConfirmed());
-    }
-
-    @Override
-    public long getLastAddConfirmed() {
-        return getLedgerMetadata().getLastEntryId();
-    }
-
-    @Override
-    public long getLength() {
-        return getLedgerMetadata().getLength();
-    }
-
-    @Override
-    public boolean isClosed() {
-        return getLedgerMetadata().isClosed();
-    }
-
-    @Override
-    public CompletableFuture<LastConfirmedAndEntry> readLastAddConfirmedAndEntryAsync(long entryId,
-                                                                                      long timeOutInMillis,
-                                                                                      boolean parallel) {
-        CompletableFuture<LastConfirmedAndEntry> promise = new CompletableFuture<>();
-        promise.completeExceptionally(new UnsupportedOperationException());
-        return promise;
-    }
-
-    public static ReadHandle open(ScheduledExecutorService executor,
-                                  BlobStore blobStore, String bucket, String key, String indexKey,
-                                  VersionCheck versionCheck,
-                                  long ledgerId, int readBufferSize,
-                                  LedgerOffloaderStats offloaderStats, String managedLedgerName,
-                                  OffsetsCache entryOffsetsCache)
-            throws IOException, BKException.BKNoSuchLedgerExistsException {
-        int retryCount = 3;
-        OffloadIndexBlock index = null;
-        IOException lastException = null;
-        String topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
-        // The following retry is used to avoid to some network issue cause read index file failure.
-        // If it can not recovery in the retry, we will throw the exception and the dispatcher will schedule to
-        // next read.
-        // If we use a backoff to control the retry, it will introduce a concurrent operation.
-        // We don't want to make it complicated, because in the most of case it shouldn't in the retry loop.
-        while (retryCount-- > 0) {
-            long readIndexStartTime = System.nanoTime();
-            Blob blob = blobStore.getBlob(bucket, indexKey);
-            if (blob == null) {
-                log.error("{} not found in container {}", indexKey, bucket);
-                throw new BKException.BKNoSuchLedgerExistsException();
-            }
-            offloaderStats.recordReadOffloadIndexLatency(topicName,
-                    System.nanoTime() - readIndexStartTime, TimeUnit.NANOSECONDS);
-            versionCheck.check(indexKey, blob);
-            OffloadIndexBlockBuilder indexBuilder = OffloadIndexBlockBuilder.create();
-            try (InputStream payLoadStream = blob.getPayload().openStream()) {
-                index = (OffloadIndexBlock) indexBuilder.fromStream(payLoadStream);
-            } catch (IOException e) {
-                // retry to avoid the network issue caused read failure
-                log.warn("Failed to get index block from the offoaded index file {}, still have {} times to retry",
-                    indexKey, retryCount, e);
-                lastException = e;
-                continue;
-            }
-            lastException = null;
-            break;
-        }
-        if (lastException != null) {
-            throw lastException;
-        }
-
-        BackedInputStream inputStream = new BlobStoreBackedInputStreamImpl(blobStore, bucket, key,
-                versionCheck, index.getDataObjectLength(), readBufferSize, offloaderStats, managedLedgerName);
-
-        return new BlobStoreBackedReadHandleImpl(ledgerId, index, inputStream, executor, entryOffsetsCache);
-    }
-
-    // for testing
-    @VisibleForTesting
-    State getState() {
-        return this.state;
-    }
-
-    @Override
-    public long lastAccessTimestamp() {
-        return lastAccessTimestamp;
-    }
-
-    @Override
-    public int getPendingRead() {
-        return PENDING_READ_UPDATER.get(this);
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import com.google.common.annotations.VisibleForTesting;
+import io.netty.buffer.ByteBuf;
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;
+import java.util.concurrent.atomic.AtomicReference;
+import org.apache.bookkeeper.client.BKException;
+import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;
+import org.apache.bookkeeper.client.api.LedgerEntries;
+import org.apache.bookkeeper.client.api.LedgerEntry;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.client.api.ReadHandle;
+import org.apache.bookkeeper.client.impl.LedgerEntriesImpl;
+import org.apache.bookkeeper.client.impl.LedgerEntryImpl;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.bookkeeper.mledger.ManagedLedgerException;
+import org.apache.bookkeeper.mledger.OffloadedLedgerHandle;
+import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockBuilder;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;
+import org.apache.bookkeeper.mledger.offload.jcloud.impl.DataBlockUtils.VersionCheck;
+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
+import org.apache.pulsar.common.naming.TopicName;
+import org.jclouds.blobstore.BlobStore;
+import org.jclouds.blobstore.KeyNotFoundException;
+import org.jclouds.blobstore.domain.Blob;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class BlobStoreBackedReadHandleImpl implements ReadHandle, OffloadedLedgerHandle {
+    private static final Logger log = LoggerFactory.getLogger(BlobStoreBackedReadHandleImpl.class);
+
+    protected static final AtomicIntegerFieldUpdater<BlobStoreBackedReadHandleImpl> PENDING_READ_UPDATER =
+            AtomicIntegerFieldUpdater.newUpdater(BlobStoreBackedReadHandleImpl.class, "pendingRead");
+
+    private final long ledgerId;
+    private final OffloadIndexBlock index;
+    private final BackedInputStream inputStream;
+    private final DataInputStream dataStream;
+    private final ExecutorService executor;
+    private final OffsetsCache entryOffsetsCache;
+    private final AtomicReference<CompletableFuture<Void>> closeFuture = new AtomicReference<>();
+
+    enum State {
+        Opened,
+        Closed
+    }
+
+    private volatile State state = null;
+
+    private volatile int pendingRead;
+
+    private volatile long lastAccessTimestamp = System.currentTimeMillis();
+
+    @VisibleForTesting
+    BlobStoreBackedReadHandleImpl(long ledgerId, OffloadIndexBlock index,
+                                          BackedInputStream inputStream, ExecutorService executor,
+                                          OffsetsCache entryOffsetsCache) {
+        this.ledgerId = ledgerId;
+        this.index = index;
+        this.inputStream = inputStream;
+        this.dataStream = new DataInputStream(inputStream);
+        this.executor = executor;
+        this.entryOffsetsCache = entryOffsetsCache;
+        state = State.Opened;
+    }
+
+    @Override
+    public long getId() {
+        return ledgerId;
+    }
+
+    @Override
+    public LedgerMetadata getLedgerMetadata() {
+        return index.getLedgerMetadata();
+    }
+
+    @Override
+    public CompletableFuture<Void> closeAsync() {
+        if (closeFuture.get() != null || !closeFuture.compareAndSet(null, new CompletableFuture<>())) {
+            return closeFuture.get();
+        }
+
+        CompletableFuture<Void> promise = closeFuture.get();
+        executor.execute(() -> {
+            try {
+                index.close();
+                inputStream.close();
+                state = State.Closed;
+                promise.complete(null);
+            } catch (IOException t) {
+                promise.completeExceptionally(t);
+            }
+        });
+        return promise;
+    }
+
+    private class ReadTask implements Runnable {
+        private final long firstEntry;
+        private final long lastEntry;
+        private final CompletableFuture<LedgerEntries> promise;
+        private int seekedAndTryTimes = 0;
+
+        public ReadTask(long firstEntry, long lastEntry, CompletableFuture<LedgerEntries> promise) {
+            this.firstEntry = firstEntry;
+            this.lastEntry = lastEntry;
+            this.promise = promise;
+        }
+
+        @Override
+        public void run() {
+            if (state == State.Closed) {
+                log.warn("Reading a closed read handler. Ledger ID: {}, Read range: {}-{}",
+                        ledgerId, firstEntry, lastEntry);
+                promise.completeExceptionally(new ManagedLedgerException.OffloadReadHandleClosedException());
+                return;
+            }
+
+            List<LedgerEntry> entryCollector = new ArrayList<LedgerEntry>();
+            try {
+                if (firstEntry > lastEntry
+                        || firstEntry < 0
+                        || lastEntry > getLastAddConfirmed()) {
+                    promise.completeExceptionally(new BKException.BKIncorrectParameterException());
+                    return;
+                }
+                long entriesToRead = (lastEntry - firstEntry) + 1;
+                long expectedEntryId = firstEntry;
+                seekToEntryOffset(firstEntry);
+                seekedAndTryTimes++;
+
+                while (entriesToRead > 0) {
+                    long currentPosition = inputStream.getCurrentPosition();
+                    int length = dataStream.readInt();
+                    if (length < 0) { // hit padding or new block
+                        seekToEntryOffset(expectedEntryId);
+                        continue;
+                    }
+                    long entryId = dataStream.readLong();
+                    if (entryId == expectedEntryId) {
+                        entryOffsetsCache.put(ledgerId, entryId, currentPosition);
+                        ByteBuf buf = PulsarByteBufAllocator.DEFAULT.buffer(length, length);
+                        entryCollector.add(LedgerEntryImpl.create(ledgerId, entryId, length, buf));
+                        int toWrite = length;
+                        while (toWrite > 0) {
+                            toWrite -= buf.writeBytes(dataStream, toWrite);
+                        }
+                        entriesToRead--;
+                        expectedEntryId++;
+                    } else {
+                        handleUnexpectedEntryId(expectedEntryId, entryId);
+                    }
+                }
+                promise.complete(LedgerEntriesImpl.create(entryCollector));
+            } catch (Throwable t) {
+                log.error("Failed to read entries {} - {} from the offloader in ledger {}, current position of input"
+                        + " stream is {}", firstEntry, lastEntry, ledgerId, inputStream.getCurrentPosition(), t);
+                if (t instanceof KeyNotFoundException) {
+                    promise.completeExceptionally(new BKException.BKNoSuchLedgerExistsException());
+                } else {
+                    promise.completeExceptionally(t);
+                }
+                entryCollector.forEach(LedgerEntry::close);
+            }
+        }
+
+        // in the normal case, the entry id should increment in order. But if there has random access in
+        // the read method, we should allow to seek to the right position and the entry id should
+        // never over to the last entry again.
+        private void handleUnexpectedEntryId(long expectedId, long actEntryId) throws Exception {
+            LedgerMetadata ledgerMetadata = getLedgerMetadata();
+            OffloadIndexEntry offsetOfExpectedId = index.getIndexEntryForEntry(expectedId);
+            OffloadIndexEntry offsetOfActId = actEntryId <= getLedgerMetadata().getLastEntryId() && actEntryId >= 0
+                    ? index.getIndexEntryForEntry(actEntryId) : null;
+            String logLine = String.format("Failed to read [ %s ~ %s ] of the ledger %s."
+                    + " Because got a incorrect entry id %s, the offset is %s."
+                    + " The expected entry id is %s, the offset is %s."
+                    + " Have seeked and retry read times: %s. LAC is %s.",
+                    firstEntry, lastEntry, ledgerId,
+                    actEntryId, offsetOfActId == null ? "null because it does not exist"
+                            : String.valueOf(offsetOfActId),
+                    expectedId, String.valueOf(offsetOfExpectedId),
+                    seekedAndTryTimes, ledgerMetadata != null ? ledgerMetadata.getLastEntryId() : "unknown");
+            // If it still fails after tried entries count times, throw the exception.
+            long maxTryTimes = Math.max(3, (lastEntry - firstEntry + 1) >> 2);
+            if (seekedAndTryTimes > maxTryTimes) {
+                log.error(logLine);
+                throw new BKException.BKUnexpectedConditionException();
+            } else {
+                log.warn(logLine);
+            }
+            seekToEntryOffset(expectedId);
+            seekedAndTryTimes++;
+        }
+
+        private void skipPreviousEntry(long startEntryId, long expectedEntryId) throws IOException, BKException {
+            long nextExpectedEntryId = startEntryId;
+            while (nextExpectedEntryId < expectedEntryId) {
+                long offset = inputStream.getCurrentPosition();
+                int len = dataStream.readInt();
+                if (len < 0) {
+                    LedgerMetadata ledgerMetadata = getLedgerMetadata();
+                    OffloadIndexEntry offsetOfExpectedId = index.getIndexEntryForEntry(expectedEntryId);
+                    log.error("Failed to read [ {} ~ {} ] of the ledger {}."
+                        + " Because failed to skip a previous entry {}, len: {}, got a negative len."
+                        + " The expected entry id is {}, the offset is {}."
+                        + " Have seeked and retry read times: {}. LAC is {}.",
+                        firstEntry, lastEntry, ledgerId,
+                        nextExpectedEntryId, len,
+                        expectedEntryId, String.valueOf(offsetOfExpectedId),
+                        seekedAndTryTimes, ledgerMetadata != null ? ledgerMetadata.getLastEntryId() : "unknown");
+                    throw new BKException.BKUnexpectedConditionException();
+                }
+                long entryId = dataStream.readLong();
+                if (entryId == nextExpectedEntryId) {
+                    long skipped = inputStream.skip(len);
+                    if (skipped != len) {
+                        LedgerMetadata ledgerMetadata = getLedgerMetadata();
+                        OffloadIndexEntry offsetOfExpectedId = index.getIndexEntryForEntry(expectedEntryId);
+                        log.error("Failed to read [ {} ~ {} ] of the ledger {}."
+                            + " Because failed to skip a previous entry {}, offset: {}, len: {}, there is no more data."
+                            + " The expected entry id is {}, the offset is {}."
+                            + " Have seeked and retry read times: {}. LAC is {}.",
+                            firstEntry, lastEntry, ledgerId,
+                            entryId, offset, len,
+                            expectedEntryId, String.valueOf(offsetOfExpectedId),
+                            seekedAndTryTimes, ledgerMetadata != null ? ledgerMetadata.getLastEntryId() : "unknown");
+                        throw new BKException.BKUnexpectedConditionException();
+                    }
+                    nextExpectedEntryId++;
+                } else {
+                    LedgerMetadata ledgerMetadata = getLedgerMetadata();
+                    OffloadIndexEntry offsetOfExpectedId = index.getIndexEntryForEntry(expectedEntryId);
+                    log.error("Failed to read [ {} ~ {} ] of the ledger {}."
+                        + " Because got a incorrect entry id {},."
+                        + " The expected entry id is {}, the offset is {}."
+                        + " Have seeked and retry read times: {}. LAC is {}.",
+                        firstEntry, lastEntry, ledgerId,
+                        entryId, expectedEntryId, String.valueOf(offsetOfExpectedId),
+                        seekedAndTryTimes, ledgerMetadata != null ? ledgerMetadata.getLastEntryId() : "unknown");
+                    throw new BKException.BKUnexpectedConditionException();
+                }
+            }
+        }
+
+        private void seekToEntryOffset(long expectedEntryId) throws IOException, BKException {
+            // 1. Try to find the precise index.
+            // 1-1. Precise cached indexes.
+            Long cachedPreciseIndex = entryOffsetsCache.getIfPresent(ledgerId, expectedEntryId);
+            if (cachedPreciseIndex != null) {
+                inputStream.seek(cachedPreciseIndex);
+                return;
+            }
+            // 1-2. Precise persistent indexes.
+            OffloadIndexEntry indexOfNearestEntry = index.getIndexEntryForEntry(expectedEntryId);
+            if (indexOfNearestEntry.getEntryId() == expectedEntryId) {
+                inputStream.seek(indexOfNearestEntry.getDataOffset());
+                return;
+            }
+            // 2. Try to use the previous index. Since the entry-0 must have a precise index, we can skip to check
+            //    whether "expectedEntryId" is larger than 0;
+            Long cachedPreviousKnownOffset = entryOffsetsCache.getIfPresent(ledgerId, expectedEntryId - 1);
+            if (cachedPreviousKnownOffset != null) {
+                inputStream.seek(cachedPreviousKnownOffset);
+                skipPreviousEntry(expectedEntryId - 1, expectedEntryId);
+                return;
+            }
+            // 3. Use the persistent index of the nearest entry that is smaller than "expectedEntryId".
+            //    Because it is a sparse index, some entries need to be skipped.
+            if (indexOfNearestEntry.getEntryId() < expectedEntryId) {
+                inputStream.seek(indexOfNearestEntry.getDataOffset());
+                skipPreviousEntry(indexOfNearestEntry.getEntryId(), expectedEntryId);
+            } else {
+                LedgerMetadata ledgerMetadata = getLedgerMetadata();
+                log.error("Failed to read [ {} ~ {} ] of the ledger {}."
+                    + " Because got a incorrect index {} of the entry {}, which is greater than expected."
+                    + " Have seeked and retry read times: {}. LAC is {}.",
+                    firstEntry, lastEntry, ledgerId,
+                    String.valueOf(indexOfNearestEntry), expectedEntryId,
+                    seekedAndTryTimes, ledgerMetadata != null ? ledgerMetadata.getLastEntryId() : "unknown");
+                throw new BKException.BKUnexpectedConditionException();
+            }
+        }
+    }
+
+    @Override
+    public CompletableFuture<LedgerEntries> readAsync(long firstEntry, long lastEntry) {
+        if (log.isDebugEnabled()) {
+            log.debug("Ledger {}: reading {} - {} ({} entries}",
+                    getId(), firstEntry, lastEntry, (1 + lastEntry - firstEntry));
+        }
+        CompletableFuture<LedgerEntries> promise = new CompletableFuture<>();
+
+        // Ledger handles will be only marked idle when "pendingRead" is "0", it is not needed to update
+        // "lastAccessTimestamp" if "pendingRead" is larger than "0".
+        // Rather than update "lastAccessTimestamp" when starts a reading, updating it when a reading task is finished
+        // is better.
+        PENDING_READ_UPDATER.incrementAndGet(this);
+        promise.whenComplete((__, ex) -> {
+            lastAccessTimestamp = System.currentTimeMillis();
+            PENDING_READ_UPDATER.decrementAndGet(BlobStoreBackedReadHandleImpl.this);
+        });
+        executor.execute(new ReadTask(firstEntry, lastEntry, promise));
+        return promise;
+    }
+
+    private void seekToEntry(long nextExpectedId) throws IOException {
+        Long knownOffset = entryOffsetsCache.getIfPresent(ledgerId, nextExpectedId);
+        if (knownOffset != null) {
+            inputStream.seek(knownOffset);
+        } else {
+            // we don't know the exact position
+            // we seek to somewhere before the entry
+            long dataOffset = index.getIndexEntryForEntry(nextExpectedId).getDataOffset();
+            inputStream.seek(dataOffset);
+        }
+    }
+
+    private void seekToEntry(OffloadIndexEntry offloadIndexEntry) throws IOException {
+        long dataOffset = offloadIndexEntry.getDataOffset();
+        inputStream.seek(dataOffset);
+    }
+
+    @Override
+    public CompletableFuture<LedgerEntries> readUnconfirmedAsync(long firstEntry, long lastEntry) {
+        return readAsync(firstEntry, lastEntry);
+    }
+
+    @Override
+    public CompletableFuture<Long> readLastAddConfirmedAsync() {
+        return CompletableFuture.completedFuture(getLastAddConfirmed());
+    }
+
+    @Override
+    public CompletableFuture<Long> tryReadLastAddConfirmedAsync() {
+        return CompletableFuture.completedFuture(getLastAddConfirmed());
+    }
+
+    @Override
+    public long getLastAddConfirmed() {
+        return getLedgerMetadata().getLastEntryId();
+    }
+
+    @Override
+    public long getLength() {
+        return getLedgerMetadata().getLength();
+    }
+
+    @Override
+    public boolean isClosed() {
+        return getLedgerMetadata().isClosed();
+    }
+
+    @Override
+    public CompletableFuture<LastConfirmedAndEntry> readLastAddConfirmedAndEntryAsync(long entryId,
+                                                                                      long timeOutInMillis,
+                                                                                      boolean parallel) {
+        CompletableFuture<LastConfirmedAndEntry> promise = new CompletableFuture<>();
+        promise.completeExceptionally(new UnsupportedOperationException());
+        return promise;
+    }
+
+    public static ReadHandle open(ScheduledExecutorService executor,
+                                  BlobStore blobStore, String bucket, String key, String indexKey,
+                                  VersionCheck versionCheck,
+                                  long ledgerId, int readBufferSize,
+                                  LedgerOffloaderStats offloaderStats, String managedLedgerName,
+                                  OffsetsCache entryOffsetsCache)
+            throws IOException, BKException.BKNoSuchLedgerExistsException {
+        int retryCount = 3;
+        OffloadIndexBlock index = null;
+        IOException lastException = null;
+        String topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
+        // The following retry is used to avoid to some network issue cause read index file failure.
+        // If it can not recovery in the retry, we will throw the exception and the dispatcher will schedule to
+        // next read.
+        // If we use a backoff to control the retry, it will introduce a concurrent operation.
+        // We don't want to make it complicated, because in the most of case it shouldn't in the retry loop.
+        while (retryCount-- > 0) {
+            long readIndexStartTime = System.nanoTime();
+            Blob blob = blobStore.getBlob(bucket, indexKey);
+            if (blob == null) {
+                log.error("{} not found in container {}", indexKey, bucket);
+                throw new BKException.BKNoSuchLedgerExistsException();
+            }
+            offloaderStats.recordReadOffloadIndexLatency(topicName,
+                    System.nanoTime() - readIndexStartTime, TimeUnit.NANOSECONDS);
+            versionCheck.check(indexKey, blob);
+            OffloadIndexBlockBuilder indexBuilder = OffloadIndexBlockBuilder.create();
+            try (InputStream payLoadStream = blob.getPayload().openStream()) {
+                index = (OffloadIndexBlock) indexBuilder.fromStream(payLoadStream);
+            } catch (IOException e) {
+                // retry to avoid the network issue caused read failure
+                log.warn("Failed to get index block from the offoaded index file {}, still have {} times to retry",
+                    indexKey, retryCount, e);
+                lastException = e;
+                continue;
+            }
+            lastException = null;
+            break;
+        }
+        if (lastException != null) {
+            throw lastException;
+        }
+
+        BackedInputStream inputStream = new BlobStoreBackedInputStreamImpl(blobStore, bucket, key,
+                versionCheck, index.getDataObjectLength(), readBufferSize, offloaderStats, managedLedgerName);
+
+        return new BlobStoreBackedReadHandleImpl(ledgerId, index, inputStream, executor, entryOffsetsCache);
+    }
+
+    // for testing
+    @VisibleForTesting
+    State getState() {
+        return this.state;
+    }
+
+    @Override
+    public long lastAccessTimestamp() {
+        return lastAccessTimestamp;
+    }
+
+    @Override
+    public int getPendingRead() {
+        return PENDING_READ_UPDATER.get(this);
+    }
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedReadHandleImplV2.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedReadHandleImplV2.java
index 3fffd18a5d..b00458ca5f 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedReadHandleImplV2.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedReadHandleImplV2.java
@@ -1,342 +1,342 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static com.google.common.base.Preconditions.checkArgument;
-import io.netty.buffer.ByteBuf;
-import java.io.DataInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.ArrayList;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.ScheduledExecutorService;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicReference;
-import lombok.val;
-import org.apache.bookkeeper.client.BKException;
-import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;
-import org.apache.bookkeeper.client.api.LedgerEntries;
-import org.apache.bookkeeper.client.api.LedgerEntry;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.client.api.ReadHandle;
-import org.apache.bookkeeper.client.impl.LedgerEntriesImpl;
-import org.apache.bookkeeper.client.impl.LedgerEntryImpl;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.bookkeeper.mledger.ManagedLedgerException;
-import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2Builder;
-import org.apache.bookkeeper.mledger.offload.jcloud.impl.DataBlockUtils.VersionCheck;
-import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
-import org.apache.pulsar.common.naming.TopicName;
-import org.jclouds.blobstore.BlobStore;
-import org.jclouds.blobstore.KeyNotFoundException;
-import org.jclouds.blobstore.domain.Blob;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class BlobStoreBackedReadHandleImplV2 implements ReadHandle {
-    private static final Logger log = LoggerFactory.getLogger(BlobStoreBackedReadHandleImplV2.class);
-
-    private final long ledgerId;
-    private final List<OffloadIndexBlockV2> indices;
-    private final List<BackedInputStream> inputStreams;
-    private final List<DataInputStream> dataStreams;
-    private final ExecutorService executor;
-    private volatile State state = null;
-    private final AtomicReference<CompletableFuture<Void>> closeFuture = new AtomicReference<>();
-
-    enum State {
-        Opened,
-        Closed
-    }
-
-    static class GroupedReader {
-        @Override
-        public String toString() {
-            return "GroupedReader{"
-                    + "ledgerId=" + ledgerId
-                    + ", firstEntry=" + firstEntry
-                    + ", lastEntry=" + lastEntry
-                    + '}';
-        }
-
-        public final long ledgerId;
-        public final long firstEntry;
-        public final long lastEntry;
-        OffloadIndexBlockV2 index;
-        BackedInputStream inputStream;
-        DataInputStream dataStream;
-
-        public GroupedReader(long ledgerId, long firstEntry, long lastEntry,
-                             OffloadIndexBlockV2 index,
-                             BackedInputStream inputStream, DataInputStream dataStream) {
-            this.ledgerId = ledgerId;
-            this.firstEntry = firstEntry;
-            this.lastEntry = lastEntry;
-            this.index = index;
-            this.inputStream = inputStream;
-            this.dataStream = dataStream;
-        }
-    }
-
-    private BlobStoreBackedReadHandleImplV2(long ledgerId, List<OffloadIndexBlockV2> indices,
-                                            List<BackedInputStream> inputStreams,
-                                            ExecutorService executor) {
-        this.ledgerId = ledgerId;
-        this.indices = indices;
-        this.inputStreams = inputStreams;
-        this.dataStreams = new LinkedList<>();
-        for (BackedInputStream inputStream : inputStreams) {
-            dataStreams.add(new DataInputStream(inputStream));
-        }
-        this.executor = executor;
-        this.state = State.Opened;
-    }
-
-    @Override
-    public long getId() {
-        return ledgerId;
-    }
-
-    @Override
-    public LedgerMetadata getLedgerMetadata() {
-        //get the most complete one
-        return indices.get(indices.size() - 1).getLedgerMetadata(ledgerId);
-    }
-
-    @Override
-    public CompletableFuture<Void> closeAsync() {
-        if (closeFuture.get() != null || !closeFuture.compareAndSet(null, new CompletableFuture<>())) {
-            return closeFuture.get();
-        }
-
-        CompletableFuture<Void> promise = closeFuture.get();
-        executor.execute(() -> {
-            try {
-                for (OffloadIndexBlockV2 indexBlock : indices) {
-                    indexBlock.close();
-                }
-                for (DataInputStream dataStream : dataStreams) {
-                    dataStream.close();
-                }
-                state = State.Closed;
-                promise.complete(null);
-            } catch (IOException t) {
-                promise.completeExceptionally(t);
-            }
-        });
-        return promise;
-    }
-
-    @Override
-    public CompletableFuture<LedgerEntries> readAsync(long firstEntry, long lastEntry) {
-        if (log.isDebugEnabled()) {
-            log.debug("Ledger {}: reading {} - {}", getId(), firstEntry, lastEntry);
-        }
-        CompletableFuture<LedgerEntries> promise = new CompletableFuture<>();
-        executor.execute(() -> {
-            if (state == State.Closed) {
-                log.warn("Reading a closed read handler. Ledger ID: {}, Read range: {}-{}",
-                        ledgerId, firstEntry, lastEntry);
-                promise.completeExceptionally(new ManagedLedgerException.OffloadReadHandleClosedException());
-                return;
-            }
-
-            if (firstEntry > lastEntry
-                    || firstEntry < 0
-                    || lastEntry > getLastAddConfirmed()) {
-                promise.completeExceptionally(new BKException.BKIncorrectParameterException());
-                return;
-            }
-            List<LedgerEntry> entries = new ArrayList<LedgerEntry>();
-            List<GroupedReader> groupedReaders = null;
-            try {
-                groupedReaders = getGroupedReader(firstEntry, lastEntry);
-            } catch (Exception e) {
-                promise.completeExceptionally(e);
-                return;
-            }
-
-            for (GroupedReader groupedReader : groupedReaders) {
-                long entriesToRead = (groupedReader.lastEntry - groupedReader.firstEntry) + 1;
-                long nextExpectedId = groupedReader.firstEntry;
-                try {
-                    while (entriesToRead > 0) {
-                        int length = groupedReader.dataStream.readInt();
-                        if (length < 0) { // hit padding or new block
-                            groupedReader.inputStream
-                                    .seek(groupedReader.index
-                                            .getIndexEntryForEntry(groupedReader.ledgerId, nextExpectedId)
-                                            .getDataOffset());
-                            continue;
-                        }
-                        long entryId = groupedReader.dataStream.readLong();
-
-                        if (entryId == nextExpectedId) {
-                            ByteBuf buf = PulsarByteBufAllocator.DEFAULT.buffer(length, length);
-                            entries.add(LedgerEntryImpl.create(ledgerId, entryId, length, buf));
-                            int toWrite = length;
-                            while (toWrite > 0) {
-                                toWrite -= buf.writeBytes(groupedReader.dataStream, toWrite);
-                            }
-                            entriesToRead--;
-                            nextExpectedId++;
-                        } else if (entryId > nextExpectedId) {
-                            groupedReader.inputStream
-                                    .seek(groupedReader.index
-                                            .getIndexEntryForEntry(groupedReader.ledgerId, nextExpectedId)
-                                            .getDataOffset());
-                            continue;
-                        } else if (entryId < nextExpectedId
-                                && !groupedReader.index.getIndexEntryForEntry(groupedReader.ledgerId, nextExpectedId)
-                                .equals(
-                                        groupedReader.index.getIndexEntryForEntry(groupedReader.ledgerId, entryId))) {
-                            groupedReader.inputStream
-                                    .seek(groupedReader.index
-                                            .getIndexEntryForEntry(groupedReader.ledgerId, nextExpectedId)
-                                            .getDataOffset());
-                            continue;
-                        } else if (entryId > groupedReader.lastEntry) {
-                            log.info("Expected to read {}, but read {}, which is greater than last entry {}",
-                                    nextExpectedId, entryId, groupedReader.lastEntry);
-                            throw new BKException.BKUnexpectedConditionException();
-                        } else {
-                            val skipped = groupedReader.inputStream.skip(length);
-                        }
-                    }
-                } catch (Throwable t) {
-                    if (t instanceof KeyNotFoundException) {
-                        promise.completeExceptionally(new BKException.BKNoSuchLedgerExistsException());
-                    } else {
-                        promise.completeExceptionally(t);
-                    }
-                    entries.forEach(LedgerEntry::close);
-                }
-
-            }
-            promise.complete(LedgerEntriesImpl.create(entries));
-        });
-        return promise;
-    }
-
-    private List<GroupedReader> getGroupedReader(long firstEntry, long lastEntry) throws Exception {
-        List<GroupedReader> groupedReaders = new LinkedList<>();
-        for (int i = indices.size() - 1; i >= 0 && firstEntry <= lastEntry; i--) {
-            final OffloadIndexBlockV2 index = indices.get(i);
-            final long startEntryId = index.getStartEntryId(ledgerId);
-            if (startEntryId > lastEntry) {
-                log.debug("entries are in earlier indices, skip this segment ledger id: {}, begin entry id: {}",
-                        ledgerId, startEntryId);
-            } else {
-                groupedReaders.add(new GroupedReader(ledgerId, startEntryId, lastEntry, index, inputStreams.get(i),
-                        dataStreams.get(i)));
-                lastEntry = startEntryId - 1;
-            }
-        }
-
-        checkArgument(firstEntry > lastEntry);
-        for (int i = 0; i < groupedReaders.size() - 1; i++) {
-            final GroupedReader readerI = groupedReaders.get(i);
-            final GroupedReader readerII = groupedReaders.get(i + 1);
-            checkArgument(readerI.ledgerId == readerII.ledgerId);
-            checkArgument(readerI.firstEntry >= readerII.lastEntry);
-        }
-        return groupedReaders;
-    }
-
-    @Override
-    public CompletableFuture<LedgerEntries> readUnconfirmedAsync(long firstEntry, long lastEntry) {
-        return readAsync(firstEntry, lastEntry);
-    }
-
-    @Override
-    public CompletableFuture<Long> readLastAddConfirmedAsync() {
-        return CompletableFuture.completedFuture(getLastAddConfirmed());
-    }
-
-    @Override
-    public CompletableFuture<Long> tryReadLastAddConfirmedAsync() {
-        return CompletableFuture.completedFuture(getLastAddConfirmed());
-    }
-
-    @Override
-    public long getLastAddConfirmed() {
-        return getLedgerMetadata().getLastEntryId();
-    }
-
-    @Override
-    public long getLength() {
-        return getLedgerMetadata().getLength();
-    }
-
-    @Override
-    public boolean isClosed() {
-        return getLedgerMetadata().isClosed();
-    }
-
-    @Override
-    public CompletableFuture<LastConfirmedAndEntry> readLastAddConfirmedAndEntryAsync(long entryId,
-                                                                                      long timeOutInMillis,
-                                                                                      boolean parallel) {
-        CompletableFuture<LastConfirmedAndEntry> promise = new CompletableFuture<>();
-        promise.completeExceptionally(new UnsupportedOperationException());
-        return promise;
-    }
-
-    public static ReadHandle open(ScheduledExecutorService executor,
-                                  BlobStore blobStore, String bucket, List<String> keys, List<String> indexKeys,
-                                  VersionCheck versionCheck,
-                                  long ledgerId, int readBufferSize, LedgerOffloaderStats offloaderStats,
-                                  String managedLedgerName)
-            throws IOException, BKException.BKNoSuchLedgerExistsException {
-        List<BackedInputStream> inputStreams = new LinkedList<>();
-        List<OffloadIndexBlockV2> indice = new LinkedList<>();
-        String topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
-        for (int i = 0; i < indexKeys.size(); i++) {
-            String indexKey = indexKeys.get(i);
-            String key = keys.get(i);
-            log.debug("open bucket: {} index key: {}", bucket, indexKey);
-            long startTime = System.nanoTime();
-            Blob blob = blobStore.getBlob(bucket, indexKey);
-            if (blob == null) {
-                log.error("{} not found in container {}", indexKey, bucket);
-                throw new BKException.BKNoSuchLedgerExistsException();
-            }
-            offloaderStats.recordReadOffloadIndexLatency(topicName,
-                    System.nanoTime() - startTime, TimeUnit.NANOSECONDS);
-            log.debug("indexKey blob: {} {}", indexKey, blob);
-            versionCheck.check(indexKey, blob);
-            OffloadIndexBlockV2Builder indexBuilder = OffloadIndexBlockV2Builder.create();
-            OffloadIndexBlockV2 index;
-            try (InputStream payloadStream = blob.getPayload().openStream()) {
-                index = indexBuilder.fromStream(payloadStream);
-            }
-
-            BackedInputStream inputStream = new BlobStoreBackedInputStreamImpl(blobStore, bucket, key,
-                    versionCheck, index.getDataObjectLength(), readBufferSize, offloaderStats, managedLedgerName);
-            inputStreams.add(inputStream);
-            indice.add(index);
-        }
-        return new BlobStoreBackedReadHandleImplV2(ledgerId, indice, inputStreams, executor);
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import io.netty.buffer.ByteBuf;
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.ArrayList;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicReference;
+import lombok.val;
+import org.apache.bookkeeper.client.BKException;
+import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;
+import org.apache.bookkeeper.client.api.LedgerEntries;
+import org.apache.bookkeeper.client.api.LedgerEntry;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.client.api.ReadHandle;
+import org.apache.bookkeeper.client.impl.LedgerEntriesImpl;
+import org.apache.bookkeeper.client.impl.LedgerEntryImpl;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.bookkeeper.mledger.ManagedLedgerException;
+import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2Builder;
+import org.apache.bookkeeper.mledger.offload.jcloud.impl.DataBlockUtils.VersionCheck;
+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
+import org.apache.pulsar.common.naming.TopicName;
+import org.jclouds.blobstore.BlobStore;
+import org.jclouds.blobstore.KeyNotFoundException;
+import org.jclouds.blobstore.domain.Blob;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class BlobStoreBackedReadHandleImplV2 implements ReadHandle {
+    private static final Logger log = LoggerFactory.getLogger(BlobStoreBackedReadHandleImplV2.class);
+
+    private final long ledgerId;
+    private final List<OffloadIndexBlockV2> indices;
+    private final List<BackedInputStream> inputStreams;
+    private final List<DataInputStream> dataStreams;
+    private final ExecutorService executor;
+    private volatile State state = null;
+    private final AtomicReference<CompletableFuture<Void>> closeFuture = new AtomicReference<>();
+
+    enum State {
+        Opened,
+        Closed
+    }
+
+    static class GroupedReader {
+        @Override
+        public String toString() {
+            return "GroupedReader{"
+                    + "ledgerId=" + ledgerId
+                    + ", firstEntry=" + firstEntry
+                    + ", lastEntry=" + lastEntry
+                    + '}';
+        }
+
+        public final long ledgerId;
+        public final long firstEntry;
+        public final long lastEntry;
+        OffloadIndexBlockV2 index;
+        BackedInputStream inputStream;
+        DataInputStream dataStream;
+
+        public GroupedReader(long ledgerId, long firstEntry, long lastEntry,
+                             OffloadIndexBlockV2 index,
+                             BackedInputStream inputStream, DataInputStream dataStream) {
+            this.ledgerId = ledgerId;
+            this.firstEntry = firstEntry;
+            this.lastEntry = lastEntry;
+            this.index = index;
+            this.inputStream = inputStream;
+            this.dataStream = dataStream;
+        }
+    }
+
+    private BlobStoreBackedReadHandleImplV2(long ledgerId, List<OffloadIndexBlockV2> indices,
+                                            List<BackedInputStream> inputStreams,
+                                            ExecutorService executor) {
+        this.ledgerId = ledgerId;
+        this.indices = indices;
+        this.inputStreams = inputStreams;
+        this.dataStreams = new LinkedList<>();
+        for (BackedInputStream inputStream : inputStreams) {
+            dataStreams.add(new DataInputStream(inputStream));
+        }
+        this.executor = executor;
+        this.state = State.Opened;
+    }
+
+    @Override
+    public long getId() {
+        return ledgerId;
+    }
+
+    @Override
+    public LedgerMetadata getLedgerMetadata() {
+        //get the most complete one
+        return indices.get(indices.size() - 1).getLedgerMetadata(ledgerId);
+    }
+
+    @Override
+    public CompletableFuture<Void> closeAsync() {
+        if (closeFuture.get() != null || !closeFuture.compareAndSet(null, new CompletableFuture<>())) {
+            return closeFuture.get();
+        }
+
+        CompletableFuture<Void> promise = closeFuture.get();
+        executor.execute(() -> {
+            try {
+                for (OffloadIndexBlockV2 indexBlock : indices) {
+                    indexBlock.close();
+                }
+                for (DataInputStream dataStream : dataStreams) {
+                    dataStream.close();
+                }
+                state = State.Closed;
+                promise.complete(null);
+            } catch (IOException t) {
+                promise.completeExceptionally(t);
+            }
+        });
+        return promise;
+    }
+
+    @Override
+    public CompletableFuture<LedgerEntries> readAsync(long firstEntry, long lastEntry) {
+        if (log.isDebugEnabled()) {
+            log.debug("Ledger {}: reading {} - {}", getId(), firstEntry, lastEntry);
+        }
+        CompletableFuture<LedgerEntries> promise = new CompletableFuture<>();
+        executor.execute(() -> {
+            if (state == State.Closed) {
+                log.warn("Reading a closed read handler. Ledger ID: {}, Read range: {}-{}",
+                        ledgerId, firstEntry, lastEntry);
+                promise.completeExceptionally(new ManagedLedgerException.OffloadReadHandleClosedException());
+                return;
+            }
+
+            if (firstEntry > lastEntry
+                    || firstEntry < 0
+                    || lastEntry > getLastAddConfirmed()) {
+                promise.completeExceptionally(new BKException.BKIncorrectParameterException());
+                return;
+            }
+            List<LedgerEntry> entries = new ArrayList<LedgerEntry>();
+            List<GroupedReader> groupedReaders = null;
+            try {
+                groupedReaders = getGroupedReader(firstEntry, lastEntry);
+            } catch (Exception e) {
+                promise.completeExceptionally(e);
+                return;
+            }
+
+            for (GroupedReader groupedReader : groupedReaders) {
+                long entriesToRead = (groupedReader.lastEntry - groupedReader.firstEntry) + 1;
+                long nextExpectedId = groupedReader.firstEntry;
+                try {
+                    while (entriesToRead > 0) {
+                        int length = groupedReader.dataStream.readInt();
+                        if (length < 0) { // hit padding or new block
+                            groupedReader.inputStream
+                                    .seek(groupedReader.index
+                                            .getIndexEntryForEntry(groupedReader.ledgerId, nextExpectedId)
+                                            .getDataOffset());
+                            continue;
+                        }
+                        long entryId = groupedReader.dataStream.readLong();
+
+                        if (entryId == nextExpectedId) {
+                            ByteBuf buf = PulsarByteBufAllocator.DEFAULT.buffer(length, length);
+                            entries.add(LedgerEntryImpl.create(ledgerId, entryId, length, buf));
+                            int toWrite = length;
+                            while (toWrite > 0) {
+                                toWrite -= buf.writeBytes(groupedReader.dataStream, toWrite);
+                            }
+                            entriesToRead--;
+                            nextExpectedId++;
+                        } else if (entryId > nextExpectedId) {
+                            groupedReader.inputStream
+                                    .seek(groupedReader.index
+                                            .getIndexEntryForEntry(groupedReader.ledgerId, nextExpectedId)
+                                            .getDataOffset());
+                            continue;
+                        } else if (entryId < nextExpectedId
+                                && !groupedReader.index.getIndexEntryForEntry(groupedReader.ledgerId, nextExpectedId)
+                                .equals(
+                                        groupedReader.index.getIndexEntryForEntry(groupedReader.ledgerId, entryId))) {
+                            groupedReader.inputStream
+                                    .seek(groupedReader.index
+                                            .getIndexEntryForEntry(groupedReader.ledgerId, nextExpectedId)
+                                            .getDataOffset());
+                            continue;
+                        } else if (entryId > groupedReader.lastEntry) {
+                            log.info("Expected to read {}, but read {}, which is greater than last entry {}",
+                                    nextExpectedId, entryId, groupedReader.lastEntry);
+                            throw new BKException.BKUnexpectedConditionException();
+                        } else {
+                            val skipped = groupedReader.inputStream.skip(length);
+                        }
+                    }
+                } catch (Throwable t) {
+                    if (t instanceof KeyNotFoundException) {
+                        promise.completeExceptionally(new BKException.BKNoSuchLedgerExistsException());
+                    } else {
+                        promise.completeExceptionally(t);
+                    }
+                    entries.forEach(LedgerEntry::close);
+                }
+
+            }
+            promise.complete(LedgerEntriesImpl.create(entries));
+        });
+        return promise;
+    }
+
+    private List<GroupedReader> getGroupedReader(long firstEntry, long lastEntry) throws Exception {
+        List<GroupedReader> groupedReaders = new LinkedList<>();
+        for (int i = indices.size() - 1; i >= 0 && firstEntry <= lastEntry; i--) {
+            final OffloadIndexBlockV2 index = indices.get(i);
+            final long startEntryId = index.getStartEntryId(ledgerId);
+            if (startEntryId > lastEntry) {
+                log.debug("entries are in earlier indices, skip this segment ledger id: {}, begin entry id: {}",
+                        ledgerId, startEntryId);
+            } else {
+                groupedReaders.add(new GroupedReader(ledgerId, startEntryId, lastEntry, index, inputStreams.get(i),
+                        dataStreams.get(i)));
+                lastEntry = startEntryId - 1;
+            }
+        }
+
+        checkArgument(firstEntry > lastEntry);
+        for (int i = 0; i < groupedReaders.size() - 1; i++) {
+            final GroupedReader readerI = groupedReaders.get(i);
+            final GroupedReader readerII = groupedReaders.get(i + 1);
+            checkArgument(readerI.ledgerId == readerII.ledgerId);
+            checkArgument(readerI.firstEntry >= readerII.lastEntry);
+        }
+        return groupedReaders;
+    }
+
+    @Override
+    public CompletableFuture<LedgerEntries> readUnconfirmedAsync(long firstEntry, long lastEntry) {
+        return readAsync(firstEntry, lastEntry);
+    }
+
+    @Override
+    public CompletableFuture<Long> readLastAddConfirmedAsync() {
+        return CompletableFuture.completedFuture(getLastAddConfirmed());
+    }
+
+    @Override
+    public CompletableFuture<Long> tryReadLastAddConfirmedAsync() {
+        return CompletableFuture.completedFuture(getLastAddConfirmed());
+    }
+
+    @Override
+    public long getLastAddConfirmed() {
+        return getLedgerMetadata().getLastEntryId();
+    }
+
+    @Override
+    public long getLength() {
+        return getLedgerMetadata().getLength();
+    }
+
+    @Override
+    public boolean isClosed() {
+        return getLedgerMetadata().isClosed();
+    }
+
+    @Override
+    public CompletableFuture<LastConfirmedAndEntry> readLastAddConfirmedAndEntryAsync(long entryId,
+                                                                                      long timeOutInMillis,
+                                                                                      boolean parallel) {
+        CompletableFuture<LastConfirmedAndEntry> promise = new CompletableFuture<>();
+        promise.completeExceptionally(new UnsupportedOperationException());
+        return promise;
+    }
+
+    public static ReadHandle open(ScheduledExecutorService executor,
+                                  BlobStore blobStore, String bucket, List<String> keys, List<String> indexKeys,
+                                  VersionCheck versionCheck,
+                                  long ledgerId, int readBufferSize, LedgerOffloaderStats offloaderStats,
+                                  String managedLedgerName)
+            throws IOException, BKException.BKNoSuchLedgerExistsException {
+        List<BackedInputStream> inputStreams = new LinkedList<>();
+        List<OffloadIndexBlockV2> indice = new LinkedList<>();
+        String topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
+        for (int i = 0; i < indexKeys.size(); i++) {
+            String indexKey = indexKeys.get(i);
+            String key = keys.get(i);
+            log.debug("open bucket: {} index key: {}", bucket, indexKey);
+            long startTime = System.nanoTime();
+            Blob blob = blobStore.getBlob(bucket, indexKey);
+            if (blob == null) {
+                log.error("{} not found in container {}", indexKey, bucket);
+                throw new BKException.BKNoSuchLedgerExistsException();
+            }
+            offloaderStats.recordReadOffloadIndexLatency(topicName,
+                    System.nanoTime() - startTime, TimeUnit.NANOSECONDS);
+            log.debug("indexKey blob: {} {}", indexKey, blob);
+            versionCheck.check(indexKey, blob);
+            OffloadIndexBlockV2Builder indexBuilder = OffloadIndexBlockV2Builder.create();
+            OffloadIndexBlockV2 index;
+            try (InputStream payloadStream = blob.getPayload().openStream()) {
+                index = indexBuilder.fromStream(payloadStream);
+            }
+
+            BackedInputStream inputStream = new BlobStoreBackedInputStreamImpl(blobStore, bucket, key,
+                    versionCheck, index.getDataObjectLength(), readBufferSize, offloaderStats, managedLedgerName);
+            inputStreams.add(inputStream);
+            indice.add(index);
+        }
+        return new BlobStoreBackedReadHandleImplV2(ledgerId, indice, inputStreams, executor);
+    }
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java
index 33bbc49ee2..f67cf26667 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java
@@ -1,757 +1,757 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import com.google.common.base.Strings;
-import com.google.common.collect.ImmutableList;
-import com.google.common.collect.Lists;
-import java.io.IOException;
-import java.net.URI;
-import java.time.Duration;
-import java.util.Collections;
-import java.util.Date;
-import java.util.HashMap;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-import java.util.UUID;
-import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentLinkedQueue;
-import java.util.concurrent.ConcurrentMap;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicLong;
-import lombok.NonNull;
-import lombok.extern.slf4j.Slf4j;
-import org.apache.bookkeeper.client.api.ReadHandle;
-import org.apache.bookkeeper.common.util.OrderedScheduler;
-import org.apache.bookkeeper.mledger.Entry;
-import org.apache.bookkeeper.mledger.LedgerOffloader;
-import org.apache.bookkeeper.mledger.LedgerOffloader.OffloadHandle.OfferEntryResult;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.bookkeeper.mledger.ManagedLedger;
-import org.apache.bookkeeper.mledger.ManagedLedgerException;
-import org.apache.bookkeeper.mledger.OffloadedLedgerMetadata;
-import org.apache.bookkeeper.mledger.OffloadedLedgerMetadataConsumer;
-import org.apache.bookkeeper.mledger.Position;
-import org.apache.bookkeeper.mledger.PositionFactory;
-import org.apache.bookkeeper.mledger.impl.EntryImpl;
-import org.apache.bookkeeper.mledger.impl.OffloadSegmentInfoImpl;
-import org.apache.bookkeeper.mledger.offload.jcloud.BlockAwareSegmentInputStream;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock.IndexInputStream;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockBuilder;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2Builder;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.BlobStoreLocation;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration;
-import org.apache.bookkeeper.mledger.proto.MLDataFormats;
-import org.apache.pulsar.common.naming.TopicName;
-import org.apache.pulsar.common.policies.data.OffloadPolicies;
-import org.apache.pulsar.common.policies.data.OffloadPoliciesImpl;
-import org.jclouds.blobstore.BlobStore;
-import org.jclouds.blobstore.domain.Blob;
-import org.jclouds.blobstore.domain.BlobBuilder;
-import org.jclouds.blobstore.domain.MultipartPart;
-import org.jclouds.blobstore.domain.MultipartUpload;
-import org.jclouds.blobstore.domain.PageSet;
-import org.jclouds.blobstore.domain.StorageMetadata;
-import org.jclouds.blobstore.domain.StorageType;
-import org.jclouds.blobstore.options.ListContainerOptions;
-import org.jclouds.blobstore.options.PutOptions;
-import org.jclouds.domain.Location;
-import org.jclouds.domain.LocationBuilder;
-import org.jclouds.domain.LocationScope;
-import org.jclouds.io.Payload;
-import org.jclouds.io.Payloads;
-import org.jclouds.io.payloads.InputStreamPayload;
-
-/**
- * Tiered Storage Offloader that is backed by a JCloud Blob Store.
- * <p>
- * The constructor takes an instance of TieredStorageConfiguration, which
- * contains all of the configuration data necessary to connect to a JCloud
- * Provider service.
- * </p>
- */
-@Slf4j
-public class BlobStoreManagedLedgerOffloader implements LedgerOffloader {
-
-    private static final String MANAGED_LEDGER_NAME = "ManagedLedgerName";
-
-    private final OrderedScheduler scheduler;
-    private final OrderedScheduler readExecutor;
-    private final TieredStorageConfiguration config;
-    private final OffloadPolicies policies;
-    private final Location writeLocation;
-
-    // metadata to be stored as part of the offloaded ledger metadata
-    private final Map<String, String> userMetadata;
-
-    private final ConcurrentMap<BlobStoreLocation, BlobStore> blobStores = new ConcurrentHashMap<>();
-    private OffloadSegmentInfoImpl segmentInfo;
-    private final AtomicLong bufferLength = new AtomicLong(0);
-    private final AtomicLong segmentLength = new AtomicLong(0);
-    private final long maxBufferLength;
-    private final OffsetsCache entryOffsetsCache;
-    private final ConcurrentLinkedQueue<Entry> offloadBuffer = new ConcurrentLinkedQueue<>();
-    private CompletableFuture<OffloadResult> offloadResult;
-    private volatile Position lastOfferedPosition = PositionFactory.LATEST;
-    private final Duration maxSegmentCloseTime;
-    private final long minSegmentCloseTimeMillis;
-    private final long segmentBeginTimeMillis;
-    private final long maxSegmentLength;
-    private final int streamingBlockSize;
-    private volatile ManagedLedger ml;
-    private OffloadIndexBlockV2Builder streamingIndexBuilder;
-    private final LedgerOffloaderStats offloaderStats;
-
-    public static BlobStoreManagedLedgerOffloader create(TieredStorageConfiguration config,
-                                                         Map<String, String> userMetadata,
-                                                         OrderedScheduler scheduler,
-                                                         OrderedScheduler readExecutor,
-                                                         LedgerOffloaderStats offloaderStats,
-                                                         OffsetsCache entryOffsetsCache)
-            throws IOException {
-
-        return new BlobStoreManagedLedgerOffloader(config, scheduler, readExecutor,
-                userMetadata, offloaderStats, entryOffsetsCache);
-    }
-
-    BlobStoreManagedLedgerOffloader(TieredStorageConfiguration config, OrderedScheduler scheduler,
-                                    OrderedScheduler readExecutor,
-                                    Map<String, String> userMetadata, LedgerOffloaderStats offloaderStats,
-                                    OffsetsCache entryOffsetsCache) {
-        this.scheduler = scheduler;
-        this.readExecutor = readExecutor;
-        this.userMetadata = userMetadata;
-        this.config = config;
-        Properties properties = new Properties();
-        properties.putAll(config.getConfigProperties());
-        this.policies = OffloadPoliciesImpl.create(properties);
-        this.streamingBlockSize = config.getMinBlockSizeInBytes();
-        this.maxSegmentCloseTime = Duration.ofSeconds(config.getMaxSegmentTimeInSecond());
-        this.maxSegmentLength = config.getMaxSegmentSizeInBytes();
-        this.minSegmentCloseTimeMillis = Duration.ofSeconds(config.getMinSegmentTimeInSecond()).toMillis();
-        //ensure buffer can have enough content to fill a block
-        this.maxBufferLength = Math.max(config.getWriteBufferSizeInBytes(), config.getMinBlockSizeInBytes());
-        this.entryOffsetsCache = entryOffsetsCache;
-        this.segmentBeginTimeMillis = System.currentTimeMillis();
-        if (!Strings.isNullOrEmpty(config.getRegion())) {
-            this.writeLocation = new LocationBuilder()
-                    .scope(LocationScope.REGION)
-                    .id(config.getRegion())
-                    .description(config.getRegion())
-                    .build();
-        } else {
-            this.writeLocation = null;
-        }
-
-        log.info("Constructor offload driver: {}, host: {}, container: {}, region: {} ",
-                config.getProvider().getDriver(), config.getServiceEndpoint(),
-                config.getBucket(), config.getRegion());
-
-        this.offloaderStats = offloaderStats;
-        log.info("The ledger offloader was created.");
-    }
-
-    private BlobStore getBlobStore(BlobStoreLocation blobStoreLocation) {
-        return blobStores.computeIfAbsent(blobStoreLocation, location -> {
-            log.info("Creating blob store for location {}", location);
-            return config.getBlobStore();
-        });
-    }
-
-    @Override
-    public String getOffloadDriverName() {
-        return config.getDriver();
-    }
-
-    @Override
-    public Map<String, String> getOffloadDriverMetadata() {
-        return config.getOffloadDriverMetadata();
-    }
-
-    /**
-     * Upload the DataBlocks associated with the given ReadHandle using MultiPartUpload,
-     * Creating indexBlocks for each corresponding DataBlock that is uploaded.
-     */
-    @Override
-    public CompletableFuture<Void> offload(ReadHandle readHandle,
-                                           UUID uuid,
-                                           Map<String, String> extraMetadata) {
-        final String managedLedgerName = extraMetadata.get(MANAGED_LEDGER_NAME);
-        final String topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
-        CompletableFuture<Void> promise = new CompletableFuture<>();
-        scheduler.chooseThread(readHandle.getId()).execute(() -> {
-            final BlobStore writeBlobStore = getBlobStore(config.getBlobStoreLocation());
-            log.info("offload {} uuid {} extraMetadata {} to {} {}", readHandle.getId(), uuid, extraMetadata,
-                config.getBlobStoreLocation(), writeBlobStore);
-            if (!readHandle.isClosed() || readHandle.getLastAddConfirmed() < 0) {
-                promise.completeExceptionally(
-                        new IllegalArgumentException("An empty or open ledger should never be offloaded"));
-                return;
-            }
-            if (readHandle.getLength() <= 0) {
-                log.warn("[{}] Ledger [{}] has zero length, but it contains {} entries."
-                    + " Attempting to offload ledger since it contains entries.", topicName, readHandle.getId(),
-                    readHandle.getLastAddConfirmed() + 1);
-            }
-            OffloadIndexBlockBuilder indexBuilder = OffloadIndexBlockBuilder.create()
-                .withLedgerMetadata(readHandle.getLedgerMetadata())
-                .withDataBlockHeaderLength(BlockAwareSegmentInputStreamImpl.getHeaderSize());
-            String dataBlockKey = DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid);
-            String indexBlockKey = DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid);
-            log.info("ledger {} dataBlockKey {} indexBlockKey {}", readHandle.getId(), dataBlockKey, indexBlockKey);
-
-            MultipartUpload mpu = null;
-            List<MultipartPart> parts = Lists.newArrayList();
-
-            // init multi part upload for data block.
-            try {
-                BlobBuilder blobBuilder = writeBlobStore.blobBuilder(dataBlockKey);
-                Map<String, String> objectMetadata = new HashMap<>(userMetadata);
-                objectMetadata.put("role", "data");
-                if (extraMetadata != null) {
-                   objectMetadata.putAll(extraMetadata);
-                }
-                DataBlockUtils.addVersionInfo(blobBuilder, objectMetadata);
-                Blob blob = blobBuilder.build();
-                log.info("initiateMultipartUpload bucket {}, metadata {} ", config.getBucket(), blob.getMetadata());
-                mpu = writeBlobStore.initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());
-            } catch (Throwable t) {
-                promise.completeExceptionally(t);
-                return;
-            }
-
-            long dataObjectLength = 0;
-            // start multi part upload for data block.
-            try {
-                long startEntry = 0;
-                int partId = 1;
-                long start = System.nanoTime();
-                long entryBytesWritten = 0;
-                while (startEntry <= readHandle.getLastAddConfirmed()) {
-                    int blockSize = BlockAwareSegmentInputStreamImpl
-                        .calculateBlockSize(config.getMaxBlockSizeInBytes(), readHandle, startEntry, entryBytesWritten);
-
-                    try (BlockAwareSegmentInputStream blockStream = new BlockAwareSegmentInputStreamImpl(
-                            readHandle, startEntry, blockSize, this.offloaderStats, managedLedgerName)) {
-
-                        Payload partPayload = Payloads.newInputStreamPayload(blockStream);
-                        partPayload.getContentMetadata().setContentLength((long) blockSize);
-                        partPayload.getContentMetadata().setContentType("application/octet-stream");
-                        parts.add(writeBlobStore.uploadMultipartPart(mpu, partId, partPayload));
-                        log.debug("UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}",
-                                config.getBucket(), dataBlockKey, partId, mpu.id());
-
-                        indexBuilder.addBlock(startEntry, partId, blockSize);
-
-                        if (blockStream.getEndEntryId() != -1) {
-                            startEntry = blockStream.getEndEntryId() + 1;
-                        } else {
-                            // could not read entry from ledger.
-                            break;
-                        }
-                        entryBytesWritten += blockStream.getBlockEntryBytesCount();
-                        partId++;
-                        this.offloaderStats.recordOffloadBytes(topicName, blockStream.getBlockEntryBytesCount());
-                    }
-
-                    dataObjectLength += blockSize;
-                }
-
-                String etag = writeBlobStore.completeMultipartUpload(mpu, parts);
-                log.info("Ledger {}, upload finished, etag {}", readHandle.getId(), etag);
-                mpu = null;
-            } catch (Throwable t) {
-                try {
-                    if (mpu != null) {
-                        writeBlobStore.abortMultipartUpload(mpu);
-                    }
-                } catch (Throwable throwable) {
-                    log.error("Failed abortMultipartUpload in bucket - {} with key - {}, uploadId - {}.",
-                            config.getBucket(), dataBlockKey, mpu.id(), throwable);
-                }
-                this.offloaderStats.recordWriteToStorageError(topicName);
-                this.offloaderStats.recordOffloadError(topicName);
-                promise.completeExceptionally(t);
-                return;
-            }
-
-            // upload index block
-            try (OffloadIndexBlock index = indexBuilder.withDataObjectLength(dataObjectLength).build();
-                 IndexInputStream indexStream = index.toStream()) {
-                // write the index block
-                BlobBuilder blobBuilder = writeBlobStore.blobBuilder(indexBlockKey);
-                Map<String, String> objectMetadata = new HashMap<>(userMetadata);
-                objectMetadata.put("role", "index");
-                if (extraMetadata != null) {
-                    objectMetadata.putAll(extraMetadata);
-                }
-                DataBlockUtils.addVersionInfo(blobBuilder, objectMetadata);
-                Payload indexPayload = Payloads.newInputStreamPayload(indexStream);
-                indexPayload.getContentMetadata().setContentLength((long) indexStream.getStreamSize());
-                indexPayload.getContentMetadata().setContentType("application/octet-stream");
-
-                Blob blob = blobBuilder
-                        .payload(indexPayload)
-                        .contentLength((long) indexStream.getStreamSize())
-                    .build();
-                writeBlobStore.putBlob(config.getBucket(), blob);
-                promise.complete(null);
-            } catch (Throwable t) {
-                try {
-                    writeBlobStore.removeBlob(config.getBucket(), dataBlockKey);
-                } catch (Throwable throwable) {
-                    log.error("Failed deleteObject in bucket - {} with key - {}.",
-                            config.getBucket(), dataBlockKey, throwable);
-                }
-
-                this.offloaderStats.recordWriteToStorageError(topicName);
-                this.offloaderStats.recordOffloadError(topicName);
-                promise.completeExceptionally(t);
-                return;
-            }
-        });
-        return promise;
-    }
-
-    BlobStore blobStore;
-    String streamingDataBlockKey;
-    String streamingDataIndexKey;
-    MultipartUpload streamingMpu = null;
-    List<MultipartPart> streamingParts = Lists.newArrayList();
-
-    @Override
-    public CompletableFuture<OffloadHandle> streamingOffload(@NonNull ManagedLedger ml, UUID uuid, long beginLedger,
-                                                             long beginEntry,
-                                                             Map<String, String> driverMetadata) {
-        if (this.ml != null) {
-            log.error("streamingOffload should only be called once");
-            final CompletableFuture<OffloadHandle> result = new CompletableFuture<>();
-            result.completeExceptionally(new RuntimeException("streamingOffload should only be called once"));
-        }
-
-        this.ml = ml;
-        this.segmentInfo = new OffloadSegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(),
-                driverMetadata);
-        log.debug("begin offload with {}:{}", beginLedger, beginEntry);
-        this.offloadResult = new CompletableFuture<>();
-        blobStore = getBlobStore(config.getBlobStoreLocation());
-        streamingIndexBuilder = OffloadIndexBlockV2Builder.create();
-        streamingDataBlockKey = segmentInfo.uuid.toString();
-        streamingDataIndexKey = String.format("%s-index", segmentInfo.uuid);
-        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);
-        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);
-        Blob blob = blobBuilder.build();
-        streamingMpu = blobStore
-                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());
-
-        scheduler.chooseThread(segmentInfo).execute(() -> {
-            log.info("start offloading segment: {}", segmentInfo);
-            streamingOffloadLoop(1, 0);
-        });
-        scheduler.schedule(this::closeSegment, maxSegmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);
-
-        return CompletableFuture.completedFuture(new OffloadHandle() {
-            @Override
-            public Position lastOffered() {
-                return BlobStoreManagedLedgerOffloader.this.lastOffered();
-            }
-
-            @Override
-            public CompletableFuture<Position> lastOfferedAsync() {
-                return CompletableFuture.completedFuture(lastOffered());
-            }
-
-            @Override
-            public OfferEntryResult offerEntry(Entry entry) {
-                return BlobStoreManagedLedgerOffloader.this.offerEntry(entry);
-            }
-
-            @Override
-            public CompletableFuture<OfferEntryResult> offerEntryAsync(Entry entry) {
-                return CompletableFuture.completedFuture(offerEntry(entry));
-            }
-
-            @Override
-            public CompletableFuture<OffloadResult> getOffloadResultAsync() {
-                return BlobStoreManagedLedgerOffloader.this.getOffloadResultAsync();
-            }
-
-            @Override
-            public boolean close() {
-                return BlobStoreManagedLedgerOffloader.this.closeSegment();
-            }
-        });
-    }
-
-    private void streamingOffloadLoop(int partId, int dataObjectLength) {
-        log.debug("streaming offload loop {} {}", partId, dataObjectLength);
-        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {
-            buildIndexAndCompleteResult(dataObjectLength);
-            offloadResult.complete(segmentInfo.result());
-        } else if ((segmentInfo.isClosed() && !offloadBuffer.isEmpty())
-                // last time to build and upload block
-                || bufferLength.get() >= streamingBlockSize
-            // buffer size full, build and upload block
-        ) {
-            List<Entry> entries = new LinkedList<>();
-            int blockEntrySize = 0;
-            final Entry firstEntry = offloadBuffer.poll();
-            entries.add(firstEntry);
-            long blockLedgerId = firstEntry.getLedgerId();
-            long blockEntryId = firstEntry.getEntryId();
-
-            while (!offloadBuffer.isEmpty() && offloadBuffer.peek().getLedgerId() == blockLedgerId
-                    && blockEntrySize <= streamingBlockSize) {
-                final Entry entryInBlock = offloadBuffer.poll();
-                final int entrySize = entryInBlock.getLength();
-                bufferLength.addAndGet(-entrySize);
-                blockEntrySize += entrySize;
-                entries.add(entryInBlock);
-            }
-            final int blockSize = BufferedOffloadStream
-                    .calculateBlockSize(streamingBlockSize, entries.size(), blockEntrySize);
-            buildBlockAndUpload(blockSize, entries, blockLedgerId, blockEntryId, partId);
-            streamingOffloadLoop(partId + 1, dataObjectLength + blockSize);
-        } else {
-            log.debug("not enough data, delay schedule for part: {} length: {}", partId, dataObjectLength);
-            scheduler.chooseThread(segmentInfo)
-                    .schedule(() -> {
-                        streamingOffloadLoop(partId, dataObjectLength);
-                    }, 100, TimeUnit.MILLISECONDS);
-        }
-    }
-
-    private void buildBlockAndUpload(int blockSize, List<Entry> entries, long blockLedgerId, long beginEntryId,
-                                     int partId) {
-        try (final BufferedOffloadStream payloadStream = new BufferedOffloadStream(blockSize, entries,
-                blockLedgerId, beginEntryId)) {
-            log.debug("begin upload payload: {} {}", blockLedgerId, beginEntryId);
-            Payload partPayload = Payloads.newInputStreamPayload(payloadStream);
-            partPayload.getContentMetadata().setContentType("application/octet-stream");
-            streamingParts.add(blobStore.uploadMultipartPart(streamingMpu, partId, partPayload));
-            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());
-            streamingIndexBuilder.addBlock(blockLedgerId, beginEntryId, partId, blockSize);
-            final MLDataFormats.ManagedLedgerInfo.LedgerInfo ledgerInfo = ml.getLedgerInfo(blockLedgerId).get();
-            final MLDataFormats.ManagedLedgerInfo.LedgerInfo.Builder ledgerInfoBuilder =
-                    MLDataFormats.ManagedLedgerInfo.LedgerInfo.newBuilder();
-            if (ledgerInfo != null) {
-                ledgerInfoBuilder.mergeFrom(ledgerInfo);
-            }
-            if (ledgerInfoBuilder.getEntries() == 0) {
-                //ledger unclosed, use last entry id of the block
-                ledgerInfoBuilder.setEntries(payloadStream.getEndEntryId() + 1);
-            }
-            streamingIndexBuilder.addLedgerMeta(blockLedgerId, ledgerInfoBuilder.build());
-            log.debug("UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}",
-                    config.getBucket(), streamingDataBlockKey, partId, streamingMpu.id());
-        } catch (Throwable e) {
-            blobStore.abortMultipartUpload(streamingMpu);
-            offloadResult.completeExceptionally(e);
-            return;
-        }
-    }
-
-    private void buildIndexAndCompleteResult(long dataObjectLength) {
-        try {
-            blobStore.completeMultipartUpload(streamingMpu, streamingParts);
-            streamingIndexBuilder.withDataObjectLength(dataObjectLength);
-            final OffloadIndexBlockV2 index = streamingIndexBuilder.buildV2();
-            final IndexInputStream indexStream = index.toStream();
-            final BlobBuilder indexBlobBuilder = blobStore.blobBuilder(streamingDataIndexKey);
-            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());
-
-            DataBlockUtils.addVersionInfo(indexBlobBuilder, userMetadata);
-            try (final InputStreamPayload indexPayLoad = Payloads.newInputStreamPayload(indexStream)) {
-                indexPayLoad.getContentMetadata().setContentLength(indexStream.getStreamSize());
-                indexPayLoad.getContentMetadata().setContentType("application/octet-stream");
-                final Blob indexBlob = indexBlobBuilder.payload(indexPayLoad)
-                        .contentLength(indexStream.getStreamSize())
-                        .build();
-                blobStore.putBlob(config.getBucket(), indexBlob);
-
-                final OffloadResult result = segmentInfo.result();
-                offloadResult.complete(result);
-                log.debug("offload segment completed {}", result);
-            } catch (Exception e) {
-                log.error("streaming offload failed", e);
-                offloadResult.completeExceptionally(e);
-            }
-        } catch (Exception e) {
-            log.error("streaming offload failed", e);
-            offloadResult.completeExceptionally(e);
-        }
-    }
-
-    private CompletableFuture<OffloadResult> getOffloadResultAsync() {
-        return this.offloadResult;
-    }
-
-    private synchronized OfferEntryResult offerEntry(Entry entry) {
-
-        if (segmentInfo.isClosed()) {
-            log.debug("Segment already closed {}", segmentInfo);
-            return OfferEntryResult.FAIL_SEGMENT_CLOSED;
-        } else if (maxBufferLength <= bufferLength.get()) {
-            //buffer length can over fill maxBufferLength a bit with the last entry
-            //to prevent insufficient content to build a block
-            return OfferEntryResult.FAIL_BUFFER_FULL;
-        } else {
-            final EntryImpl entryImpl = EntryImpl
-                    .create(entry.getLedgerId(), entry.getEntryId(), entry.getDataBuffer());
-            offloadBuffer.add(entryImpl);
-            bufferLength.getAndAdd(entryImpl.getLength());
-            segmentLength.getAndAdd(entryImpl.getLength());
-            lastOfferedPosition = entryImpl.getPosition();
-            if (segmentLength.get() >= maxSegmentLength
-                    && System.currentTimeMillis() - segmentBeginTimeMillis >= minSegmentCloseTimeMillis) {
-                closeSegment();
-            }
-            return OfferEntryResult.SUCCESS;
-        }
-    }
-
-    private synchronized boolean closeSegment() {
-        final boolean result = !segmentInfo.isClosed();
-        log.debug("close segment {} {}", lastOfferedPosition.getLedgerId(), lastOfferedPosition.getEntryId());
-        this.segmentInfo.closeSegment(lastOfferedPosition.getLedgerId(), lastOfferedPosition.getEntryId());
-        return result;
-    }
-
-    private Position lastOffered() {
-        return lastOfferedPosition;
-    }
-
-    /**
-     * Attempts to create a BlobStoreLocation from the values in the offloadDriverMetadata,
-     * however, if no values are available, it defaults to the currently configured
-     * provider, region, bucket, etc.
-     *
-     * @param offloadDriverMetadata
-     * @return
-     */
-    private BlobStoreLocation getBlobStoreLocation(Map<String, String> offloadDriverMetadata) {
-        return (!offloadDriverMetadata.isEmpty()) ? new BlobStoreLocation(offloadDriverMetadata) :
-            new BlobStoreLocation(getOffloadDriverMetadata());
-    }
-
-    @Override
-    public CompletableFuture<ReadHandle> readOffloaded(long ledgerId, UUID uid,
-                                                       Map<String, String> offloadDriverMetadata) {
-
-        BlobStoreLocation bsKey = getBlobStoreLocation(offloadDriverMetadata);
-        String readBucket = bsKey.getBucket();
-
-        CompletableFuture<ReadHandle> promise = new CompletableFuture<>();
-        String key = DataBlockUtils.dataBlockOffloadKey(ledgerId, uid);
-        String indexKey = DataBlockUtils.indexBlockOffloadKey(ledgerId, uid);
-        readExecutor.chooseThread(ledgerId).execute(() -> {
-            try {
-                BlobStore readBlobstore = getBlobStore(config.getBlobStoreLocation());
-                promise.complete(BlobStoreBackedReadHandleImpl.open(readExecutor.chooseThread(ledgerId),
-                        readBlobstore,
-                        readBucket, key, indexKey,
-                        DataBlockUtils.VERSION_CHECK,
-                        ledgerId, config.getReadBufferSizeInBytes(),
-                        this.offloaderStats, offloadDriverMetadata.get(MANAGED_LEDGER_NAME),
-                        this.entryOffsetsCache));
-            } catch (Throwable t) {
-                log.error("Failed readOffloaded: ", t);
-                promise.completeExceptionally(t);
-            }
-        });
-        return promise;
-    }
-
-    @Override
-    public CompletableFuture<ReadHandle> readOffloaded(long ledgerId, MLDataFormats.OffloadContext ledgerContext,
-                                                       Map<String, String> offloadDriverMetadata) {
-        BlobStoreLocation bsKey = getBlobStoreLocation(offloadDriverMetadata);
-        String readBucket = bsKey.getBucket();
-        CompletableFuture<ReadHandle> promise = new CompletableFuture<>();
-        final List<MLDataFormats.OffloadSegment> offloadSegmentList = ledgerContext.getOffloadSegmentList();
-        List<String> keys = Lists.newLinkedList();
-        List<String> indexKeys = Lists.newLinkedList();
-        offloadSegmentList.forEach(seg -> {
-            final UUID uuid = new UUID(seg.getUidMsb(), seg.getUidLsb());
-            final String key = uuid.toString();
-            final String indexKey = DataBlockUtils.indexBlockOffloadKey(uuid);
-            keys.add(key);
-            indexKeys.add(indexKey);
-        });
-
-        readExecutor.chooseThread(ledgerId).execute(() -> {
-            try {
-                BlobStore readBlobstore = getBlobStore(config.getBlobStoreLocation());
-                promise.complete(BlobStoreBackedReadHandleImplV2.open(readExecutor.chooseThread(ledgerId),
-                        readBlobstore,
-                        readBucket, keys, indexKeys,
-                        DataBlockUtils.VERSION_CHECK,
-                        ledgerId, config.getReadBufferSizeInBytes(),
-                        this.offloaderStats, offloadDriverMetadata.get(MANAGED_LEDGER_NAME)));
-            } catch (Throwable t) {
-                log.error("Failed readOffloaded: ", t);
-                promise.completeExceptionally(t);
-            }
-        });
-        return promise;
-    }
-
-    @Override
-    public CompletableFuture<Void> deleteOffloaded(long ledgerId, UUID uid,
-                                                   Map<String, String> offloadDriverMetadata) {
-        BlobStoreLocation bsKey = getBlobStoreLocation(offloadDriverMetadata);
-        String readBucket = bsKey.getBucket(offloadDriverMetadata);
-
-        CompletableFuture<Void> promise = new CompletableFuture<>();
-        scheduler.chooseThread(ledgerId).execute(() -> {
-            try {
-                BlobStore readBlobstore = getBlobStore(config.getBlobStoreLocation());
-                readBlobstore.removeBlobs(readBucket,
-                    ImmutableList.of(DataBlockUtils.dataBlockOffloadKey(ledgerId, uid),
-                                     DataBlockUtils.indexBlockOffloadKey(ledgerId, uid)));
-                promise.complete(null);
-            } catch (Throwable t) {
-                log.error("Failed delete Blob", t);
-                promise.completeExceptionally(t);
-            }
-        });
-
-        return promise.whenComplete((__, t) -> {
-            if (null != this.ml) {
-                this.offloaderStats.recordDeleteOffloadOps(
-                  TopicName.fromPersistenceNamingEncoding(this.ml.getName()), t == null);
-            }
-        });
-    }
-
-    @Override
-    public CompletableFuture<Void> deleteOffloaded(UUID uid, Map<String, String> offloadDriverMetadata) {
-        BlobStoreLocation bsKey = getBlobStoreLocation(offloadDriverMetadata);
-        String readBucket = bsKey.getBucket(offloadDriverMetadata);
-
-        CompletableFuture<Void> promise = new CompletableFuture<>();
-        scheduler.execute(() -> {
-            try {
-                BlobStore readBlobstore = getBlobStore(config.getBlobStoreLocation());
-                readBlobstore.removeBlobs(readBucket,
-                        ImmutableList.of(uid.toString(),
-                                DataBlockUtils.indexBlockOffloadKey(uid)));
-                promise.complete(null);
-            } catch (Throwable t) {
-                log.error("Failed delete Blob", t);
-                promise.completeExceptionally(t);
-            }
-        });
-
-        return promise.whenComplete((__, t) ->
-                this.offloaderStats.recordDeleteOffloadOps(
-                  TopicName.fromPersistenceNamingEncoding(this.ml.getName()), t == null));
-    }
-
-    @Override
-    public OffloadPolicies getOffloadPolicies() {
-        return this.policies;
-    }
-
-    @Override
-    public void close() {
-        for (BlobStore readBlobStore : blobStores.values()) {
-            if (readBlobStore != null) {
-                readBlobStore.getContext().close();
-            }
-        }
-    }
-
-    @Override
-    public void scanLedgers(OffloadedLedgerMetadataConsumer consumer, Map<String,
-            String> offloadDriverMetadata) throws ManagedLedgerException {
-        BlobStoreLocation bsKey = getBlobStoreLocation(offloadDriverMetadata);
-        String endpoint = bsKey.getEndpoint();
-        String readBucket = bsKey.getBucket();
-        log.info("Scanning bucket {}, bsKey {}, location {} endpoint{} ", readBucket, bsKey,
-                config.getBlobStoreLocation(), endpoint);
-        BlobStore readBlobstore = getBlobStore(config.getBlobStoreLocation());
-        int batchSize = 100;
-        String bucketName = config.getBucket();
-        String marker = null;
-        do {
-            marker = scanContainer(consumer, readBlobstore, bucketName, marker, batchSize);
-        } while (marker != null);
-
-    }
-
-    private String scanContainer(OffloadedLedgerMetadataConsumer consumer, BlobStore readBlobstore,
-                                 String bucketName,
-                                 String lastMarker,
-                                 int batchSize) throws ManagedLedgerException {
-        ListContainerOptions options = new ListContainerOptions()
-                .maxResults(batchSize)
-                .withDetails();
-        if (lastMarker != null) {
-            options.afterMarker(lastMarker);
-        }
-        PageSet<? extends StorageMetadata> pages = readBlobstore.list(bucketName, options);
-        for (StorageMetadata md : pages) {
-            log.info("Found {} ", md);
-            String name = md.getName();
-            Long size = md.getSize();
-            Date lastModified = md.getLastModified();
-            StorageType type = md.getType();
-            if (type != StorageType.BLOB) {
-                continue;
-            }
-            URI uri = md.getUri();
-            Map<String, String> userMetadata = md.getUserMetadata();
-            Long ledgerId = DataBlockUtils.parseLedgerId(name);
-            String contextUuid = DataBlockUtils.parseContextUuid(name, ledgerId);
-            log.info("info {} {} {} {} {} {} ledgerId {}", name, size, lastModified, type, uri, userMetadata, ledgerId);
-            OffloadedLedgerMetadata offloadedLedgerMetadata = OffloadedLedgerMetadata.builder()
-                    .name(name)
-                    .bucketName(bucketName)
-                    .uuid(contextUuid)
-                    .ledgerId(ledgerId != null ? ledgerId : -1)
-                    .lastModified(lastModified != null ? lastModified.getTime() : 0)
-                    .size(size != null ? size : -1)
-                    .uri(uri != null ? uri.toString() : null)
-                    .userMetadata(userMetadata != null ? userMetadata : Collections.emptyMap())
-                    .build();
-            try {
-                boolean canContinue = consumer.accept(offloadedLedgerMetadata);
-                if (!canContinue) {
-                    log.info("Iteration stopped by the OffloadedLedgerMetadataConsumer");
-                    return null;
-                }
-            } catch (Exception err) {
-                log.error("Error in the OffloadedLedgerMetadataConsumer", err);
-                if (err instanceof InterruptedException) {
-                    Thread.currentThread().interrupt();
-                }
-                throw ManagedLedgerException.getManagedLedgerException(err);
-            }
-        }
-        log.info("NextMarker is {}", pages.getNextMarker());
-        return pages.getNextMarker();
-    }
-
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import com.google.common.base.Strings;
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.Lists;
+import java.io.IOException;
+import java.net.URI;
+import java.time.Duration;
+import java.util.Collections;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentLinkedQueue;
+import java.util.concurrent.ConcurrentMap;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicLong;
+import lombok.NonNull;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.bookkeeper.client.api.ReadHandle;
+import org.apache.bookkeeper.common.util.OrderedScheduler;
+import org.apache.bookkeeper.mledger.Entry;
+import org.apache.bookkeeper.mledger.LedgerOffloader;
+import org.apache.bookkeeper.mledger.LedgerOffloader.OffloadHandle.OfferEntryResult;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.bookkeeper.mledger.ManagedLedger;
+import org.apache.bookkeeper.mledger.ManagedLedgerException;
+import org.apache.bookkeeper.mledger.OffloadedLedgerMetadata;
+import org.apache.bookkeeper.mledger.OffloadedLedgerMetadataConsumer;
+import org.apache.bookkeeper.mledger.Position;
+import org.apache.bookkeeper.mledger.PositionFactory;
+import org.apache.bookkeeper.mledger.impl.EntryImpl;
+import org.apache.bookkeeper.mledger.impl.OffloadSegmentInfoImpl;
+import org.apache.bookkeeper.mledger.offload.jcloud.BlockAwareSegmentInputStream;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock.IndexInputStream;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockBuilder;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2Builder;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.BlobStoreLocation;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration;
+import org.apache.bookkeeper.mledger.proto.MLDataFormats;
+import org.apache.pulsar.common.naming.TopicName;
+import org.apache.pulsar.common.policies.data.OffloadPolicies;
+import org.apache.pulsar.common.policies.data.OffloadPoliciesImpl;
+import org.jclouds.blobstore.BlobStore;
+import org.jclouds.blobstore.domain.Blob;
+import org.jclouds.blobstore.domain.BlobBuilder;
+import org.jclouds.blobstore.domain.MultipartPart;
+import org.jclouds.blobstore.domain.MultipartUpload;
+import org.jclouds.blobstore.domain.PageSet;
+import org.jclouds.blobstore.domain.StorageMetadata;
+import org.jclouds.blobstore.domain.StorageType;
+import org.jclouds.blobstore.options.ListContainerOptions;
+import org.jclouds.blobstore.options.PutOptions;
+import org.jclouds.domain.Location;
+import org.jclouds.domain.LocationBuilder;
+import org.jclouds.domain.LocationScope;
+import org.jclouds.io.Payload;
+import org.jclouds.io.Payloads;
+import org.jclouds.io.payloads.InputStreamPayload;
+
+/**
+ * Tiered Storage Offloader that is backed by a JCloud Blob Store.
+ * <p>
+ * The constructor takes an instance of TieredStorageConfiguration, which
+ * contains all of the configuration data necessary to connect to a JCloud
+ * Provider service.
+ * </p>
+ */
+@Slf4j
+public class BlobStoreManagedLedgerOffloader implements LedgerOffloader {
+
+    private static final String MANAGED_LEDGER_NAME = "ManagedLedgerName";
+
+    private final OrderedScheduler scheduler;
+    private final OrderedScheduler readExecutor;
+    private final TieredStorageConfiguration config;
+    private final OffloadPolicies policies;
+    private final Location writeLocation;
+
+    // metadata to be stored as part of the offloaded ledger metadata
+    private final Map<String, String> userMetadata;
+
+    private final ConcurrentMap<BlobStoreLocation, BlobStore> blobStores = new ConcurrentHashMap<>();
+    private OffloadSegmentInfoImpl segmentInfo;
+    private final AtomicLong bufferLength = new AtomicLong(0);
+    private final AtomicLong segmentLength = new AtomicLong(0);
+    private final long maxBufferLength;
+    private final OffsetsCache entryOffsetsCache;
+    private final ConcurrentLinkedQueue<Entry> offloadBuffer = new ConcurrentLinkedQueue<>();
+    private CompletableFuture<OffloadResult> offloadResult;
+    private volatile Position lastOfferedPosition = PositionFactory.LATEST;
+    private final Duration maxSegmentCloseTime;
+    private final long minSegmentCloseTimeMillis;
+    private final long segmentBeginTimeMillis;
+    private final long maxSegmentLength;
+    private final int streamingBlockSize;
+    private volatile ManagedLedger ml;
+    private OffloadIndexBlockV2Builder streamingIndexBuilder;
+    private final LedgerOffloaderStats offloaderStats;
+
+    public static BlobStoreManagedLedgerOffloader create(TieredStorageConfiguration config,
+                                                         Map<String, String> userMetadata,
+                                                         OrderedScheduler scheduler,
+                                                         OrderedScheduler readExecutor,
+                                                         LedgerOffloaderStats offloaderStats,
+                                                         OffsetsCache entryOffsetsCache)
+            throws IOException {
+
+        return new BlobStoreManagedLedgerOffloader(config, scheduler, readExecutor,
+                userMetadata, offloaderStats, entryOffsetsCache);
+    }
+
+    BlobStoreManagedLedgerOffloader(TieredStorageConfiguration config, OrderedScheduler scheduler,
+                                    OrderedScheduler readExecutor,
+                                    Map<String, String> userMetadata, LedgerOffloaderStats offloaderStats,
+                                    OffsetsCache entryOffsetsCache) {
+        this.scheduler = scheduler;
+        this.readExecutor = readExecutor;
+        this.userMetadata = userMetadata;
+        this.config = config;
+        Properties properties = new Properties();
+        properties.putAll(config.getConfigProperties());
+        this.policies = OffloadPoliciesImpl.create(properties);
+        this.streamingBlockSize = config.getMinBlockSizeInBytes();
+        this.maxSegmentCloseTime = Duration.ofSeconds(config.getMaxSegmentTimeInSecond());
+        this.maxSegmentLength = config.getMaxSegmentSizeInBytes();
+        this.minSegmentCloseTimeMillis = Duration.ofSeconds(config.getMinSegmentTimeInSecond()).toMillis();
+        //ensure buffer can have enough content to fill a block
+        this.maxBufferLength = Math.max(config.getWriteBufferSizeInBytes(), config.getMinBlockSizeInBytes());
+        this.entryOffsetsCache = entryOffsetsCache;
+        this.segmentBeginTimeMillis = System.currentTimeMillis();
+        if (!Strings.isNullOrEmpty(config.getRegion())) {
+            this.writeLocation = new LocationBuilder()
+                    .scope(LocationScope.REGION)
+                    .id(config.getRegion())
+                    .description(config.getRegion())
+                    .build();
+        } else {
+            this.writeLocation = null;
+        }
+
+        log.info("Constructor offload driver: {}, host: {}, container: {}, region: {} ",
+                config.getProvider().getDriver(), config.getServiceEndpoint(),
+                config.getBucket(), config.getRegion());
+
+        this.offloaderStats = offloaderStats;
+        log.info("The ledger offloader was created.");
+    }
+
+    private BlobStore getBlobStore(BlobStoreLocation blobStoreLocation) {
+        return blobStores.computeIfAbsent(blobStoreLocation, location -> {
+            log.info("Creating blob store for location {}", location);
+            return config.getBlobStore();
+        });
+    }
+
+    @Override
+    public String getOffloadDriverName() {
+        return config.getDriver();
+    }
+
+    @Override
+    public Map<String, String> getOffloadDriverMetadata() {
+        return config.getOffloadDriverMetadata();
+    }
+
+    /**
+     * Upload the DataBlocks associated with the given ReadHandle using MultiPartUpload,
+     * Creating indexBlocks for each corresponding DataBlock that is uploaded.
+     */
+    @Override
+    public CompletableFuture<Void> offload(ReadHandle readHandle,
+                                           UUID uuid,
+                                           Map<String, String> extraMetadata) {
+        final String managedLedgerName = extraMetadata.get(MANAGED_LEDGER_NAME);
+        final String topicName = TopicName.fromPersistenceNamingEncoding(managedLedgerName);
+        CompletableFuture<Void> promise = new CompletableFuture<>();
+        scheduler.chooseThread(readHandle.getId()).execute(() -> {
+            final BlobStore writeBlobStore = getBlobStore(config.getBlobStoreLocation());
+            log.info("offload {} uuid {} extraMetadata {} to {} {}", readHandle.getId(), uuid, extraMetadata,
+                config.getBlobStoreLocation(), writeBlobStore);
+            if (!readHandle.isClosed() || readHandle.getLastAddConfirmed() < 0) {
+                promise.completeExceptionally(
+                        new IllegalArgumentException("An empty or open ledger should never be offloaded"));
+                return;
+            }
+            if (readHandle.getLength() <= 0) {
+                log.warn("[{}] Ledger [{}] has zero length, but it contains {} entries."
+                    + " Attempting to offload ledger since it contains entries.", topicName, readHandle.getId(),
+                    readHandle.getLastAddConfirmed() + 1);
+            }
+            OffloadIndexBlockBuilder indexBuilder = OffloadIndexBlockBuilder.create()
+                .withLedgerMetadata(readHandle.getLedgerMetadata())
+                .withDataBlockHeaderLength(BlockAwareSegmentInputStreamImpl.getHeaderSize());
+            String dataBlockKey = DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid);
+            String indexBlockKey = DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid);
+            log.info("ledger {} dataBlockKey {} indexBlockKey {}", readHandle.getId(), dataBlockKey, indexBlockKey);
+
+            MultipartUpload mpu = null;
+            List<MultipartPart> parts = Lists.newArrayList();
+
+            // init multi part upload for data block.
+            try {
+                BlobBuilder blobBuilder = writeBlobStore.blobBuilder(dataBlockKey);
+                Map<String, String> objectMetadata = new HashMap<>(userMetadata);
+                objectMetadata.put("role", "data");
+                if (extraMetadata != null) {
+                   objectMetadata.putAll(extraMetadata);
+                }
+                DataBlockUtils.addVersionInfo(blobBuilder, objectMetadata);
+                Blob blob = blobBuilder.build();
+                log.info("initiateMultipartUpload bucket {}, metadata {} ", config.getBucket(), blob.getMetadata());
+                mpu = writeBlobStore.initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());
+            } catch (Throwable t) {
+                promise.completeExceptionally(t);
+                return;
+            }
+
+            long dataObjectLength = 0;
+            // start multi part upload for data block.
+            try {
+                long startEntry = 0;
+                int partId = 1;
+                long start = System.nanoTime();
+                long entryBytesWritten = 0;
+                while (startEntry <= readHandle.getLastAddConfirmed()) {
+                    int blockSize = BlockAwareSegmentInputStreamImpl
+                        .calculateBlockSize(config.getMaxBlockSizeInBytes(), readHandle, startEntry, entryBytesWritten);
+
+                    try (BlockAwareSegmentInputStream blockStream = new BlockAwareSegmentInputStreamImpl(
+                            readHandle, startEntry, blockSize, this.offloaderStats, managedLedgerName)) {
+
+                        Payload partPayload = Payloads.newInputStreamPayload(blockStream);
+                        partPayload.getContentMetadata().setContentLength((long) blockSize);
+                        partPayload.getContentMetadata().setContentType("application/octet-stream");
+                        parts.add(writeBlobStore.uploadMultipartPart(mpu, partId, partPayload));
+                        log.debug("UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}",
+                                config.getBucket(), dataBlockKey, partId, mpu.id());
+
+                        indexBuilder.addBlock(startEntry, partId, blockSize);
+
+                        if (blockStream.getEndEntryId() != -1) {
+                            startEntry = blockStream.getEndEntryId() + 1;
+                        } else {
+                            // could not read entry from ledger.
+                            break;
+                        }
+                        entryBytesWritten += blockStream.getBlockEntryBytesCount();
+                        partId++;
+                        this.offloaderStats.recordOffloadBytes(topicName, blockStream.getBlockEntryBytesCount());
+                    }
+
+                    dataObjectLength += blockSize;
+                }
+
+                String etag = writeBlobStore.completeMultipartUpload(mpu, parts);
+                log.info("Ledger {}, upload finished, etag {}", readHandle.getId(), etag);
+                mpu = null;
+            } catch (Throwable t) {
+                try {
+                    if (mpu != null) {
+                        writeBlobStore.abortMultipartUpload(mpu);
+                    }
+                } catch (Throwable throwable) {
+                    log.error("Failed abortMultipartUpload in bucket - {} with key - {}, uploadId - {}.",
+                            config.getBucket(), dataBlockKey, mpu.id(), throwable);
+                }
+                this.offloaderStats.recordWriteToStorageError(topicName);
+                this.offloaderStats.recordOffloadError(topicName);
+                promise.completeExceptionally(t);
+                return;
+            }
+
+            // upload index block
+            try (OffloadIndexBlock index = indexBuilder.withDataObjectLength(dataObjectLength).build();
+                 IndexInputStream indexStream = index.toStream()) {
+                // write the index block
+                BlobBuilder blobBuilder = writeBlobStore.blobBuilder(indexBlockKey);
+                Map<String, String> objectMetadata = new HashMap<>(userMetadata);
+                objectMetadata.put("role", "index");
+                if (extraMetadata != null) {
+                    objectMetadata.putAll(extraMetadata);
+                }
+                DataBlockUtils.addVersionInfo(blobBuilder, objectMetadata);
+                Payload indexPayload = Payloads.newInputStreamPayload(indexStream);
+                indexPayload.getContentMetadata().setContentLength((long) indexStream.getStreamSize());
+                indexPayload.getContentMetadata().setContentType("application/octet-stream");
+
+                Blob blob = blobBuilder
+                        .payload(indexPayload)
+                        .contentLength((long) indexStream.getStreamSize())
+                    .build();
+                writeBlobStore.putBlob(config.getBucket(), blob);
+                promise.complete(null);
+            } catch (Throwable t) {
+                try {
+                    writeBlobStore.removeBlob(config.getBucket(), dataBlockKey);
+                } catch (Throwable throwable) {
+                    log.error("Failed deleteObject in bucket - {} with key - {}.",
+                            config.getBucket(), dataBlockKey, throwable);
+                }
+
+                this.offloaderStats.recordWriteToStorageError(topicName);
+                this.offloaderStats.recordOffloadError(topicName);
+                promise.completeExceptionally(t);
+                return;
+            }
+        });
+        return promise;
+    }
+
+    BlobStore blobStore;
+    String streamingDataBlockKey;
+    String streamingDataIndexKey;
+    MultipartUpload streamingMpu = null;
+    List<MultipartPart> streamingParts = Lists.newArrayList();
+
+    @Override
+    public CompletableFuture<OffloadHandle> streamingOffload(@NonNull ManagedLedger ml, UUID uuid, long beginLedger,
+                                                             long beginEntry,
+                                                             Map<String, String> driverMetadata) {
+        if (this.ml != null) {
+            log.error("streamingOffload should only be called once");
+            final CompletableFuture<OffloadHandle> result = new CompletableFuture<>();
+            result.completeExceptionally(new RuntimeException("streamingOffload should only be called once"));
+        }
+
+        this.ml = ml;
+        this.segmentInfo = new OffloadSegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(),
+                driverMetadata);
+        log.debug("begin offload with {}:{}", beginLedger, beginEntry);
+        this.offloadResult = new CompletableFuture<>();
+        blobStore = getBlobStore(config.getBlobStoreLocation());
+        streamingIndexBuilder = OffloadIndexBlockV2Builder.create();
+        streamingDataBlockKey = segmentInfo.uuid.toString();
+        streamingDataIndexKey = String.format("%s-index", segmentInfo.uuid);
+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);
+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);
+        Blob blob = blobBuilder.build();
+        streamingMpu = blobStore
+                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());
+
+        scheduler.chooseThread(segmentInfo).execute(() -> {
+            log.info("start offloading segment: {}", segmentInfo);
+            streamingOffloadLoop(1, 0);
+        });
+        scheduler.schedule(this::closeSegment, maxSegmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);
+
+        return CompletableFuture.completedFuture(new OffloadHandle() {
+            @Override
+            public Position lastOffered() {
+                return BlobStoreManagedLedgerOffloader.this.lastOffered();
+            }
+
+            @Override
+            public CompletableFuture<Position> lastOfferedAsync() {
+                return CompletableFuture.completedFuture(lastOffered());
+            }
+
+            @Override
+            public OfferEntryResult offerEntry(Entry entry) {
+                return BlobStoreManagedLedgerOffloader.this.offerEntry(entry);
+            }
+
+            @Override
+            public CompletableFuture<OfferEntryResult> offerEntryAsync(Entry entry) {
+                return CompletableFuture.completedFuture(offerEntry(entry));
+            }
+
+            @Override
+            public CompletableFuture<OffloadResult> getOffloadResultAsync() {
+                return BlobStoreManagedLedgerOffloader.this.getOffloadResultAsync();
+            }
+
+            @Override
+            public boolean close() {
+                return BlobStoreManagedLedgerOffloader.this.closeSegment();
+            }
+        });
+    }
+
+    private void streamingOffloadLoop(int partId, int dataObjectLength) {
+        log.debug("streaming offload loop {} {}", partId, dataObjectLength);
+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {
+            buildIndexAndCompleteResult(dataObjectLength);
+            offloadResult.complete(segmentInfo.result());
+        } else if ((segmentInfo.isClosed() && !offloadBuffer.isEmpty())
+                // last time to build and upload block
+                || bufferLength.get() >= streamingBlockSize
+            // buffer size full, build and upload block
+        ) {
+            List<Entry> entries = new LinkedList<>();
+            int blockEntrySize = 0;
+            final Entry firstEntry = offloadBuffer.poll();
+            entries.add(firstEntry);
+            long blockLedgerId = firstEntry.getLedgerId();
+            long blockEntryId = firstEntry.getEntryId();
+
+            while (!offloadBuffer.isEmpty() && offloadBuffer.peek().getLedgerId() == blockLedgerId
+                    && blockEntrySize <= streamingBlockSize) {
+                final Entry entryInBlock = offloadBuffer.poll();
+                final int entrySize = entryInBlock.getLength();
+                bufferLength.addAndGet(-entrySize);
+                blockEntrySize += entrySize;
+                entries.add(entryInBlock);
+            }
+            final int blockSize = BufferedOffloadStream
+                    .calculateBlockSize(streamingBlockSize, entries.size(), blockEntrySize);
+            buildBlockAndUpload(blockSize, entries, blockLedgerId, blockEntryId, partId);
+            streamingOffloadLoop(partId + 1, dataObjectLength + blockSize);
+        } else {
+            log.debug("not enough data, delay schedule for part: {} length: {}", partId, dataObjectLength);
+            scheduler.chooseThread(segmentInfo)
+                    .schedule(() -> {
+                        streamingOffloadLoop(partId, dataObjectLength);
+                    }, 100, TimeUnit.MILLISECONDS);
+        }
+    }
+
+    private void buildBlockAndUpload(int blockSize, List<Entry> entries, long blockLedgerId, long beginEntryId,
+                                     int partId) {
+        try (final BufferedOffloadStream payloadStream = new BufferedOffloadStream(blockSize, entries,
+                blockLedgerId, beginEntryId)) {
+            log.debug("begin upload payload: {} {}", blockLedgerId, beginEntryId);
+            Payload partPayload = Payloads.newInputStreamPayload(payloadStream);
+            partPayload.getContentMetadata().setContentType("application/octet-stream");
+            streamingParts.add(blobStore.uploadMultipartPart(streamingMpu, partId, partPayload));
+            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());
+            streamingIndexBuilder.addBlock(blockLedgerId, beginEntryId, partId, blockSize);
+            final MLDataFormats.ManagedLedgerInfo.LedgerInfo ledgerInfo = ml.getLedgerInfo(blockLedgerId).get();
+            final MLDataFormats.ManagedLedgerInfo.LedgerInfo.Builder ledgerInfoBuilder =
+                    MLDataFormats.ManagedLedgerInfo.LedgerInfo.newBuilder();
+            if (ledgerInfo != null) {
+                ledgerInfoBuilder.mergeFrom(ledgerInfo);
+            }
+            if (ledgerInfoBuilder.getEntries() == 0) {
+                //ledger unclosed, use last entry id of the block
+                ledgerInfoBuilder.setEntries(payloadStream.getEndEntryId() + 1);
+            }
+            streamingIndexBuilder.addLedgerMeta(blockLedgerId, ledgerInfoBuilder.build());
+            log.debug("UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}",
+                    config.getBucket(), streamingDataBlockKey, partId, streamingMpu.id());
+        } catch (Throwable e) {
+            blobStore.abortMultipartUpload(streamingMpu);
+            offloadResult.completeExceptionally(e);
+            return;
+        }
+    }
+
+    private void buildIndexAndCompleteResult(long dataObjectLength) {
+        try {
+            blobStore.completeMultipartUpload(streamingMpu, streamingParts);
+            streamingIndexBuilder.withDataObjectLength(dataObjectLength);
+            final OffloadIndexBlockV2 index = streamingIndexBuilder.buildV2();
+            final IndexInputStream indexStream = index.toStream();
+            final BlobBuilder indexBlobBuilder = blobStore.blobBuilder(streamingDataIndexKey);
+            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());
+
+            DataBlockUtils.addVersionInfo(indexBlobBuilder, userMetadata);
+            try (final InputStreamPayload indexPayLoad = Payloads.newInputStreamPayload(indexStream)) {
+                indexPayLoad.getContentMetadata().setContentLength(indexStream.getStreamSize());
+                indexPayLoad.getContentMetadata().setContentType("application/octet-stream");
+                final Blob indexBlob = indexBlobBuilder.payload(indexPayLoad)
+                        .contentLength(indexStream.getStreamSize())
+                        .build();
+                blobStore.putBlob(config.getBucket(), indexBlob);
+
+                final OffloadResult result = segmentInfo.result();
+                offloadResult.complete(result);
+                log.debug("offload segment completed {}", result);
+            } catch (Exception e) {
+                log.error("streaming offload failed", e);
+                offloadResult.completeExceptionally(e);
+            }
+        } catch (Exception e) {
+            log.error("streaming offload failed", e);
+            offloadResult.completeExceptionally(e);
+        }
+    }
+
+    private CompletableFuture<OffloadResult> getOffloadResultAsync() {
+        return this.offloadResult;
+    }
+
+    private synchronized OfferEntryResult offerEntry(Entry entry) {
+
+        if (segmentInfo.isClosed()) {
+            log.debug("Segment already closed {}", segmentInfo);
+            return OfferEntryResult.FAIL_SEGMENT_CLOSED;
+        } else if (maxBufferLength <= bufferLength.get()) {
+            //buffer length can over fill maxBufferLength a bit with the last entry
+            //to prevent insufficient content to build a block
+            return OfferEntryResult.FAIL_BUFFER_FULL;
+        } else {
+            final EntryImpl entryImpl = EntryImpl
+                    .create(entry.getLedgerId(), entry.getEntryId(), entry.getDataBuffer());
+            offloadBuffer.add(entryImpl);
+            bufferLength.getAndAdd(entryImpl.getLength());
+            segmentLength.getAndAdd(entryImpl.getLength());
+            lastOfferedPosition = entryImpl.getPosition();
+            if (segmentLength.get() >= maxSegmentLength
+                    && System.currentTimeMillis() - segmentBeginTimeMillis >= minSegmentCloseTimeMillis) {
+                closeSegment();
+            }
+            return OfferEntryResult.SUCCESS;
+        }
+    }
+
+    private synchronized boolean closeSegment() {
+        final boolean result = !segmentInfo.isClosed();
+        log.debug("close segment {} {}", lastOfferedPosition.getLedgerId(), lastOfferedPosition.getEntryId());
+        this.segmentInfo.closeSegment(lastOfferedPosition.getLedgerId(), lastOfferedPosition.getEntryId());
+        return result;
+    }
+
+    private Position lastOffered() {
+        return lastOfferedPosition;
+    }
+
+    /**
+     * Attempts to create a BlobStoreLocation from the values in the offloadDriverMetadata,
+     * however, if no values are available, it defaults to the currently configured
+     * provider, region, bucket, etc.
+     *
+     * @param offloadDriverMetadata
+     * @return
+     */
+    private BlobStoreLocation getBlobStoreLocation(Map<String, String> offloadDriverMetadata) {
+        return (!offloadDriverMetadata.isEmpty()) ? new BlobStoreLocation(offloadDriverMetadata) :
+            new BlobStoreLocation(getOffloadDriverMetadata());
+    }
+
+    @Override
+    public CompletableFuture<ReadHandle> readOffloaded(long ledgerId, UUID uid,
+                                                       Map<String, String> offloadDriverMetadata) {
+
+        BlobStoreLocation bsKey = getBlobStoreLocation(offloadDriverMetadata);
+        String readBucket = bsKey.getBucket();
+
+        CompletableFuture<ReadHandle> promise = new CompletableFuture<>();
+        String key = DataBlockUtils.dataBlockOffloadKey(ledgerId, uid);
+        String indexKey = DataBlockUtils.indexBlockOffloadKey(ledgerId, uid);
+        readExecutor.chooseThread(ledgerId).execute(() -> {
+            try {
+                BlobStore readBlobstore = getBlobStore(config.getBlobStoreLocation());
+                promise.complete(BlobStoreBackedReadHandleImpl.open(readExecutor.chooseThread(ledgerId),
+                        readBlobstore,
+                        readBucket, key, indexKey,
+                        DataBlockUtils.VERSION_CHECK,
+                        ledgerId, config.getReadBufferSizeInBytes(),
+                        this.offloaderStats, offloadDriverMetadata.get(MANAGED_LEDGER_NAME),
+                        this.entryOffsetsCache));
+            } catch (Throwable t) {
+                log.error("Failed readOffloaded: ", t);
+                promise.completeExceptionally(t);
+            }
+        });
+        return promise;
+    }
+
+    @Override
+    public CompletableFuture<ReadHandle> readOffloaded(long ledgerId, MLDataFormats.OffloadContext ledgerContext,
+                                                       Map<String, String> offloadDriverMetadata) {
+        BlobStoreLocation bsKey = getBlobStoreLocation(offloadDriverMetadata);
+        String readBucket = bsKey.getBucket();
+        CompletableFuture<ReadHandle> promise = new CompletableFuture<>();
+        final List<MLDataFormats.OffloadSegment> offloadSegmentList = ledgerContext.getOffloadSegmentList();
+        List<String> keys = Lists.newLinkedList();
+        List<String> indexKeys = Lists.newLinkedList();
+        offloadSegmentList.forEach(seg -> {
+            final UUID uuid = new UUID(seg.getUidMsb(), seg.getUidLsb());
+            final String key = uuid.toString();
+            final String indexKey = DataBlockUtils.indexBlockOffloadKey(uuid);
+            keys.add(key);
+            indexKeys.add(indexKey);
+        });
+
+        readExecutor.chooseThread(ledgerId).execute(() -> {
+            try {
+                BlobStore readBlobstore = getBlobStore(config.getBlobStoreLocation());
+                promise.complete(BlobStoreBackedReadHandleImplV2.open(readExecutor.chooseThread(ledgerId),
+                        readBlobstore,
+                        readBucket, keys, indexKeys,
+                        DataBlockUtils.VERSION_CHECK,
+                        ledgerId, config.getReadBufferSizeInBytes(),
+                        this.offloaderStats, offloadDriverMetadata.get(MANAGED_LEDGER_NAME)));
+            } catch (Throwable t) {
+                log.error("Failed readOffloaded: ", t);
+                promise.completeExceptionally(t);
+            }
+        });
+        return promise;
+    }
+
+    @Override
+    public CompletableFuture<Void> deleteOffloaded(long ledgerId, UUID uid,
+                                                   Map<String, String> offloadDriverMetadata) {
+        BlobStoreLocation bsKey = getBlobStoreLocation(offloadDriverMetadata);
+        String readBucket = bsKey.getBucket(offloadDriverMetadata);
+
+        CompletableFuture<Void> promise = new CompletableFuture<>();
+        scheduler.chooseThread(ledgerId).execute(() -> {
+            try {
+                BlobStore readBlobstore = getBlobStore(config.getBlobStoreLocation());
+                readBlobstore.removeBlobs(readBucket,
+                    ImmutableList.of(DataBlockUtils.dataBlockOffloadKey(ledgerId, uid),
+                                     DataBlockUtils.indexBlockOffloadKey(ledgerId, uid)));
+                promise.complete(null);
+            } catch (Throwable t) {
+                log.error("Failed delete Blob", t);
+                promise.completeExceptionally(t);
+            }
+        });
+
+        return promise.whenComplete((__, t) -> {
+            if (null != this.ml) {
+                this.offloaderStats.recordDeleteOffloadOps(
+                  TopicName.fromPersistenceNamingEncoding(this.ml.getName()), t == null);
+            }
+        });
+    }
+
+    @Override
+    public CompletableFuture<Void> deleteOffloaded(UUID uid, Map<String, String> offloadDriverMetadata) {
+        BlobStoreLocation bsKey = getBlobStoreLocation(offloadDriverMetadata);
+        String readBucket = bsKey.getBucket(offloadDriverMetadata);
+
+        CompletableFuture<Void> promise = new CompletableFuture<>();
+        scheduler.execute(() -> {
+            try {
+                BlobStore readBlobstore = getBlobStore(config.getBlobStoreLocation());
+                readBlobstore.removeBlobs(readBucket,
+                        ImmutableList.of(uid.toString(),
+                                DataBlockUtils.indexBlockOffloadKey(uid)));
+                promise.complete(null);
+            } catch (Throwable t) {
+                log.error("Failed delete Blob", t);
+                promise.completeExceptionally(t);
+            }
+        });
+
+        return promise.whenComplete((__, t) ->
+                this.offloaderStats.recordDeleteOffloadOps(
+                  TopicName.fromPersistenceNamingEncoding(this.ml.getName()), t == null));
+    }
+
+    @Override
+    public OffloadPolicies getOffloadPolicies() {
+        return this.policies;
+    }
+
+    @Override
+    public void close() {
+        for (BlobStore readBlobStore : blobStores.values()) {
+            if (readBlobStore != null) {
+                readBlobStore.getContext().close();
+            }
+        }
+    }
+
+    @Override
+    public void scanLedgers(OffloadedLedgerMetadataConsumer consumer, Map<String,
+            String> offloadDriverMetadata) throws ManagedLedgerException {
+        BlobStoreLocation bsKey = getBlobStoreLocation(offloadDriverMetadata);
+        String endpoint = bsKey.getEndpoint();
+        String readBucket = bsKey.getBucket();
+        log.info("Scanning bucket {}, bsKey {}, location {} endpoint{} ", readBucket, bsKey,
+                config.getBlobStoreLocation(), endpoint);
+        BlobStore readBlobstore = getBlobStore(config.getBlobStoreLocation());
+        int batchSize = 100;
+        String bucketName = config.getBucket();
+        String marker = null;
+        do {
+            marker = scanContainer(consumer, readBlobstore, bucketName, marker, batchSize);
+        } while (marker != null);
+
+    }
+
+    private String scanContainer(OffloadedLedgerMetadataConsumer consumer, BlobStore readBlobstore,
+                                 String bucketName,
+                                 String lastMarker,
+                                 int batchSize) throws ManagedLedgerException {
+        ListContainerOptions options = new ListContainerOptions()
+                .maxResults(batchSize)
+                .withDetails();
+        if (lastMarker != null) {
+            options.afterMarker(lastMarker);
+        }
+        PageSet<? extends StorageMetadata> pages = readBlobstore.list(bucketName, options);
+        for (StorageMetadata md : pages) {
+            log.info("Found {} ", md);
+            String name = md.getName();
+            Long size = md.getSize();
+            Date lastModified = md.getLastModified();
+            StorageType type = md.getType();
+            if (type != StorageType.BLOB) {
+                continue;
+            }
+            URI uri = md.getUri();
+            Map<String, String> userMetadata = md.getUserMetadata();
+            Long ledgerId = DataBlockUtils.parseLedgerId(name);
+            String contextUuid = DataBlockUtils.parseContextUuid(name, ledgerId);
+            log.info("info {} {} {} {} {} {} ledgerId {}", name, size, lastModified, type, uri, userMetadata, ledgerId);
+            OffloadedLedgerMetadata offloadedLedgerMetadata = OffloadedLedgerMetadata.builder()
+                    .name(name)
+                    .bucketName(bucketName)
+                    .uuid(contextUuid)
+                    .ledgerId(ledgerId != null ? ledgerId : -1)
+                    .lastModified(lastModified != null ? lastModified.getTime() : 0)
+                    .size(size != null ? size : -1)
+                    .uri(uri != null ? uri.toString() : null)
+                    .userMetadata(userMetadata != null ? userMetadata : Collections.emptyMap())
+                    .build();
+            try {
+                boolean canContinue = consumer.accept(offloadedLedgerMetadata);
+                if (!canContinue) {
+                    log.info("Iteration stopped by the OffloadedLedgerMetadataConsumer");
+                    return null;
+                }
+            } catch (Exception err) {
+                log.error("Error in the OffloadedLedgerMetadataConsumer", err);
+                if (err instanceof InterruptedException) {
+                    Thread.currentThread().interrupt();
+                }
+                throw ManagedLedgerException.getManagedLedgerException(err);
+            }
+        }
+        log.info("NextMarker is {}", pages.getNextMarker());
+        return pages.getNextMarker();
+    }
+
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlockAwareSegmentInputStreamImpl.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlockAwareSegmentInputStreamImpl.java
index 06d7f2129b..1230a7f740 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlockAwareSegmentInputStreamImpl.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlockAwareSegmentInputStreamImpl.java
@@ -1,349 +1,349 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static com.google.common.base.Preconditions.checkState;
-import com.google.common.collect.Lists;
-import com.google.common.primitives.Ints;
-import io.netty.buffer.ByteBuf;
-import io.netty.buffer.CompositeByteBuf;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.Iterator;
-import java.util.List;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicBoolean;
-import org.apache.bookkeeper.client.api.LedgerEntries;
-import org.apache.bookkeeper.client.api.LedgerEntry;
-import org.apache.bookkeeper.client.api.ReadHandle;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.bookkeeper.mledger.offload.jcloud.BlockAwareSegmentInputStream;
-import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
-import org.apache.pulsar.common.naming.TopicName;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * The BlockAwareSegmentInputStreamImpl for each cold storage data block.
- * It gets data from ledger, and will be read out the content for a data block.
- * DataBlockHeader + entries(each with format[[entry_size -- int][entry_id -- long][entry_data]]) + padding
- */
-public class BlockAwareSegmentInputStreamImpl extends BlockAwareSegmentInputStream {
-    private static final Logger log = LoggerFactory.getLogger(BlockAwareSegmentInputStreamImpl.class);
-
-    static final int[] BLOCK_END_PADDING = new int[]{ 0xFE, 0xDC, 0xDE, 0xAD };
-    static final byte[] BLOCK_END_PADDING_BYTES =  Ints.toByteArray(0xFEDCDEAD);
-
-    private final ByteBuf paddingBuf = PulsarByteBufAllocator.DEFAULT.buffer(128, 128);
-
-    private final ReadHandle ledger;
-    private final long startEntryId;
-    private final int blockSize;
-
-    // Number of Message entries that read from ledger and been readout from this InputStream.
-    private int blockEntryCount;
-
-    // tracking read status for both header and entries.
-    // Bytes that already been read from this InputStream
-    private int bytesReadOffset = 0;
-    // Byte from this index is all padding byte
-    private int dataBlockFullOffset;
-    private final InputStream dataBlockHeaderStream;
-
-    // how many entries want to read from ReadHandle each time.
-    private static final int ENTRIES_PER_READ = 100;
-    // buf the entry size and entry id.
-    static final int ENTRY_HEADER_SIZE = 4 /* entry size */ + 8 /* entry id */;
-    // Keep a list of all entries ByteBuf, each ByteBuf contains 2 buf: entry header and entry content.
-    private List<ByteBuf> entriesByteBuf = null;
-    private LedgerOffloaderStats offloaderStats;
-    private String managedLedgerName;
-    private String topicName;
-    private int currentOffset = 0;
-    private final AtomicBoolean close = new AtomicBoolean(false);
-
-    public BlockAwareSegmentInputStreamImpl(ReadHandle ledger, long startEntryId, int blockSize) {
-        this.ledger = ledger;
-        this.startEntryId = startEntryId;
-        this.blockSize = blockSize;
-        this.dataBlockHeaderStream = DataBlockHeaderImpl.of(blockSize, startEntryId).toStream();
-        this.blockEntryCount = 0;
-        this.dataBlockFullOffset = blockSize;
-        this.entriesByteBuf = Lists.newLinkedList();
-    }
-
-    public BlockAwareSegmentInputStreamImpl(ReadHandle ledger, long startEntryId, int blockSize,
-                                            LedgerOffloaderStats offloaderStats, String ledgerName) {
-        this(ledger, startEntryId, blockSize);
-        this.offloaderStats = offloaderStats;
-        this.managedLedgerName = ledgerName;
-        this.topicName = TopicName.fromPersistenceNamingEncoding(ledgerName);
-    }
-
-    private ByteBuf readEntries(int len) throws IOException {
-        checkState(bytesReadOffset >= DataBlockHeaderImpl.getDataStartOffset());
-        checkState(bytesReadOffset < blockSize);
-
-        // once reach the end of entry buffer, read more, if there is more
-        if (bytesReadOffset < dataBlockFullOffset
-            && entriesByteBuf.isEmpty()
-            && startEntryId + blockEntryCount <= ledger.getLastAddConfirmed()) {
-            entriesByteBuf = readNextEntriesFromLedger(startEntryId + blockEntryCount, ENTRIES_PER_READ);
-        }
-
-        if (!entriesByteBuf.isEmpty()
-            && bytesReadOffset + entriesByteBuf.get(0).readableBytes() <= blockSize) {
-            // always read from the first ByteBuf in the list, once read all of its content remove it.
-            ByteBuf entryByteBuf = entriesByteBuf.get(0);
-            int readableBytes = entryByteBuf.readableBytes();
-            int read = Math.min(readableBytes, len);
-            ByteBuf buf = entryByteBuf.slice(currentOffset, read);
-            buf.retain();
-            currentOffset += read;
-            entryByteBuf.readerIndex(currentOffset);
-            bytesReadOffset += read;
-
-            if (entryByteBuf.readableBytes() == 0) {
-                entryByteBuf.release();
-                entriesByteBuf.remove(0);
-                blockEntryCount++;
-                currentOffset = 0;
-            }
-
-            return buf;
-        } else {
-            // no space for a new entry or there are no more entries
-            // set data block full, return end padding
-            if (dataBlockFullOffset == blockSize) {
-                dataBlockFullOffset = bytesReadOffset;
-            }
-            paddingBuf.clear();
-            for (int i = 0; i < Math.min(len, paddingBuf.capacity()); i++) {
-                paddingBuf.writeByte(BLOCK_END_PADDING_BYTES[(bytesReadOffset++ - dataBlockFullOffset)
-                    % BLOCK_END_PADDING_BYTES.length]);
-            }
-            return paddingBuf.retain();
-        }
-    }
-
-    // read ledger entries.
-    private int readEntries() throws IOException {
-        checkState(bytesReadOffset >= DataBlockHeaderImpl.getDataStartOffset());
-        checkState(bytesReadOffset < blockSize);
-
-        // once reach the end of entry buffer, read more, if there is more
-        if (bytesReadOffset < dataBlockFullOffset
-            && entriesByteBuf.isEmpty()
-            && startEntryId + blockEntryCount <= ledger.getLastAddConfirmed()) {
-            entriesByteBuf = readNextEntriesFromLedger(startEntryId + blockEntryCount, ENTRIES_PER_READ);
-        }
-
-        if (!entriesByteBuf.isEmpty() && bytesReadOffset + entriesByteBuf.get(0).readableBytes() <= blockSize) {
-            // always read from the first ByteBuf in the list, once read all of its content remove it.
-            ByteBuf entryByteBuf = entriesByteBuf.get(0);
-            int ret = entryByteBuf.readUnsignedByte();
-            bytesReadOffset++;
-
-            if (entryByteBuf.readableBytes() == 0) {
-                entryByteBuf.release();
-                entriesByteBuf.remove(0);
-                blockEntryCount++;
-            }
-
-            return ret;
-        } else {
-            // no space for a new entry or there are no more entries
-            // set data block full, return end padding
-            if (dataBlockFullOffset == blockSize) {
-                dataBlockFullOffset = bytesReadOffset;
-            }
-            return BLOCK_END_PADDING[(bytesReadOffset++ - dataBlockFullOffset) % BLOCK_END_PADDING.length];
-        }
-    }
-
-    private List<ByteBuf> readNextEntriesFromLedger(long start, long maxNumberEntries) throws IOException {
-        long end = Math.min(start + maxNumberEntries - 1, ledger.getLastAddConfirmed());
-        long startTime = System.nanoTime();
-        try (LedgerEntries ledgerEntriesOnce = ledger.readAsync(start, end).get()) {
-            if (log.isDebugEnabled()) {
-                log.debug("read ledger entries. start: {}, end: {} cost {}", start, end,
-                        TimeUnit.NANOSECONDS.toMicros(System.nanoTime() - startTime));
-            }
-            if (offloaderStats != null && managedLedgerName != null) {
-                offloaderStats.recordReadLedgerLatency(topicName, System.nanoTime() - startTime,
-                        TimeUnit.NANOSECONDS);
-            }
-
-            List<ByteBuf> entries = Lists.newLinkedList();
-            Iterator<LedgerEntry> iterator = ledgerEntriesOnce.iterator();
-            while (iterator.hasNext()) {
-                LedgerEntry entry = iterator.next();
-                ByteBuf buf = entry.getEntryBuffer().retain();
-                int entryLength = buf.readableBytes();
-                long entryId = entry.getEntryId();
-
-                CompositeByteBuf entryBuf = PulsarByteBufAllocator.DEFAULT.compositeBuffer(2);
-                ByteBuf entryHeaderBuf = PulsarByteBufAllocator.DEFAULT.buffer(ENTRY_HEADER_SIZE, ENTRY_HEADER_SIZE);
-
-                entryHeaderBuf.writeInt(entryLength).writeLong(entryId);
-                entryBuf.addComponents(true, entryHeaderBuf, buf);
-
-                entries.add(entryBuf);
-            }
-            return entries;
-        } catch (InterruptedException | ExecutionException e) {
-            log.error("Exception when get CompletableFuture<LedgerEntries>. ", e);
-            if (e instanceof InterruptedException) {
-                Thread.currentThread().interrupt();
-            }
-            throw new IOException(e);
-        }
-    }
-
-    @Override
-    public int read(byte[] b, int off, int len) throws IOException {
-        if (b == null) {
-            throw new NullPointerException("The given bytes are null");
-        } else if (off < 0 || len < 0 || len > b.length - off) {
-            throw new IndexOutOfBoundsException("off=" + off + ", len=" + len + ", b.length=" + b.length);
-        } else if (len == 0) {
-            return 0;
-        }
-
-        int offset = off;
-        int readLen = len;
-        int readBytes = 0;
-        // reading header
-        if (dataBlockHeaderStream.available() > 0) {
-            int read = dataBlockHeaderStream.read(b, off, len);
-            offset += read;
-            readLen -= read;
-            readBytes += read;
-            bytesReadOffset += read;
-        }
-        if (readLen == 0) {
-            return readBytes;
-        }
-
-        // reading ledger entries
-        if (bytesReadOffset < blockSize) {
-            readLen = Math.min(readLen, blockSize - bytesReadOffset);
-            ByteBuf readEntries = readEntries(readLen);
-            int read = readEntries.readableBytes();
-            readEntries.readBytes(b, offset, read);
-            readEntries.release();
-            readBytes += read;
-            return readBytes;
-        }
-
-        // reached end
-        return -1;
-    }
-
-    @Override
-    public int read() throws IOException {
-        // reading header
-        if (dataBlockHeaderStream.available() > 0) {
-            bytesReadOffset++;
-            return dataBlockHeaderStream.read();
-        }
-
-        // reading Ledger entries.
-        if (bytesReadOffset < blockSize) {
-            return readEntries();
-        }
-
-        // reached end
-        return -1;
-    }
-
-    @Override
-    public void close() throws IOException {
-        // The close method will be triggered twice in the BlobStoreManagedLedgerOffloader#offload method.
-        // The stream resource used by the try-with block which will called the close
-        // And through debug, writeBlobStore.uploadMultipartPart in the offload method also will trigger
-        // the close method.
-        // So we add the close variable to avoid release paddingBuf twice.
-        if (close.compareAndSet(false, true)) {
-            super.close();
-            dataBlockHeaderStream.close();
-            if (!entriesByteBuf.isEmpty()) {
-                entriesByteBuf.forEach(buf -> buf.release());
-                entriesByteBuf.clear();
-            }
-            paddingBuf.clear();
-            paddingBuf.release();
-        }
-    }
-
-    @Override
-    public ReadHandle getLedger() {
-        return ledger;
-    }
-
-    @Override
-    public long getStartEntryId() {
-        return startEntryId;
-    }
-
-    @Override
-    public int getBlockSize() {
-        return blockSize;
-    }
-
-    public int getDataBlockFullOffset() {
-        return dataBlockFullOffset;
-    }
-
-    @Override
-    public int getBlockEntryCount() {
-        return blockEntryCount;
-    }
-
-    @Override
-    public long getEndEntryId() {
-        // return -1 when no entry contained
-        if (blockEntryCount == 0) {
-            return -1;
-        }
-        return startEntryId + blockEntryCount - 1;
-    }
-
-    @Override
-    public int getBlockEntryBytesCount() {
-        return dataBlockFullOffset - DataBlockHeaderImpl.getDataStartOffset() - ENTRY_HEADER_SIZE * blockEntryCount;
-    }
-
-    public static long getHeaderSize() {
-        return DataBlockHeaderImpl.getDataStartOffset();
-    }
-
-    // Calculate the block size after uploaded `entryBytesAlreadyWritten` bytes
-    public static int calculateBlockSize(int maxBlockSize, ReadHandle readHandle,
-                                         long firstEntryToWrite, long entryBytesAlreadyWritten) {
-        return (int) Math.min(
-            maxBlockSize,
-            (readHandle.getLastAddConfirmed() - firstEntryToWrite + 1) * ENTRY_HEADER_SIZE
-                + (readHandle.getLength() - entryBytesAlreadyWritten)
-                + DataBlockHeaderImpl.getDataStartOffset());
-    }
-
-}
-
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static com.google.common.base.Preconditions.checkState;
+import com.google.common.collect.Lists;
+import com.google.common.primitives.Ints;
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.CompositeByteBuf;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.Iterator;
+import java.util.List;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+import org.apache.bookkeeper.client.api.LedgerEntries;
+import org.apache.bookkeeper.client.api.LedgerEntry;
+import org.apache.bookkeeper.client.api.ReadHandle;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.bookkeeper.mledger.offload.jcloud.BlockAwareSegmentInputStream;
+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
+import org.apache.pulsar.common.naming.TopicName;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * The BlockAwareSegmentInputStreamImpl for each cold storage data block.
+ * It gets data from ledger, and will be read out the content for a data block.
+ * DataBlockHeader + entries(each with format[[entry_size -- int][entry_id -- long][entry_data]]) + padding
+ */
+public class BlockAwareSegmentInputStreamImpl extends BlockAwareSegmentInputStream {
+    private static final Logger log = LoggerFactory.getLogger(BlockAwareSegmentInputStreamImpl.class);
+
+    static final int[] BLOCK_END_PADDING = new int[]{ 0xFE, 0xDC, 0xDE, 0xAD };
+    static final byte[] BLOCK_END_PADDING_BYTES =  Ints.toByteArray(0xFEDCDEAD);
+
+    private final ByteBuf paddingBuf = PulsarByteBufAllocator.DEFAULT.buffer(128, 128);
+
+    private final ReadHandle ledger;
+    private final long startEntryId;
+    private final int blockSize;
+
+    // Number of Message entries that read from ledger and been readout from this InputStream.
+    private int blockEntryCount;
+
+    // tracking read status for both header and entries.
+    // Bytes that already been read from this InputStream
+    private int bytesReadOffset = 0;
+    // Byte from this index is all padding byte
+    private int dataBlockFullOffset;
+    private final InputStream dataBlockHeaderStream;
+
+    // how many entries want to read from ReadHandle each time.
+    private static final int ENTRIES_PER_READ = 100;
+    // buf the entry size and entry id.
+    static final int ENTRY_HEADER_SIZE = 4 /* entry size */ + 8 /* entry id */;
+    // Keep a list of all entries ByteBuf, each ByteBuf contains 2 buf: entry header and entry content.
+    private List<ByteBuf> entriesByteBuf = null;
+    private LedgerOffloaderStats offloaderStats;
+    private String managedLedgerName;
+    private String topicName;
+    private int currentOffset = 0;
+    private final AtomicBoolean close = new AtomicBoolean(false);
+
+    public BlockAwareSegmentInputStreamImpl(ReadHandle ledger, long startEntryId, int blockSize) {
+        this.ledger = ledger;
+        this.startEntryId = startEntryId;
+        this.blockSize = blockSize;
+        this.dataBlockHeaderStream = DataBlockHeaderImpl.of(blockSize, startEntryId).toStream();
+        this.blockEntryCount = 0;
+        this.dataBlockFullOffset = blockSize;
+        this.entriesByteBuf = Lists.newLinkedList();
+    }
+
+    public BlockAwareSegmentInputStreamImpl(ReadHandle ledger, long startEntryId, int blockSize,
+                                            LedgerOffloaderStats offloaderStats, String ledgerName) {
+        this(ledger, startEntryId, blockSize);
+        this.offloaderStats = offloaderStats;
+        this.managedLedgerName = ledgerName;
+        this.topicName = TopicName.fromPersistenceNamingEncoding(ledgerName);
+    }
+
+    private ByteBuf readEntries(int len) throws IOException {
+        checkState(bytesReadOffset >= DataBlockHeaderImpl.getDataStartOffset());
+        checkState(bytesReadOffset < blockSize);
+
+        // once reach the end of entry buffer, read more, if there is more
+        if (bytesReadOffset < dataBlockFullOffset
+            && entriesByteBuf.isEmpty()
+            && startEntryId + blockEntryCount <= ledger.getLastAddConfirmed()) {
+            entriesByteBuf = readNextEntriesFromLedger(startEntryId + blockEntryCount, ENTRIES_PER_READ);
+        }
+
+        if (!entriesByteBuf.isEmpty()
+            && bytesReadOffset + entriesByteBuf.get(0).readableBytes() <= blockSize) {
+            // always read from the first ByteBuf in the list, once read all of its content remove it.
+            ByteBuf entryByteBuf = entriesByteBuf.get(0);
+            int readableBytes = entryByteBuf.readableBytes();
+            int read = Math.min(readableBytes, len);
+            ByteBuf buf = entryByteBuf.slice(currentOffset, read);
+            buf.retain();
+            currentOffset += read;
+            entryByteBuf.readerIndex(currentOffset);
+            bytesReadOffset += read;
+
+            if (entryByteBuf.readableBytes() == 0) {
+                entryByteBuf.release();
+                entriesByteBuf.remove(0);
+                blockEntryCount++;
+                currentOffset = 0;
+            }
+
+            return buf;
+        } else {
+            // no space for a new entry or there are no more entries
+            // set data block full, return end padding
+            if (dataBlockFullOffset == blockSize) {
+                dataBlockFullOffset = bytesReadOffset;
+            }
+            paddingBuf.clear();
+            for (int i = 0; i < Math.min(len, paddingBuf.capacity()); i++) {
+                paddingBuf.writeByte(BLOCK_END_PADDING_BYTES[(bytesReadOffset++ - dataBlockFullOffset)
+                    % BLOCK_END_PADDING_BYTES.length]);
+            }
+            return paddingBuf.retain();
+        }
+    }
+
+    // read ledger entries.
+    private int readEntries() throws IOException {
+        checkState(bytesReadOffset >= DataBlockHeaderImpl.getDataStartOffset());
+        checkState(bytesReadOffset < blockSize);
+
+        // once reach the end of entry buffer, read more, if there is more
+        if (bytesReadOffset < dataBlockFullOffset
+            && entriesByteBuf.isEmpty()
+            && startEntryId + blockEntryCount <= ledger.getLastAddConfirmed()) {
+            entriesByteBuf = readNextEntriesFromLedger(startEntryId + blockEntryCount, ENTRIES_PER_READ);
+        }
+
+        if (!entriesByteBuf.isEmpty() && bytesReadOffset + entriesByteBuf.get(0).readableBytes() <= blockSize) {
+            // always read from the first ByteBuf in the list, once read all of its content remove it.
+            ByteBuf entryByteBuf = entriesByteBuf.get(0);
+            int ret = entryByteBuf.readUnsignedByte();
+            bytesReadOffset++;
+
+            if (entryByteBuf.readableBytes() == 0) {
+                entryByteBuf.release();
+                entriesByteBuf.remove(0);
+                blockEntryCount++;
+            }
+
+            return ret;
+        } else {
+            // no space for a new entry or there are no more entries
+            // set data block full, return end padding
+            if (dataBlockFullOffset == blockSize) {
+                dataBlockFullOffset = bytesReadOffset;
+            }
+            return BLOCK_END_PADDING[(bytesReadOffset++ - dataBlockFullOffset) % BLOCK_END_PADDING.length];
+        }
+    }
+
+    private List<ByteBuf> readNextEntriesFromLedger(long start, long maxNumberEntries) throws IOException {
+        long end = Math.min(start + maxNumberEntries - 1, ledger.getLastAddConfirmed());
+        long startTime = System.nanoTime();
+        try (LedgerEntries ledgerEntriesOnce = ledger.readAsync(start, end).get()) {
+            if (log.isDebugEnabled()) {
+                log.debug("read ledger entries. start: {}, end: {} cost {}", start, end,
+                        TimeUnit.NANOSECONDS.toMicros(System.nanoTime() - startTime));
+            }
+            if (offloaderStats != null && managedLedgerName != null) {
+                offloaderStats.recordReadLedgerLatency(topicName, System.nanoTime() - startTime,
+                        TimeUnit.NANOSECONDS);
+            }
+
+            List<ByteBuf> entries = Lists.newLinkedList();
+            Iterator<LedgerEntry> iterator = ledgerEntriesOnce.iterator();
+            while (iterator.hasNext()) {
+                LedgerEntry entry = iterator.next();
+                ByteBuf buf = entry.getEntryBuffer().retain();
+                int entryLength = buf.readableBytes();
+                long entryId = entry.getEntryId();
+
+                CompositeByteBuf entryBuf = PulsarByteBufAllocator.DEFAULT.compositeBuffer(2);
+                ByteBuf entryHeaderBuf = PulsarByteBufAllocator.DEFAULT.buffer(ENTRY_HEADER_SIZE, ENTRY_HEADER_SIZE);
+
+                entryHeaderBuf.writeInt(entryLength).writeLong(entryId);
+                entryBuf.addComponents(true, entryHeaderBuf, buf);
+
+                entries.add(entryBuf);
+            }
+            return entries;
+        } catch (InterruptedException | ExecutionException e) {
+            log.error("Exception when get CompletableFuture<LedgerEntries>. ", e);
+            if (e instanceof InterruptedException) {
+                Thread.currentThread().interrupt();
+            }
+            throw new IOException(e);
+        }
+    }
+
+    @Override
+    public int read(byte[] b, int off, int len) throws IOException {
+        if (b == null) {
+            throw new NullPointerException("The given bytes are null");
+        } else if (off < 0 || len < 0 || len > b.length - off) {
+            throw new IndexOutOfBoundsException("off=" + off + ", len=" + len + ", b.length=" + b.length);
+        } else if (len == 0) {
+            return 0;
+        }
+
+        int offset = off;
+        int readLen = len;
+        int readBytes = 0;
+        // reading header
+        if (dataBlockHeaderStream.available() > 0) {
+            int read = dataBlockHeaderStream.read(b, off, len);
+            offset += read;
+            readLen -= read;
+            readBytes += read;
+            bytesReadOffset += read;
+        }
+        if (readLen == 0) {
+            return readBytes;
+        }
+
+        // reading ledger entries
+        if (bytesReadOffset < blockSize) {
+            readLen = Math.min(readLen, blockSize - bytesReadOffset);
+            ByteBuf readEntries = readEntries(readLen);
+            int read = readEntries.readableBytes();
+            readEntries.readBytes(b, offset, read);
+            readEntries.release();
+            readBytes += read;
+            return readBytes;
+        }
+
+        // reached end
+        return -1;
+    }
+
+    @Override
+    public int read() throws IOException {
+        // reading header
+        if (dataBlockHeaderStream.available() > 0) {
+            bytesReadOffset++;
+            return dataBlockHeaderStream.read();
+        }
+
+        // reading Ledger entries.
+        if (bytesReadOffset < blockSize) {
+            return readEntries();
+        }
+
+        // reached end
+        return -1;
+    }
+
+    @Override
+    public void close() throws IOException {
+        // The close method will be triggered twice in the BlobStoreManagedLedgerOffloader#offload method.
+        // The stream resource used by the try-with block which will called the close
+        // And through debug, writeBlobStore.uploadMultipartPart in the offload method also will trigger
+        // the close method.
+        // So we add the close variable to avoid release paddingBuf twice.
+        if (close.compareAndSet(false, true)) {
+            super.close();
+            dataBlockHeaderStream.close();
+            if (!entriesByteBuf.isEmpty()) {
+                entriesByteBuf.forEach(buf -> buf.release());
+                entriesByteBuf.clear();
+            }
+            paddingBuf.clear();
+            paddingBuf.release();
+        }
+    }
+
+    @Override
+    public ReadHandle getLedger() {
+        return ledger;
+    }
+
+    @Override
+    public long getStartEntryId() {
+        return startEntryId;
+    }
+
+    @Override
+    public int getBlockSize() {
+        return blockSize;
+    }
+
+    public int getDataBlockFullOffset() {
+        return dataBlockFullOffset;
+    }
+
+    @Override
+    public int getBlockEntryCount() {
+        return blockEntryCount;
+    }
+
+    @Override
+    public long getEndEntryId() {
+        // return -1 when no entry contained
+        if (blockEntryCount == 0) {
+            return -1;
+        }
+        return startEntryId + blockEntryCount - 1;
+    }
+
+    @Override
+    public int getBlockEntryBytesCount() {
+        return dataBlockFullOffset - DataBlockHeaderImpl.getDataStartOffset() - ENTRY_HEADER_SIZE * blockEntryCount;
+    }
+
+    public static long getHeaderSize() {
+        return DataBlockHeaderImpl.getDataStartOffset();
+    }
+
+    // Calculate the block size after uploaded `entryBytesAlreadyWritten` bytes
+    public static int calculateBlockSize(int maxBlockSize, ReadHandle readHandle,
+                                         long firstEntryToWrite, long entryBytesAlreadyWritten) {
+        return (int) Math.min(
+            maxBlockSize,
+            (readHandle.getLastAddConfirmed() - firstEntryToWrite + 1) * ENTRY_HEADER_SIZE
+                + (readHandle.getLength() - entryBytesAlreadyWritten)
+                + DataBlockHeaderImpl.getDataStartOffset());
+    }
+
+}
+
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BufferedOffloadStream.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BufferedOffloadStream.java
index f25c16b951..c220ed0f79 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BufferedOffloadStream.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BufferedOffloadStream.java
@@ -1,135 +1,135 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import io.netty.buffer.ByteBuf;
-import io.netty.buffer.CompositeByteBuf;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.List;
-import lombok.extern.slf4j.Slf4j;
-import org.apache.bookkeeper.mledger.Entry;
-import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
-
-@Slf4j
-public class BufferedOffloadStream extends InputStream {
-    static final int[] BLOCK_END_PADDING = BlockAwareSegmentInputStreamImpl.BLOCK_END_PADDING;
-
-    private final long ledgerId;
-    private final long beginEntryId;
-
-    public BufferedOffloadStream(int blockSize, List<Entry> entries, long ledgerId, long beginEntryId) {
-        this.ledgerId = ledgerId;
-        this.beginEntryId = beginEntryId;
-        this.endEntryId = beginEntryId;
-        this.blockSize = blockSize;
-        this.entryBuffer = entries;
-        this.blockHead = StreamingDataBlockHeaderImpl.of(blockSize, ledgerId, beginEntryId)
-                .toStream();
-    }
-
-    public long getEndEntryId() {
-        return endEntryId;
-    }
-
-    private volatile long endEntryId;
-    static final int ENTRY_HEADER_SIZE = 4 /* entry size */ + 8 /* entry id */;
-    private final long blockSize;
-    private final List<Entry> entryBuffer;
-    private final InputStream blockHead;
-    int offset = 0;
-    static final int NOT_INITIALIZED = -1;
-    int validDataOffset = NOT_INITIALIZED;
-    CompositeByteBuf currentEntry;
-
-    public long getLedgerId() {
-        return ledgerId;
-    }
-
-    public long getBeginEntryId() {
-        return beginEntryId;
-    }
-
-    public long getBlockSize() {
-        return blockSize;
-    }
-
-    @Override
-    public int read() throws IOException {
-        if (blockHead.available() > 0) {
-            offset++;
-            return blockHead.read();
-        }
-        //if current exists, use current first
-        if (currentEntry != null) {
-            if (currentEntry.readableBytes() > 0) {
-                offset += 1;
-                return currentEntry.readUnsignedByte();
-            } else {
-                currentEntry.release();
-                currentEntry = null;
-            }
-        }
-
-        if (blockSize <= offset) {
-            return -1;
-        } else if (validDataOffset != NOT_INITIALIZED) {
-            return BLOCK_END_PADDING[(offset++ - validDataOffset) % BLOCK_END_PADDING.length];
-        }
-
-
-        if (entryBuffer.isEmpty()) {
-            validDataOffset = offset;
-            return read();
-        }
-
-        Entry headEntry = entryBuffer.remove(0);
-
-        //create new block when a ledger end
-        if (headEntry.getLedgerId() != this.ledgerId) {
-            throw new RuntimeException(
-                    String.format("there should not be multi ledger in a block %s %s", headEntry.getLedgerId(),
-                            this.ledgerId));
-        }
-
-        final int entryLength = headEntry.getLength();
-        final long entryId = headEntry.getEntryId();
-        CompositeByteBuf entryBuf = PulsarByteBufAllocator.DEFAULT.compositeBuffer(2);
-        ByteBuf entryHeaderBuf = PulsarByteBufAllocator.DEFAULT.buffer(ENTRY_HEADER_SIZE, ENTRY_HEADER_SIZE);
-        entryHeaderBuf.writeInt(entryLength).writeLong(entryId);
-        entryBuf.addComponents(true, entryHeaderBuf, headEntry.getDataBuffer().retain());
-        endEntryId = headEntry.getEntryId();
-        headEntry.release();
-        currentEntry = entryBuf;
-        return read();
-
-    }
-
-    @Override
-    public void close() throws IOException {
-        blockHead.close();
-    }
-
-    public static int calculateBlockSize(int streamingBlockSize, int entryCount, int entrySize) {
-        int validDataSize = (entryCount * ENTRY_HEADER_SIZE
-                + entrySize
-                + StreamingDataBlockHeaderImpl.getDataStartOffset());
-        return Math.max(streamingBlockSize, validDataSize);
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.CompositeByteBuf;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.List;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.bookkeeper.mledger.Entry;
+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
+
+@Slf4j
+public class BufferedOffloadStream extends InputStream {
+    static final int[] BLOCK_END_PADDING = BlockAwareSegmentInputStreamImpl.BLOCK_END_PADDING;
+
+    private final long ledgerId;
+    private final long beginEntryId;
+
+    public BufferedOffloadStream(int blockSize, List<Entry> entries, long ledgerId, long beginEntryId) {
+        this.ledgerId = ledgerId;
+        this.beginEntryId = beginEntryId;
+        this.endEntryId = beginEntryId;
+        this.blockSize = blockSize;
+        this.entryBuffer = entries;
+        this.blockHead = StreamingDataBlockHeaderImpl.of(blockSize, ledgerId, beginEntryId)
+                .toStream();
+    }
+
+    public long getEndEntryId() {
+        return endEntryId;
+    }
+
+    private volatile long endEntryId;
+    static final int ENTRY_HEADER_SIZE = 4 /* entry size */ + 8 /* entry id */;
+    private final long blockSize;
+    private final List<Entry> entryBuffer;
+    private final InputStream blockHead;
+    int offset = 0;
+    static final int NOT_INITIALIZED = -1;
+    int validDataOffset = NOT_INITIALIZED;
+    CompositeByteBuf currentEntry;
+
+    public long getLedgerId() {
+        return ledgerId;
+    }
+
+    public long getBeginEntryId() {
+        return beginEntryId;
+    }
+
+    public long getBlockSize() {
+        return blockSize;
+    }
+
+    @Override
+    public int read() throws IOException {
+        if (blockHead.available() > 0) {
+            offset++;
+            return blockHead.read();
+        }
+        //if current exists, use current first
+        if (currentEntry != null) {
+            if (currentEntry.readableBytes() > 0) {
+                offset += 1;
+                return currentEntry.readUnsignedByte();
+            } else {
+                currentEntry.release();
+                currentEntry = null;
+            }
+        }
+
+        if (blockSize <= offset) {
+            return -1;
+        } else if (validDataOffset != NOT_INITIALIZED) {
+            return BLOCK_END_PADDING[(offset++ - validDataOffset) % BLOCK_END_PADDING.length];
+        }
+
+
+        if (entryBuffer.isEmpty()) {
+            validDataOffset = offset;
+            return read();
+        }
+
+        Entry headEntry = entryBuffer.remove(0);
+
+        //create new block when a ledger end
+        if (headEntry.getLedgerId() != this.ledgerId) {
+            throw new RuntimeException(
+                    String.format("there should not be multi ledger in a block %s %s", headEntry.getLedgerId(),
+                            this.ledgerId));
+        }
+
+        final int entryLength = headEntry.getLength();
+        final long entryId = headEntry.getEntryId();
+        CompositeByteBuf entryBuf = PulsarByteBufAllocator.DEFAULT.compositeBuffer(2);
+        ByteBuf entryHeaderBuf = PulsarByteBufAllocator.DEFAULT.buffer(ENTRY_HEADER_SIZE, ENTRY_HEADER_SIZE);
+        entryHeaderBuf.writeInt(entryLength).writeLong(entryId);
+        entryBuf.addComponents(true, entryHeaderBuf, headEntry.getDataBuffer().retain());
+        endEntryId = headEntry.getEntryId();
+        headEntry.release();
+        currentEntry = entryBuf;
+        return read();
+
+    }
+
+    @Override
+    public void close() throws IOException {
+        blockHead.close();
+    }
+
+    public static int calculateBlockSize(int streamingBlockSize, int entryCount, int entrySize) {
+        int validDataSize = (entryCount * ENTRY_HEADER_SIZE
+                + entrySize
+                + StreamingDataBlockHeaderImpl.getDataStartOffset());
+        return Math.max(streamingBlockSize, validDataSize);
+    }
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockHeaderImpl.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockHeaderImpl.java
index d7631d01dd..c1ea19a69a 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockHeaderImpl.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockHeaderImpl.java
@@ -1,129 +1,129 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import com.google.common.io.CountingInputStream;
-import io.netty.buffer.ByteBuf;
-import io.netty.buffer.ByteBufInputStream;
-import java.io.DataInputStream;
-import java.io.EOFException;
-import java.io.IOException;
-import java.io.InputStream;
-import org.apache.bookkeeper.mledger.offload.jcloud.DataBlockHeader;
-import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
-
-/**
- * The data block header in tiered storage for each data block.
- */
-public class DataBlockHeaderImpl implements DataBlockHeader {
-    // Magic Word for data block.
-    // It is a sequence of bytes used to identify the start of a block.
-    static final int MAGIC_WORD = 0xFBDBABCB;
-    // This is bigger than header size. Leaving some place for alignment and future enhancement.
-    // Payload use this as the start offset.
-    private static final int HEADER_MAX_SIZE = 128;
-    private static final int HEADER_BYTES_USED = 4 /* magic */
-                                               + 8 /* header len */
-                                               + 8 /* block len */
-                                               + 8 /* first entry id */;
-    private static final byte[] PADDING = new byte[HEADER_MAX_SIZE - HEADER_BYTES_USED];
-
-    public static DataBlockHeaderImpl of(int blockLength, long firstEntryId) {
-        return new DataBlockHeaderImpl(HEADER_MAX_SIZE, blockLength, firstEntryId);
-    }
-
-    // Construct DataBlockHeader from InputStream, which contains `HEADER_MAX_SIZE` bytes readable.
-    public static DataBlockHeader fromStream(InputStream stream) throws IOException {
-        CountingInputStream countingStream = new CountingInputStream(stream);
-        DataInputStream dis = new DataInputStream(countingStream);
-        int magic = dis.readInt();
-        if (magic != MAGIC_WORD) {
-            throw new IOException("Data block header magic word not match. read: " + magic
-                    + " expected: " + MAGIC_WORD);
-        }
-
-        long headerLen = dis.readLong();
-        long blockLen = dis.readLong();
-        long firstEntryId = dis.readLong();
-        long toSkip = headerLen - countingStream.getCount();
-        if (dis.skip(toSkip) != toSkip) {
-            throw new EOFException("Header was too small");
-        }
-
-        return new DataBlockHeaderImpl(headerLen, blockLen, firstEntryId);
-    }
-
-    private final long headerLength;
-    private final long blockLength;
-    private final long firstEntryId;
-
-    public static int getBlockMagicWord() {
-        return MAGIC_WORD;
-    }
-
-    public static int getDataStartOffset() {
-        return HEADER_MAX_SIZE;
-    }
-
-    @Override
-    public long getBlockLength() {
-        return this.blockLength;
-    }
-
-    @Override
-    public long getHeaderLength() {
-        return this.headerLength;
-    }
-
-    @Override
-    public long getFirstEntryId() {
-        return this.firstEntryId;
-    }
-
-    public DataBlockHeaderImpl(long headerLength, long blockLength, long firstEntryId) {
-        this.headerLength = headerLength;
-        this.blockLength = blockLength;
-        this.firstEntryId = firstEntryId;
-    }
-
-    /**
-     * Get the content of the data block header as InputStream.
-     * Read out in format:
-     *   [ magic_word -- int ][ block_len -- int ][ first_entry_id  -- long] [padding zeros]
-     */
-    @Override
-    public InputStream toStream() {
-        ByteBuf out = PulsarByteBufAllocator.DEFAULT.buffer(HEADER_MAX_SIZE, HEADER_MAX_SIZE);
-        out.writeInt(MAGIC_WORD)
-            .writeLong(headerLength)
-            .writeLong(blockLength)
-            .writeLong(firstEntryId)
-            .writeBytes(PADDING);
-
-        // true means the input stream will release the ByteBuf on close
-        return new ByteBufInputStream(out, true);
-    }
-
-    @Override
-    public String toString() {
-        return String.format("DataBlockHeader(len:%d,hlen:%d,firstEntry:%d)",
-                blockLength, headerLength, firstEntryId);
-    }
-}
-
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import com.google.common.io.CountingInputStream;
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.ByteBufInputStream;
+import java.io.DataInputStream;
+import java.io.EOFException;
+import java.io.IOException;
+import java.io.InputStream;
+import org.apache.bookkeeper.mledger.offload.jcloud.DataBlockHeader;
+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
+
+/**
+ * The data block header in tiered storage for each data block.
+ */
+public class DataBlockHeaderImpl implements DataBlockHeader {
+    // Magic Word for data block.
+    // It is a sequence of bytes used to identify the start of a block.
+    static final int MAGIC_WORD = 0xFBDBABCB;
+    // This is bigger than header size. Leaving some place for alignment and future enhancement.
+    // Payload use this as the start offset.
+    private static final int HEADER_MAX_SIZE = 128;
+    private static final int HEADER_BYTES_USED = 4 /* magic */
+                                               + 8 /* header len */
+                                               + 8 /* block len */
+                                               + 8 /* first entry id */;
+    private static final byte[] PADDING = new byte[HEADER_MAX_SIZE - HEADER_BYTES_USED];
+
+    public static DataBlockHeaderImpl of(int blockLength, long firstEntryId) {
+        return new DataBlockHeaderImpl(HEADER_MAX_SIZE, blockLength, firstEntryId);
+    }
+
+    // Construct DataBlockHeader from InputStream, which contains `HEADER_MAX_SIZE` bytes readable.
+    public static DataBlockHeader fromStream(InputStream stream) throws IOException {
+        CountingInputStream countingStream = new CountingInputStream(stream);
+        DataInputStream dis = new DataInputStream(countingStream);
+        int magic = dis.readInt();
+        if (magic != MAGIC_WORD) {
+            throw new IOException("Data block header magic word not match. read: " + magic
+                    + " expected: " + MAGIC_WORD);
+        }
+
+        long headerLen = dis.readLong();
+        long blockLen = dis.readLong();
+        long firstEntryId = dis.readLong();
+        long toSkip = headerLen - countingStream.getCount();
+        if (dis.skip(toSkip) != toSkip) {
+            throw new EOFException("Header was too small");
+        }
+
+        return new DataBlockHeaderImpl(headerLen, blockLen, firstEntryId);
+    }
+
+    private final long headerLength;
+    private final long blockLength;
+    private final long firstEntryId;
+
+    public static int getBlockMagicWord() {
+        return MAGIC_WORD;
+    }
+
+    public static int getDataStartOffset() {
+        return HEADER_MAX_SIZE;
+    }
+
+    @Override
+    public long getBlockLength() {
+        return this.blockLength;
+    }
+
+    @Override
+    public long getHeaderLength() {
+        return this.headerLength;
+    }
+
+    @Override
+    public long getFirstEntryId() {
+        return this.firstEntryId;
+    }
+
+    public DataBlockHeaderImpl(long headerLength, long blockLength, long firstEntryId) {
+        this.headerLength = headerLength;
+        this.blockLength = blockLength;
+        this.firstEntryId = firstEntryId;
+    }
+
+    /**
+     * Get the content of the data block header as InputStream.
+     * Read out in format:
+     *   [ magic_word -- int ][ block_len -- int ][ first_entry_id  -- long] [padding zeros]
+     */
+    @Override
+    public InputStream toStream() {
+        ByteBuf out = PulsarByteBufAllocator.DEFAULT.buffer(HEADER_MAX_SIZE, HEADER_MAX_SIZE);
+        out.writeInt(MAGIC_WORD)
+            .writeLong(headerLength)
+            .writeLong(blockLength)
+            .writeLong(firstEntryId)
+            .writeBytes(PADDING);
+
+        // true means the input stream will release the ByteBuf on close
+        return new ByteBufInputStream(out, true);
+    }
+
+    @Override
+    public String toString() {
+        return String.format("DataBlockHeader(len:%d,hlen:%d,firstEntry:%d)",
+                blockLength, headerLength, firstEntryId);
+    }
+}
+
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockUtils.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockUtils.java
index c5b656682d..a2da43c311 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockUtils.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockUtils.java
@@ -1,109 +1,109 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import com.google.common.collect.ImmutableMap;
-import java.io.IOException;
-import java.util.Map;
-import java.util.UUID;
-import org.jclouds.blobstore.domain.Blob;
-import org.jclouds.blobstore.domain.BlobBuilder;
-
-/**
- * Utility class for performing various Data Block functions including:
- *    - Calculating the data block offload key
- *    - Calculating the data block index key
- *    - Adding version metadata information to a Data Block
- *    - Validating the version metadata information of a Data Block.
- *  <p>
- *  Additional functions can be added in to future to tag Data Blocks with
- *  information such as the compression algorithm used to compress the contents,
- *  the md5 checksum of the content for validation, date published, etc.
- *  </p>
- */
-public class DataBlockUtils {
-
-    /**
-     * Version checking marker interface.
-     */
-    public interface VersionCheck {
-        void check(String key, Blob blob) throws IOException;
-    }
-
-    public static final String METADATA_FORMAT_VERSION_KEY = "S3ManagedLedgerOffloaderFormatVersion";
-    static final String CURRENT_VERSION = String.valueOf(1);
-
-    public static String dataBlockOffloadKey(long ledgerId, UUID uuid) {
-        return String.format("%s-ledger-%d", uuid.toString(), ledgerId);
-    }
-
-    public static String indexBlockOffloadKey(long ledgerId, UUID uuid) {
-        return String.format("%s-ledger-%d-index", uuid.toString(), ledgerId);
-    }
-
-    public static String indexBlockOffloadKey(UUID uuid) {
-        return String.format("%s-index", uuid.toString());
-    }
-
-    public static void addVersionInfo(BlobBuilder blobBuilder, Map<String, String> userMetadata) {
-        ImmutableMap.Builder<String, String> metadataBuilder = ImmutableMap.builder();
-        metadataBuilder.putAll(userMetadata);
-        metadataBuilder.put(METADATA_FORMAT_VERSION_KEY.toLowerCase(), CURRENT_VERSION);
-        blobBuilder.userMetadata(metadataBuilder.build());
-    }
-
-    public static final VersionCheck VERSION_CHECK = (key, blob) -> {
-        // NOTE all metadata in jclouds comes out as lowercase, in an effort to normalize the providers
-        String version = blob.getMetadata().getUserMetadata().get(METADATA_FORMAT_VERSION_KEY.toLowerCase());
-        if (version == null || !version.equals(CURRENT_VERSION)) {
-            throw new IOException(String.format("Invalid object version %s for %s, expect %s",
-                version, key, CURRENT_VERSION));
-        }
-    };
-
-    public static Long parseLedgerId(String name) {
-        if (name == null || name.isEmpty()) {
-            return null;
-        }
-        if (name.endsWith("-index")) {
-            name = name.substring(0, name.length() - "-index".length());
-        }
-        int pos = name.indexOf("-ledger-");
-        if (pos < 0) {
-            return null;
-        }
-        try {
-            return Long.parseLong(name.substring(pos + 8));
-        } catch (NumberFormatException err) {
-            return null;
-        }
-    }
-
-    public static String parseContextUuid(String name, Long ledgerId) {
-        if (ledgerId == null || name == null) {
-            return null;
-        }
-        int pos = name.indexOf("-ledger-" + ledgerId);
-        if (pos <= 0) {
-            return null;
-        }
-        return name.substring(0, pos);
-    }
-
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import com.google.common.collect.ImmutableMap;
+import java.io.IOException;
+import java.util.Map;
+import java.util.UUID;
+import org.jclouds.blobstore.domain.Blob;
+import org.jclouds.blobstore.domain.BlobBuilder;
+
+/**
+ * Utility class for performing various Data Block functions including:
+ *    - Calculating the data block offload key
+ *    - Calculating the data block index key
+ *    - Adding version metadata information to a Data Block
+ *    - Validating the version metadata information of a Data Block.
+ *  <p>
+ *  Additional functions can be added in to future to tag Data Blocks with
+ *  information such as the compression algorithm used to compress the contents,
+ *  the md5 checksum of the content for validation, date published, etc.
+ *  </p>
+ */
+public class DataBlockUtils {
+
+    /**
+     * Version checking marker interface.
+     */
+    public interface VersionCheck {
+        void check(String key, Blob blob) throws IOException;
+    }
+
+    public static final String METADATA_FORMAT_VERSION_KEY = "S3ManagedLedgerOffloaderFormatVersion";
+    static final String CURRENT_VERSION = String.valueOf(1);
+
+    public static String dataBlockOffloadKey(long ledgerId, UUID uuid) {
+        return String.format("%s-ledger-%d", uuid.toString(), ledgerId);
+    }
+
+    public static String indexBlockOffloadKey(long ledgerId, UUID uuid) {
+        return String.format("%s-ledger-%d-index", uuid.toString(), ledgerId);
+    }
+
+    public static String indexBlockOffloadKey(UUID uuid) {
+        return String.format("%s-index", uuid.toString());
+    }
+
+    public static void addVersionInfo(BlobBuilder blobBuilder, Map<String, String> userMetadata) {
+        ImmutableMap.Builder<String, String> metadataBuilder = ImmutableMap.builder();
+        metadataBuilder.putAll(userMetadata);
+        metadataBuilder.put(METADATA_FORMAT_VERSION_KEY.toLowerCase(), CURRENT_VERSION);
+        blobBuilder.userMetadata(metadataBuilder.build());
+    }
+
+    public static final VersionCheck VERSION_CHECK = (key, blob) -> {
+        // NOTE all metadata in jclouds comes out as lowercase, in an effort to normalize the providers
+        String version = blob.getMetadata().getUserMetadata().get(METADATA_FORMAT_VERSION_KEY.toLowerCase());
+        if (version == null || !version.equals(CURRENT_VERSION)) {
+            throw new IOException(String.format("Invalid object version %s for %s, expect %s",
+                version, key, CURRENT_VERSION));
+        }
+    };
+
+    public static Long parseLedgerId(String name) {
+        if (name == null || name.isEmpty()) {
+            return null;
+        }
+        if (name.endsWith("-index")) {
+            name = name.substring(0, name.length() - "-index".length());
+        }
+        int pos = name.indexOf("-ledger-");
+        if (pos < 0) {
+            return null;
+        }
+        try {
+            return Long.parseLong(name.substring(pos + 8));
+        } catch (NumberFormatException err) {
+            return null;
+        }
+    }
+
+    public static String parseContextUuid(String name, Long ledgerId) {
+        if (ledgerId == null || name == null) {
+            return null;
+        }
+        int pos = name.indexOf("-ledger-" + ledgerId);
+        if (pos <= 0) {
+            return null;
+        }
+        return name.substring(0, pos);
+    }
+
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexBlockImpl.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexBlockImpl.java
index ef7c84e917..6887fed080 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexBlockImpl.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexBlockImpl.java
@@ -1,362 +1,362 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static com.google.common.base.Preconditions.checkState;
-import static org.apache.bookkeeper.mledger.offload.OffloadUtils.buildLedgerMetadataFormat;
-import com.google.common.collect.Maps;
-import io.netty.buffer.ByteBuf;
-import io.netty.buffer.ByteBufInputStream;
-import io.netty.util.Recycler;
-import io.netty.util.Recycler.Handle;
-import java.io.DataInputStream;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-import java.util.NavigableMap;
-import java.util.TreeMap;
-import org.apache.bookkeeper.client.api.DigestType;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;
-import org.apache.bookkeeper.net.BookieId;
-import org.apache.bookkeeper.proto.DataFormats;
-import org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat;
-import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class OffloadIndexBlockImpl implements OffloadIndexBlock {
-    private static final Logger log = LoggerFactory.getLogger(OffloadIndexBlockImpl.class);
-
-    private static final int INDEX_MAGIC_WORD = 0xDE47DE47;
-
-    private LedgerMetadata segmentMetadata;
-    private long dataObjectLength;
-    private long dataHeaderLength;
-    private TreeMap<Long, OffloadIndexEntryImpl> indexEntries;
-
-    private final Handle<OffloadIndexBlockImpl> recyclerHandle;
-
-    private static final Recycler<OffloadIndexBlockImpl> RECYCLER = new Recycler<OffloadIndexBlockImpl>() {
-        @Override
-        protected OffloadIndexBlockImpl newObject(Recycler.Handle<OffloadIndexBlockImpl> handle) {
-            return new OffloadIndexBlockImpl(handle);
-        }
-    };
-
-    private OffloadIndexBlockImpl(Handle<OffloadIndexBlockImpl> recyclerHandle) {
-        this.recyclerHandle = recyclerHandle;
-    }
-
-    public static OffloadIndexBlockImpl get(LedgerMetadata metadata, long dataObjectLength,
-                                            long dataHeaderLength,
-                                            List<OffloadIndexEntryImpl> entries) {
-        OffloadIndexBlockImpl block = RECYCLER.get();
-        block.indexEntries = Maps.newTreeMap();
-        entries.forEach(entry -> block.indexEntries.putIfAbsent(entry.getEntryId(), entry));
-        checkState(entries.size() == block.indexEntries.size());
-        block.segmentMetadata = metadata;
-        block.dataObjectLength = dataObjectLength;
-        block.dataHeaderLength = dataHeaderLength;
-        return block;
-    }
-
-    public static OffloadIndexBlockImpl get(int magic, DataInputStream stream) throws IOException {
-        if (magic != INDEX_MAGIC_WORD) {
-            throw new IOException(String.format("Invalid MagicWord. read: 0x%x  expected: 0x%x",
-                    magic, INDEX_MAGIC_WORD));
-        }
-        OffloadIndexBlockImpl block = RECYCLER.get();
-        block.indexEntries = Maps.newTreeMap();
-        block.fromStream(stream);
-        return block;
-    }
-
-    public void recycle() {
-        dataObjectLength = -1;
-        dataHeaderLength = -1;
-        segmentMetadata = null;
-        indexEntries.clear();
-        indexEntries = null;
-        if (recyclerHandle != null) {
-            recyclerHandle.recycle(this);
-        }
-    }
-
-    @Override
-    public OffloadIndexEntry getIndexEntryForEntry(long messageEntryId) throws IOException {
-        if (messageEntryId > segmentMetadata.getLastEntryId()) {
-            log.warn("Try to get entry: {}, which beyond lastEntryId {}, return null",
-                messageEntryId, segmentMetadata.getLastEntryId());
-            throw new IndexOutOfBoundsException("Entry index: " + messageEntryId
-                + " beyond lastEntryId: " + segmentMetadata.getLastEntryId());
-        }
-        // find the greatest mapping Id whose entryId <= messageEntryId
-        return this.indexEntries.floorEntry(messageEntryId).getValue();
-    }
-
-    @Override
-    public int getEntryCount() {
-        return this.indexEntries.size();
-    }
-
-    @Override
-    public LedgerMetadata getLedgerMetadata() {
-        return this.segmentMetadata;
-    }
-
-    @Override
-    public long getDataObjectLength() {
-        return this.dataObjectLength;
-    }
-
-    @Override
-    public long getDataBlockHeaderLength() {
-        return this.dataHeaderLength;
-    }
-
-    /**
-     * Get the content of the index block as InputStream.
-     * Read out in format:
-     *   | index_magic_header | index_block_len | data_object_len | data_header_len |
-     *   | index_entry_count  | segment_metadata_len | segment metadata | index entries... |
-     */
-    @Override
-    public OffloadIndexBlock.IndexInputStream toStream() throws IOException {
-        int indexEntryCount = this.indexEntries.size();
-        byte[] ledgerMetadataByte = buildLedgerMetadataFormat(this.segmentMetadata);
-        int segmentMetadataLength = ledgerMetadataByte.length;
-
-        int indexBlockLength = 4 /* magic header */
-            + 4 /* index block length */
-            + 8 /* data object length */
-            + 8 /* data header length */
-            + 4 /* index entry count */
-            + 4 /* segment metadata length */
-            + segmentMetadataLength
-            + indexEntryCount * (8 + 4 + 8); /* messageEntryId + blockPartId + blockOffset */
-
-        ByteBuf out = PulsarByteBufAllocator.DEFAULT.buffer(indexBlockLength, indexBlockLength);
-
-        out.writeInt(INDEX_MAGIC_WORD)
-            .writeInt(indexBlockLength)
-            .writeLong(dataObjectLength)
-            .writeLong(dataHeaderLength)
-            .writeInt(indexEntryCount)
-            .writeInt(segmentMetadataLength);
-        // write metadata
-        out.writeBytes(ledgerMetadataByte);
-
-        // write entries
-        this.indexEntries.entrySet().forEach(entry ->
-                out.writeLong(entry.getValue().getEntryId())
-                .writeInt(entry.getValue().getPartId())
-                .writeLong(entry.getValue().getOffset()));
-
-        return new OffloadIndexBlock.IndexInputStream(new ByteBufInputStream(out, true), indexBlockLength);
-    }
-
-    private static class InternalLedgerMetadata implements LedgerMetadata {
-
-        private int ensembleSize;
-        private int writeQuorumSize;
-        private int ackQuorumSize;
-        private long lastEntryId;
-        private long length;
-        private DataFormats.LedgerMetadataFormat.DigestType digestType;
-        private long ctime;
-        private State state;
-        private Map<String, byte[]> customMetadata = Maps.newHashMap();
-        private TreeMap<Long, ArrayList<BookieId>> ensembles =
-                new TreeMap<>();
-
-        InternalLedgerMetadata(LedgerMetadataFormat ledgerMetadataFormat) {
-            this.ensembleSize = ledgerMetadataFormat.getEnsembleSize();
-            this.writeQuorumSize = ledgerMetadataFormat.getQuorumSize();
-            this.ackQuorumSize = ledgerMetadataFormat.getAckQuorumSize();
-            this.lastEntryId = ledgerMetadataFormat.getLastEntryId();
-            this.length = ledgerMetadataFormat.getLength();
-            this.digestType = ledgerMetadataFormat.getDigestType();
-            this.ctime = ledgerMetadataFormat.getCtime();
-            this.state = org.apache.bookkeeper.client.api.LedgerMetadata.State.valueOf(
-                    ledgerMetadataFormat.getState().toString());
-
-            if (ledgerMetadataFormat.getCustomMetadataCount() > 0) {
-                ledgerMetadataFormat.getCustomMetadataList().forEach(
-                        entry -> this.customMetadata.put(entry.getKey(), entry.getValue().toByteArray()));
-            }
-
-            ledgerMetadataFormat.getSegmentList().forEach(segment -> {
-                ArrayList<BookieId> addressArrayList = new ArrayList<>();
-                segment.getEnsembleMemberList().forEach(address -> {
-                    try {
-                        addressArrayList.add(BookieId.parse(address));
-                    } catch (IllegalArgumentException e) {
-                        log.error("Exception when create BookieSocketAddress. ", e);
-                    }
-                });
-                this.ensembles.put(segment.getFirstEntryId(), addressArrayList);
-            });
-        }
-
-        @Override
-        public long getLedgerId() {
-            throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public int getEnsembleSize() {
-            return this.ensembleSize;
-        }
-
-        @Override
-        public int getWriteQuorumSize() {
-            return this.writeQuorumSize;
-        }
-
-        @Override
-        public int getAckQuorumSize() {
-            return this.ackQuorumSize;
-        }
-
-        @Override
-        public long getLastEntryId() {
-            return this.lastEntryId;
-        }
-
-        @Override
-        public long getLength() {
-            return this.length;
-        }
-
-        @Override
-        public DigestType getDigestType() {
-            switch (this.digestType) {
-                case HMAC:
-                    return DigestType.MAC;
-                case CRC32:
-                    return DigestType.CRC32;
-                case CRC32C:
-                    return DigestType.CRC32C;
-                case DUMMY:
-                    return DigestType.DUMMY;
-                default:
-                    throw new IllegalArgumentException("Unable to convert digest type " + digestType);
-            }
-        }
-
-        @Override
-        public long getCtime() {
-            return this.ctime;
-        }
-
-        @Override
-        public boolean isClosed() {
-            return this.state == State.CLOSED;
-        }
-
-        @Override
-        public Map<String, byte[]> getCustomMetadata() {
-            return this.customMetadata;
-        }
-
-        @Override
-        public List<BookieId> getEnsembleAt(long entryId) {
-            return ensembles.get(ensembles.headMap(entryId + 1).lastKey());
-        }
-
-        @Override
-        public NavigableMap<Long, ? extends List<BookieId>> getAllEnsembles() {
-            return this.ensembles;
-        }
-
-        @Override
-        public long getCToken() {
-            // TODO Auto-generated method stub
-            return 0;
-        }
-
-        @Override
-        public int getMetadataFormatVersion() {
-            // TODO Auto-generated method stub
-            return 0;
-        }
-
-        @Override
-        public byte[] getPassword() {
-            // TODO Auto-generated method stub
-            return null;
-        }
-
-        @Override
-        public State getState() {
-            return this.state;
-        }
-
-        @Override
-        public boolean hasPassword() {
-            // TODO Auto-generated method stub
-            return false;
-        }
-
-        @Override
-        public String toSafeString() {
-            // TODO Auto-generated method stub
-            return null;
-        }
-    }
-
-    private static LedgerMetadata parseLedgerMetadata(byte[] bytes) throws IOException {
-        LedgerMetadataFormat.Builder builder = LedgerMetadataFormat.newBuilder();
-        builder.mergeFrom(bytes);
-        return new InternalLedgerMetadata(builder.build());
-    }
-
-    private OffloadIndexBlock fromStream(DataInputStream dis) throws IOException {
-        dis.readInt(); // no used index block length
-        this.dataObjectLength = dis.readLong();
-        this.dataHeaderLength = dis.readLong();
-        int indexEntryCount = dis.readInt();
-        int segmentMetadataLength = dis.readInt();
-
-        byte[] metadataBytes = new byte[segmentMetadataLength];
-        dis.readFully(metadataBytes);
-        this.segmentMetadata = parseLedgerMetadata(metadataBytes);
-
-        for (int i = 0; i < indexEntryCount; i++) {
-            long entryId = dis.readLong();
-            this.indexEntries.putIfAbsent(entryId, OffloadIndexEntryImpl.of(entryId, dis.readInt(),
-                    dis.readLong(), dataHeaderLength));
-        }
-        return this;
-    }
-
-    public static int getIndexMagicWord() {
-        return INDEX_MAGIC_WORD;
-    }
-
-    @Override
-    public void close() {
-        recycle();
-    }
-
-}
-
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static com.google.common.base.Preconditions.checkState;
+import static org.apache.bookkeeper.mledger.offload.OffloadUtils.buildLedgerMetadataFormat;
+import com.google.common.collect.Maps;
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.ByteBufInputStream;
+import io.netty.util.Recycler;
+import io.netty.util.Recycler.Handle;
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.TreeMap;
+import org.apache.bookkeeper.client.api.DigestType;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;
+import org.apache.bookkeeper.net.BookieId;
+import org.apache.bookkeeper.proto.DataFormats;
+import org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat;
+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class OffloadIndexBlockImpl implements OffloadIndexBlock {
+    private static final Logger log = LoggerFactory.getLogger(OffloadIndexBlockImpl.class);
+
+    private static final int INDEX_MAGIC_WORD = 0xDE47DE47;
+
+    private LedgerMetadata segmentMetadata;
+    private long dataObjectLength;
+    private long dataHeaderLength;
+    private TreeMap<Long, OffloadIndexEntryImpl> indexEntries;
+
+    private final Handle<OffloadIndexBlockImpl> recyclerHandle;
+
+    private static final Recycler<OffloadIndexBlockImpl> RECYCLER = new Recycler<OffloadIndexBlockImpl>() {
+        @Override
+        protected OffloadIndexBlockImpl newObject(Recycler.Handle<OffloadIndexBlockImpl> handle) {
+            return new OffloadIndexBlockImpl(handle);
+        }
+    };
+
+    private OffloadIndexBlockImpl(Handle<OffloadIndexBlockImpl> recyclerHandle) {
+        this.recyclerHandle = recyclerHandle;
+    }
+
+    public static OffloadIndexBlockImpl get(LedgerMetadata metadata, long dataObjectLength,
+                                            long dataHeaderLength,
+                                            List<OffloadIndexEntryImpl> entries) {
+        OffloadIndexBlockImpl block = RECYCLER.get();
+        block.indexEntries = Maps.newTreeMap();
+        entries.forEach(entry -> block.indexEntries.putIfAbsent(entry.getEntryId(), entry));
+        checkState(entries.size() == block.indexEntries.size());
+        block.segmentMetadata = metadata;
+        block.dataObjectLength = dataObjectLength;
+        block.dataHeaderLength = dataHeaderLength;
+        return block;
+    }
+
+    public static OffloadIndexBlockImpl get(int magic, DataInputStream stream) throws IOException {
+        if (magic != INDEX_MAGIC_WORD) {
+            throw new IOException(String.format("Invalid MagicWord. read: 0x%x  expected: 0x%x",
+                    magic, INDEX_MAGIC_WORD));
+        }
+        OffloadIndexBlockImpl block = RECYCLER.get();
+        block.indexEntries = Maps.newTreeMap();
+        block.fromStream(stream);
+        return block;
+    }
+
+    public void recycle() {
+        dataObjectLength = -1;
+        dataHeaderLength = -1;
+        segmentMetadata = null;
+        indexEntries.clear();
+        indexEntries = null;
+        if (recyclerHandle != null) {
+            recyclerHandle.recycle(this);
+        }
+    }
+
+    @Override
+    public OffloadIndexEntry getIndexEntryForEntry(long messageEntryId) throws IOException {
+        if (messageEntryId > segmentMetadata.getLastEntryId()) {
+            log.warn("Try to get entry: {}, which beyond lastEntryId {}, return null",
+                messageEntryId, segmentMetadata.getLastEntryId());
+            throw new IndexOutOfBoundsException("Entry index: " + messageEntryId
+                + " beyond lastEntryId: " + segmentMetadata.getLastEntryId());
+        }
+        // find the greatest mapping Id whose entryId <= messageEntryId
+        return this.indexEntries.floorEntry(messageEntryId).getValue();
+    }
+
+    @Override
+    public int getEntryCount() {
+        return this.indexEntries.size();
+    }
+
+    @Override
+    public LedgerMetadata getLedgerMetadata() {
+        return this.segmentMetadata;
+    }
+
+    @Override
+    public long getDataObjectLength() {
+        return this.dataObjectLength;
+    }
+
+    @Override
+    public long getDataBlockHeaderLength() {
+        return this.dataHeaderLength;
+    }
+
+    /**
+     * Get the content of the index block as InputStream.
+     * Read out in format:
+     *   | index_magic_header | index_block_len | data_object_len | data_header_len |
+     *   | index_entry_count  | segment_metadata_len | segment metadata | index entries... |
+     */
+    @Override
+    public OffloadIndexBlock.IndexInputStream toStream() throws IOException {
+        int indexEntryCount = this.indexEntries.size();
+        byte[] ledgerMetadataByte = buildLedgerMetadataFormat(this.segmentMetadata);
+        int segmentMetadataLength = ledgerMetadataByte.length;
+
+        int indexBlockLength = 4 /* magic header */
+            + 4 /* index block length */
+            + 8 /* data object length */
+            + 8 /* data header length */
+            + 4 /* index entry count */
+            + 4 /* segment metadata length */
+            + segmentMetadataLength
+            + indexEntryCount * (8 + 4 + 8); /* messageEntryId + blockPartId + blockOffset */
+
+        ByteBuf out = PulsarByteBufAllocator.DEFAULT.buffer(indexBlockLength, indexBlockLength);
+
+        out.writeInt(INDEX_MAGIC_WORD)
+            .writeInt(indexBlockLength)
+            .writeLong(dataObjectLength)
+            .writeLong(dataHeaderLength)
+            .writeInt(indexEntryCount)
+            .writeInt(segmentMetadataLength);
+        // write metadata
+        out.writeBytes(ledgerMetadataByte);
+
+        // write entries
+        this.indexEntries.entrySet().forEach(entry ->
+                out.writeLong(entry.getValue().getEntryId())
+                .writeInt(entry.getValue().getPartId())
+                .writeLong(entry.getValue().getOffset()));
+
+        return new OffloadIndexBlock.IndexInputStream(new ByteBufInputStream(out, true), indexBlockLength);
+    }
+
+    private static class InternalLedgerMetadata implements LedgerMetadata {
+
+        private int ensembleSize;
+        private int writeQuorumSize;
+        private int ackQuorumSize;
+        private long lastEntryId;
+        private long length;
+        private DataFormats.LedgerMetadataFormat.DigestType digestType;
+        private long ctime;
+        private State state;
+        private Map<String, byte[]> customMetadata = Maps.newHashMap();
+        private TreeMap<Long, ArrayList<BookieId>> ensembles =
+                new TreeMap<>();
+
+        InternalLedgerMetadata(LedgerMetadataFormat ledgerMetadataFormat) {
+            this.ensembleSize = ledgerMetadataFormat.getEnsembleSize();
+            this.writeQuorumSize = ledgerMetadataFormat.getQuorumSize();
+            this.ackQuorumSize = ledgerMetadataFormat.getAckQuorumSize();
+            this.lastEntryId = ledgerMetadataFormat.getLastEntryId();
+            this.length = ledgerMetadataFormat.getLength();
+            this.digestType = ledgerMetadataFormat.getDigestType();
+            this.ctime = ledgerMetadataFormat.getCtime();
+            this.state = org.apache.bookkeeper.client.api.LedgerMetadata.State.valueOf(
+                    ledgerMetadataFormat.getState().toString());
+
+            if (ledgerMetadataFormat.getCustomMetadataCount() > 0) {
+                ledgerMetadataFormat.getCustomMetadataList().forEach(
+                        entry -> this.customMetadata.put(entry.getKey(), entry.getValue().toByteArray()));
+            }
+
+            ledgerMetadataFormat.getSegmentList().forEach(segment -> {
+                ArrayList<BookieId> addressArrayList = new ArrayList<>();
+                segment.getEnsembleMemberList().forEach(address -> {
+                    try {
+                        addressArrayList.add(BookieId.parse(address));
+                    } catch (IllegalArgumentException e) {
+                        log.error("Exception when create BookieSocketAddress. ", e);
+                    }
+                });
+                this.ensembles.put(segment.getFirstEntryId(), addressArrayList);
+            });
+        }
+
+        @Override
+        public long getLedgerId() {
+            throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public int getEnsembleSize() {
+            return this.ensembleSize;
+        }
+
+        @Override
+        public int getWriteQuorumSize() {
+            return this.writeQuorumSize;
+        }
+
+        @Override
+        public int getAckQuorumSize() {
+            return this.ackQuorumSize;
+        }
+
+        @Override
+        public long getLastEntryId() {
+            return this.lastEntryId;
+        }
+
+        @Override
+        public long getLength() {
+            return this.length;
+        }
+
+        @Override
+        public DigestType getDigestType() {
+            switch (this.digestType) {
+                case HMAC:
+                    return DigestType.MAC;
+                case CRC32:
+                    return DigestType.CRC32;
+                case CRC32C:
+                    return DigestType.CRC32C;
+                case DUMMY:
+                    return DigestType.DUMMY;
+                default:
+                    throw new IllegalArgumentException("Unable to convert digest type " + digestType);
+            }
+        }
+
+        @Override
+        public long getCtime() {
+            return this.ctime;
+        }
+
+        @Override
+        public boolean isClosed() {
+            return this.state == State.CLOSED;
+        }
+
+        @Override
+        public Map<String, byte[]> getCustomMetadata() {
+            return this.customMetadata;
+        }
+
+        @Override
+        public List<BookieId> getEnsembleAt(long entryId) {
+            return ensembles.get(ensembles.headMap(entryId + 1).lastKey());
+        }
+
+        @Override
+        public NavigableMap<Long, ? extends List<BookieId>> getAllEnsembles() {
+            return this.ensembles;
+        }
+
+        @Override
+        public long getCToken() {
+            // TODO Auto-generated method stub
+            return 0;
+        }
+
+        @Override
+        public int getMetadataFormatVersion() {
+            // TODO Auto-generated method stub
+            return 0;
+        }
+
+        @Override
+        public byte[] getPassword() {
+            // TODO Auto-generated method stub
+            return null;
+        }
+
+        @Override
+        public State getState() {
+            return this.state;
+        }
+
+        @Override
+        public boolean hasPassword() {
+            // TODO Auto-generated method stub
+            return false;
+        }
+
+        @Override
+        public String toSafeString() {
+            // TODO Auto-generated method stub
+            return null;
+        }
+    }
+
+    private static LedgerMetadata parseLedgerMetadata(byte[] bytes) throws IOException {
+        LedgerMetadataFormat.Builder builder = LedgerMetadataFormat.newBuilder();
+        builder.mergeFrom(bytes);
+        return new InternalLedgerMetadata(builder.build());
+    }
+
+    private OffloadIndexBlock fromStream(DataInputStream dis) throws IOException {
+        dis.readInt(); // no used index block length
+        this.dataObjectLength = dis.readLong();
+        this.dataHeaderLength = dis.readLong();
+        int indexEntryCount = dis.readInt();
+        int segmentMetadataLength = dis.readInt();
+
+        byte[] metadataBytes = new byte[segmentMetadataLength];
+        dis.readFully(metadataBytes);
+        this.segmentMetadata = parseLedgerMetadata(metadataBytes);
+
+        for (int i = 0; i < indexEntryCount; i++) {
+            long entryId = dis.readLong();
+            this.indexEntries.putIfAbsent(entryId, OffloadIndexEntryImpl.of(entryId, dis.readInt(),
+                    dis.readLong(), dataHeaderLength));
+        }
+        return this;
+    }
+
+    public static int getIndexMagicWord() {
+        return INDEX_MAGIC_WORD;
+    }
+
+    @Override
+    public void close() {
+        recycle();
+    }
+
+}
+
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexBlockV2BuilderImpl.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexBlockV2BuilderImpl.java
index d5761fa0e4..fb14b39c05 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexBlockV2BuilderImpl.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexBlockV2BuilderImpl.java
@@ -1,150 +1,150 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static com.google.common.base.Preconditions.checkState;
-import com.google.common.collect.Lists;
-import java.io.DataInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.HashMap;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.SortedMap;
-import java.util.TreeMap;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockBuilder;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2Builder;
-import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;
-
-/**
- * Interface for builder of index block used for offload a ledger to long term storage.
- */
-public class OffloadIndexBlockV2BuilderImpl implements OffloadIndexBlockBuilder, OffloadIndexBlockV2Builder {
-
-    private final Map<Long, LedgerInfo> ledgerMetadataMap;
-    private LedgerMetadata ledgerMetadata;
-    private long dataObjectLength;
-    private long dataHeaderLength;
-    private List<OffloadIndexEntryImpl> entries;
-    private int lastBlockSize;
-    private int lastStreamingBlockSize;
-    private long streamingOffset = 0;
-    private final SortedMap<Long, List<OffloadIndexEntryImpl>> entryMap = new TreeMap<>();
-
-
-    public OffloadIndexBlockV2BuilderImpl() {
-        this.entries = Lists.newArrayList();
-        this.ledgerMetadataMap = new HashMap<>();
-    }
-
-    @Override
-    public OffloadIndexBlockV2BuilderImpl withDataObjectLength(long dataObjectLength) {
-        this.dataObjectLength = dataObjectLength;
-        return this;
-    }
-
-    @Override
-    public OffloadIndexBlockV2BuilderImpl withDataBlockHeaderLength(long dataHeaderLength) {
-        this.dataHeaderLength = dataHeaderLength;
-        return this;
-    }
-
-    @Override
-    public OffloadIndexBlockV2BuilderImpl withLedgerMetadata(LedgerMetadata metadata) {
-        this.ledgerMetadata = metadata;
-        return this;
-    }
-
-    @Override
-    public OffloadIndexBlockV2BuilderImpl addLedgerMeta(Long ledgerId, LedgerInfo metadata) {
-        this.ledgerMetadataMap.put(ledgerId, metadata);
-        return this;
-    }
-
-    @Override
-    public OffloadIndexBlockBuilder addBlock(long firstEntryId, int partId, int blockSize) {
-        checkState(dataHeaderLength > 0);
-
-        // we should added one by one.
-        long offset;
-        if (firstEntryId == 0) {
-            checkState(entries.size() == 0);
-            offset = 0;
-        } else {
-            checkState(entries.size() > 0);
-            offset = entries.get(entries.size() - 1).getOffset() + lastBlockSize;
-        }
-        lastBlockSize = blockSize;
-
-        this.entries.add(OffloadIndexEntryImpl.of(firstEntryId, partId, offset, dataHeaderLength));
-        return this;
-    }
-
-    @Override
-    public OffloadIndexBlockV2Builder addBlock(long ledgerId, long firstEntryId, int partId, int blockSize) {
-        checkState(dataHeaderLength > 0);
-
-        streamingOffset = streamingOffset + lastStreamingBlockSize;
-        lastStreamingBlockSize = blockSize;
-
-        final List<OffloadIndexEntryImpl> list = entryMap.getOrDefault(ledgerId, new LinkedList<>());
-        list.add(OffloadIndexEntryImpl.of(firstEntryId, partId, streamingOffset, dataHeaderLength));
-        entryMap.put(ledgerId, list);
-        return this;
-    }
-
-    @Override
-    public OffloadIndexBlockV2 fromStream(InputStream is) throws IOException {
-        final DataInputStream dataInputStream = new DataInputStream(is);
-        final int magic = dataInputStream.readInt();
-        if (magic == OffloadIndexBlockImpl.getIndexMagicWord()) {
-            return OffloadIndexBlockImpl.get(magic, dataInputStream);
-        } else if (magic == OffloadIndexBlockV2Impl.getIndexMagicWord()) {
-            return OffloadIndexBlockV2Impl.get(magic, dataInputStream);
-        } else {
-            throw new IOException(String.format("Invalid MagicWord. read: 0x%x  expected: 0x%x or 0x%x",
-                    magic, OffloadIndexBlockImpl.getIndexMagicWord(),
-                    OffloadIndexBlockV2Impl.getIndexMagicWord()));
-        }
-    }
-
-    @Override
-    public OffloadIndexBlock build() {
-        checkState(ledgerMetadata != null);
-        checkState(!entries.isEmpty());
-        checkState(dataObjectLength > 0);
-        checkState(dataHeaderLength > 0);
-        return OffloadIndexBlockImpl.get(ledgerMetadata, dataObjectLength, dataHeaderLength, entries);
-    }
-
-    @Override
-    public OffloadIndexBlockV2 buildV2() {
-        checkState(!ledgerMetadataMap.isEmpty());
-        checkState(true);
-        checkState(!entryMap.isEmpty());
-        checkState(dataObjectLength > 0);
-        checkState(dataHeaderLength > 0);
-        return OffloadIndexBlockV2Impl.get(ledgerMetadataMap, dataObjectLength, dataHeaderLength, entryMap);
-    }
-
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static com.google.common.base.Preconditions.checkState;
+import com.google.common.collect.Lists;
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockBuilder;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2Builder;
+import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;
+
+/**
+ * Interface for builder of index block used for offload a ledger to long term storage.
+ */
+public class OffloadIndexBlockV2BuilderImpl implements OffloadIndexBlockBuilder, OffloadIndexBlockV2Builder {
+
+    private final Map<Long, LedgerInfo> ledgerMetadataMap;
+    private LedgerMetadata ledgerMetadata;
+    private long dataObjectLength;
+    private long dataHeaderLength;
+    private List<OffloadIndexEntryImpl> entries;
+    private int lastBlockSize;
+    private int lastStreamingBlockSize;
+    private long streamingOffset = 0;
+    private final SortedMap<Long, List<OffloadIndexEntryImpl>> entryMap = new TreeMap<>();
+
+
+    public OffloadIndexBlockV2BuilderImpl() {
+        this.entries = Lists.newArrayList();
+        this.ledgerMetadataMap = new HashMap<>();
+    }
+
+    @Override
+    public OffloadIndexBlockV2BuilderImpl withDataObjectLength(long dataObjectLength) {
+        this.dataObjectLength = dataObjectLength;
+        return this;
+    }
+
+    @Override
+    public OffloadIndexBlockV2BuilderImpl withDataBlockHeaderLength(long dataHeaderLength) {
+        this.dataHeaderLength = dataHeaderLength;
+        return this;
+    }
+
+    @Override
+    public OffloadIndexBlockV2BuilderImpl withLedgerMetadata(LedgerMetadata metadata) {
+        this.ledgerMetadata = metadata;
+        return this;
+    }
+
+    @Override
+    public OffloadIndexBlockV2BuilderImpl addLedgerMeta(Long ledgerId, LedgerInfo metadata) {
+        this.ledgerMetadataMap.put(ledgerId, metadata);
+        return this;
+    }
+
+    @Override
+    public OffloadIndexBlockBuilder addBlock(long firstEntryId, int partId, int blockSize) {
+        checkState(dataHeaderLength > 0);
+
+        // we should added one by one.
+        long offset;
+        if (firstEntryId == 0) {
+            checkState(entries.size() == 0);
+            offset = 0;
+        } else {
+            checkState(entries.size() > 0);
+            offset = entries.get(entries.size() - 1).getOffset() + lastBlockSize;
+        }
+        lastBlockSize = blockSize;
+
+        this.entries.add(OffloadIndexEntryImpl.of(firstEntryId, partId, offset, dataHeaderLength));
+        return this;
+    }
+
+    @Override
+    public OffloadIndexBlockV2Builder addBlock(long ledgerId, long firstEntryId, int partId, int blockSize) {
+        checkState(dataHeaderLength > 0);
+
+        streamingOffset = streamingOffset + lastStreamingBlockSize;
+        lastStreamingBlockSize = blockSize;
+
+        final List<OffloadIndexEntryImpl> list = entryMap.getOrDefault(ledgerId, new LinkedList<>());
+        list.add(OffloadIndexEntryImpl.of(firstEntryId, partId, streamingOffset, dataHeaderLength));
+        entryMap.put(ledgerId, list);
+        return this;
+    }
+
+    @Override
+    public OffloadIndexBlockV2 fromStream(InputStream is) throws IOException {
+        final DataInputStream dataInputStream = new DataInputStream(is);
+        final int magic = dataInputStream.readInt();
+        if (magic == OffloadIndexBlockImpl.getIndexMagicWord()) {
+            return OffloadIndexBlockImpl.get(magic, dataInputStream);
+        } else if (magic == OffloadIndexBlockV2Impl.getIndexMagicWord()) {
+            return OffloadIndexBlockV2Impl.get(magic, dataInputStream);
+        } else {
+            throw new IOException(String.format("Invalid MagicWord. read: 0x%x  expected: 0x%x or 0x%x",
+                    magic, OffloadIndexBlockImpl.getIndexMagicWord(),
+                    OffloadIndexBlockV2Impl.getIndexMagicWord()));
+        }
+    }
+
+    @Override
+    public OffloadIndexBlock build() {
+        checkState(ledgerMetadata != null);
+        checkState(!entries.isEmpty());
+        checkState(dataObjectLength > 0);
+        checkState(dataHeaderLength > 0);
+        return OffloadIndexBlockImpl.get(ledgerMetadata, dataObjectLength, dataHeaderLength, entries);
+    }
+
+    @Override
+    public OffloadIndexBlockV2 buildV2() {
+        checkState(!ledgerMetadataMap.isEmpty());
+        checkState(true);
+        checkState(!entryMap.isEmpty());
+        checkState(dataObjectLength > 0);
+        checkState(dataHeaderLength > 0);
+        return OffloadIndexBlockV2Impl.get(ledgerMetadataMap, dataObjectLength, dataHeaderLength, entryMap);
+    }
+
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexBlockV2Impl.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexBlockV2Impl.java
index 93ae53abce..5a7b74b800 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexBlockV2Impl.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexBlockV2Impl.java
@@ -1,379 +1,379 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.collect.Maps;
-import io.netty.buffer.ByteBuf;
-import io.netty.buffer.ByteBufInputStream;
-import io.netty.util.Recycler;
-import io.netty.util.Recycler.Handle;
-import java.io.DataInputStream;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.NavigableMap;
-import java.util.Objects;
-import java.util.TreeMap;
-import org.apache.bookkeeper.client.api.DigestType;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock.IndexInputStream;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;
-import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;
-import org.apache.bookkeeper.net.BookieId;
-import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class OffloadIndexBlockV2Impl implements OffloadIndexBlockV2 {
-    private static final Logger log = LoggerFactory.getLogger(OffloadIndexBlockImpl.class);
-
-    private static final int INDEX_MAGIC_WORD = 0x3D1FB0BC;
-
-    private Map<Long, LedgerInfo> segmentMetadata;
-    private final Map<Long, LedgerMetadata> compatibleMetadata = Maps.newTreeMap();
-    private long dataObjectLength;
-    private long dataHeaderLength;
-    //    private TreeMap<Long, OffloadIndexEntryImpl> indexEntries;
-    private Map<Long, TreeMap<Long, OffloadIndexEntryImpl>> indexEntries;
-
-
-    private final Handle<OffloadIndexBlockV2Impl> recyclerHandle;
-
-    private static final Recycler<OffloadIndexBlockV2Impl> RECYCLER = new Recycler<OffloadIndexBlockV2Impl>() {
-        @Override
-        protected OffloadIndexBlockV2Impl newObject(Handle<OffloadIndexBlockV2Impl> handle) {
-            return new OffloadIndexBlockV2Impl(handle);
-        }
-    };
-
-    private OffloadIndexBlockV2Impl(Handle<OffloadIndexBlockV2Impl> recyclerHandle) {
-        this.recyclerHandle = recyclerHandle;
-    }
-
-    public static OffloadIndexBlockV2Impl get(Map<Long, LedgerInfo> metadata, long dataObjectLength,
-                                              long dataHeaderLength,
-                                              Map<Long, List<OffloadIndexEntryImpl>> entries) {
-        OffloadIndexBlockV2Impl block = RECYCLER.get();
-        block.indexEntries = new HashMap<>();
-        entries.forEach((ledgerId, list) -> {
-            final TreeMap<Long, OffloadIndexEntryImpl> inLedger = block.indexEntries
-                    .getOrDefault(ledgerId, new TreeMap<>());
-            list.forEach(indexEntry -> {
-                inLedger.put(indexEntry.getEntryId(), indexEntry);
-            });
-            block.indexEntries.put(ledgerId, inLedger);
-        });
-
-        block.segmentMetadata = metadata;
-        block.dataObjectLength = dataObjectLength;
-        block.dataHeaderLength = dataHeaderLength;
-        return block;
-    }
-
-    public static OffloadIndexBlockV2Impl get(int magic, DataInputStream stream) throws IOException {
-        OffloadIndexBlockV2Impl block = RECYCLER.get();
-        block.indexEntries = Maps.newTreeMap();
-        block.segmentMetadata = Maps.newTreeMap();
-        if (magic != INDEX_MAGIC_WORD) {
-            throw new IOException(String.format("Invalid MagicWord. read: 0x%x  expected: 0x%x",
-                    magic, INDEX_MAGIC_WORD));
-        }
-        block.fromStream(stream);
-        return block;
-    }
-
-    public void recycle() {
-        dataObjectLength = -1;
-        dataHeaderLength = -1;
-        segmentMetadata = null;
-        indexEntries.clear();
-        indexEntries = null;
-        if (recyclerHandle != null) {
-            recyclerHandle.recycle(this);
-        }
-    }
-
-    @Override
-    public OffloadIndexEntry getIndexEntryForEntry(long ledgerId, long messageEntryId) throws IOException {
-        if (messageEntryId > getLedgerMetadata(ledgerId).getLastEntryId()) {
-            log.warn("Try to get entry: {}, which beyond lastEntryId {}, return null",
-                    messageEntryId, getLedgerMetadata(ledgerId).getLastEntryId());
-            throw new IndexOutOfBoundsException("Entry index: " + messageEntryId
-                    + " beyond lastEntryId: " + getLedgerMetadata(ledgerId).getLastEntryId());
-        }
-        // find the greatest mapping Id whose entryId <= messageEntryId
-        return this.indexEntries.get(ledgerId).floorEntry(messageEntryId).getValue();
-    }
-
-    public long getStartEntryId(long ledgerId) {
-        return this.indexEntries.get(ledgerId).firstEntry().getValue().getEntryId();
-    }
-
-    @Override
-    public int getEntryCount() {
-        int ans = 0;
-        for (TreeMap<Long, OffloadIndexEntryImpl> v : this.indexEntries.values()) {
-            ans += v.size();
-        }
-
-        return ans;
-    }
-
-    @Override
-    public LedgerMetadata getLedgerMetadata(long ledgerId) {
-        if (compatibleMetadata.containsKey(ledgerId)) {
-            return compatibleMetadata.get(ledgerId);
-        } else if (segmentMetadata.containsKey(ledgerId)) {
-            final CompatibleMetadata result = new CompatibleMetadata(segmentMetadata.get(ledgerId));
-            compatibleMetadata.put(ledgerId, result);
-            return result;
-        } else {
-            return null;
-        }
-    }
-
-    @Override
-    public long getDataObjectLength() {
-        return this.dataObjectLength;
-    }
-
-    @Override
-    public long getDataBlockHeaderLength() {
-        return this.dataHeaderLength;
-    }
-
-    /**
-     * Get the content of the index block as InputStream.
-     * Read out in format:
-     *   | index_magic_header | index_block_len | data_object_len | data_header_len |
-     *   | index_entry_count  | segment_metadata_len | segment metadata | index entries... |
-     */
-    @Override
-    public IndexInputStream toStream() throws IOException {
-
-        int indexBlockLength = 4 /* magic header */
-                + 4 /* index block length */
-                + 8 /* data object length */
-                + 8; /* data header length */
-
-        Map<Long, byte[]> metaBytesMap = new HashMap<>();
-        for (Map.Entry<Long, TreeMap<Long, OffloadIndexEntryImpl>> e : this.indexEntries.entrySet()) {
-            Long ledgerId = e.getKey();
-            TreeMap<Long, OffloadIndexEntryImpl> ledgerIndexEntries = e.getValue();
-            int indexEntryCount = ledgerIndexEntries.size();
-            byte[] ledgerMetadataByte = this.segmentMetadata.get(ledgerId).toByteArray();
-            int segmentMetadataLength = ledgerMetadataByte.length;
-            indexBlockLength += 8 /* ledger id length */
-                    + 4 /* index entry count */
-                    + 4 /* segment metadata length */
-                    + segmentMetadataLength
-                    + indexEntryCount * (8 + 4 + 8);
-            metaBytesMap.put(ledgerId, ledgerMetadataByte);
-        }
-
-        ByteBuf out = PulsarByteBufAllocator.DEFAULT.buffer(indexBlockLength, indexBlockLength);
-
-        out.writeInt(INDEX_MAGIC_WORD)
-                .writeInt(indexBlockLength)
-                .writeLong(dataObjectLength)
-                .writeLong(dataHeaderLength);
-
-        for (Map.Entry<Long, TreeMap<Long, OffloadIndexEntryImpl>> e : this.indexEntries.entrySet()) {
-            Long ledgerId = e.getKey();
-            TreeMap<Long, OffloadIndexEntryImpl> ledgerIndexEntries = e.getValue();
-            int indexEntryCount = ledgerIndexEntries.size();
-            byte[] ledgerMetadataByte = metaBytesMap.get(ledgerId);
-            out.writeLong(ledgerId)
-                    .writeInt(indexEntryCount)
-                    .writeInt(ledgerMetadataByte.length)
-                    .writeBytes(ledgerMetadataByte);
-            ledgerIndexEntries.values().forEach(idxEntry -> {
-                out.writeLong(idxEntry.getEntryId())
-                        .writeInt(idxEntry.getPartId())
-                        .writeLong(idxEntry.getOffset());
-            });
-        }
-
-        return new IndexInputStream(new ByteBufInputStream(out, true), indexBlockLength);
-    }
-
-    private static LedgerInfo parseLedgerInfo(byte[] bytes) throws IOException {
-        return LedgerInfo.newBuilder().mergeFrom(bytes).build();
-    }
-
-    private OffloadIndexBlockV2 fromStream(DataInputStream dis) throws IOException {
-
-        dis.readInt(); // no used index block length
-        this.dataObjectLength = dis.readLong();
-        this.dataHeaderLength = dis.readLong();
-        while (dis.available() > 0) {
-            long ledgerId = dis.readLong();
-            int indexEntryCount = dis.readInt();
-            int segmentMetadataLength = dis.readInt();
-
-            byte[] metadataBytes = new byte[segmentMetadataLength];
-
-            if (segmentMetadataLength != dis.read(metadataBytes)) {
-                log.error("Read ledgerMetadata from bytes failed");
-                throw new IOException("Read ledgerMetadata from bytes failed");
-            }
-            final LedgerInfo ledgerInfo = parseLedgerInfo(metadataBytes);
-            this.segmentMetadata.put(ledgerId, ledgerInfo);
-            final TreeMap<Long, OffloadIndexEntryImpl> indexEntries = new TreeMap<>();
-
-            for (int i = 0; i < indexEntryCount; i++) {
-                long entryId = dis.readLong();
-                indexEntries.putIfAbsent(entryId, OffloadIndexEntryImpl.of(entryId, dis.readInt(),
-                        dis.readLong(), dataHeaderLength));
-            }
-            this.indexEntries.put(ledgerId, indexEntries);
-        }
-
-        return this;
-    }
-
-    public static int getIndexMagicWord() {
-        return INDEX_MAGIC_WORD;
-    }
-
-    @Override
-    public void close() {
-        recycle();
-    }
-
-    @VisibleForTesting
-    static class CompatibleMetadata implements LedgerMetadata {
-        LedgerInfo ledgerInfo;
-
-        public CompatibleMetadata(LedgerInfo ledgerInfo) {
-            this.ledgerInfo = ledgerInfo;
-        }
-
-        @Override
-        public long getLedgerId() {
-            return ledgerInfo.getLedgerId();
-        }
-
-        @Override
-        public int getEnsembleSize() {
-            return 0;
-        }
-
-        @Override
-        public int getWriteQuorumSize() {
-            return 0;
-        }
-
-        @Override
-        public int getAckQuorumSize() {
-            return 0;
-        }
-
-        @Override
-        public long getLastEntryId() {
-            return ledgerInfo.getEntries() - 1;
-        }
-
-        @Override
-        public long getLength() {
-            return ledgerInfo.getSize();
-        }
-
-        @Override
-        public boolean hasPassword() {
-            return false;
-        }
-
-        @Override
-        public byte[] getPassword() {
-            return new byte[0];
-        }
-
-        @Override
-        public DigestType getDigestType() {
-            return null;
-        }
-
-        @Override
-        public long getCtime() {
-            return 0;
-        }
-
-        @Override
-        public boolean isClosed() {
-            return true;
-        }
-
-        @Override
-        public Map<String, byte[]> getCustomMetadata() {
-            return null;
-        }
-
-        @Override
-        public List<BookieId> getEnsembleAt(long entryId) {
-            return null;
-        }
-
-        @Override
-        public NavigableMap<Long, ? extends List<BookieId>> getAllEnsembles() {
-            return null;
-        }
-
-        @Override
-        public State getState() {
-            return null;
-        }
-
-        @Override
-        public String toSafeString() {
-            return null;
-        }
-
-        @Override
-        public int getMetadataFormatVersion() {
-            return 0;
-        }
-
-        @Override
-        public long getCToken() {
-            return 0;
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) {
-                return true;
-            }
-            if (o == null || getClass() != o.getClass()) {
-                return false;
-            }
-            CompatibleMetadata that = (CompatibleMetadata) o;
-            return ledgerInfo.equals(that.ledgerInfo);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(ledgerInfo);
-        }
-    }
-}
-
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.Maps;
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.ByteBufInputStream;
+import io.netty.util.Recycler;
+import io.netty.util.Recycler.Handle;
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.Objects;
+import java.util.TreeMap;
+import org.apache.bookkeeper.client.api.DigestType;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock.IndexInputStream;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;
+import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;
+import org.apache.bookkeeper.net.BookieId;
+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class OffloadIndexBlockV2Impl implements OffloadIndexBlockV2 {
+    private static final Logger log = LoggerFactory.getLogger(OffloadIndexBlockImpl.class);
+
+    private static final int INDEX_MAGIC_WORD = 0x3D1FB0BC;
+
+    private Map<Long, LedgerInfo> segmentMetadata;
+    private final Map<Long, LedgerMetadata> compatibleMetadata = Maps.newTreeMap();
+    private long dataObjectLength;
+    private long dataHeaderLength;
+    //    private TreeMap<Long, OffloadIndexEntryImpl> indexEntries;
+    private Map<Long, TreeMap<Long, OffloadIndexEntryImpl>> indexEntries;
+
+
+    private final Handle<OffloadIndexBlockV2Impl> recyclerHandle;
+
+    private static final Recycler<OffloadIndexBlockV2Impl> RECYCLER = new Recycler<OffloadIndexBlockV2Impl>() {
+        @Override
+        protected OffloadIndexBlockV2Impl newObject(Handle<OffloadIndexBlockV2Impl> handle) {
+            return new OffloadIndexBlockV2Impl(handle);
+        }
+    };
+
+    private OffloadIndexBlockV2Impl(Handle<OffloadIndexBlockV2Impl> recyclerHandle) {
+        this.recyclerHandle = recyclerHandle;
+    }
+
+    public static OffloadIndexBlockV2Impl get(Map<Long, LedgerInfo> metadata, long dataObjectLength,
+                                              long dataHeaderLength,
+                                              Map<Long, List<OffloadIndexEntryImpl>> entries) {
+        OffloadIndexBlockV2Impl block = RECYCLER.get();
+        block.indexEntries = new HashMap<>();
+        entries.forEach((ledgerId, list) -> {
+            final TreeMap<Long, OffloadIndexEntryImpl> inLedger = block.indexEntries
+                    .getOrDefault(ledgerId, new TreeMap<>());
+            list.forEach(indexEntry -> {
+                inLedger.put(indexEntry.getEntryId(), indexEntry);
+            });
+            block.indexEntries.put(ledgerId, inLedger);
+        });
+
+        block.segmentMetadata = metadata;
+        block.dataObjectLength = dataObjectLength;
+        block.dataHeaderLength = dataHeaderLength;
+        return block;
+    }
+
+    public static OffloadIndexBlockV2Impl get(int magic, DataInputStream stream) throws IOException {
+        OffloadIndexBlockV2Impl block = RECYCLER.get();
+        block.indexEntries = Maps.newTreeMap();
+        block.segmentMetadata = Maps.newTreeMap();
+        if (magic != INDEX_MAGIC_WORD) {
+            throw new IOException(String.format("Invalid MagicWord. read: 0x%x  expected: 0x%x",
+                    magic, INDEX_MAGIC_WORD));
+        }
+        block.fromStream(stream);
+        return block;
+    }
+
+    public void recycle() {
+        dataObjectLength = -1;
+        dataHeaderLength = -1;
+        segmentMetadata = null;
+        indexEntries.clear();
+        indexEntries = null;
+        if (recyclerHandle != null) {
+            recyclerHandle.recycle(this);
+        }
+    }
+
+    @Override
+    public OffloadIndexEntry getIndexEntryForEntry(long ledgerId, long messageEntryId) throws IOException {
+        if (messageEntryId > getLedgerMetadata(ledgerId).getLastEntryId()) {
+            log.warn("Try to get entry: {}, which beyond lastEntryId {}, return null",
+                    messageEntryId, getLedgerMetadata(ledgerId).getLastEntryId());
+            throw new IndexOutOfBoundsException("Entry index: " + messageEntryId
+                    + " beyond lastEntryId: " + getLedgerMetadata(ledgerId).getLastEntryId());
+        }
+        // find the greatest mapping Id whose entryId <= messageEntryId
+        return this.indexEntries.get(ledgerId).floorEntry(messageEntryId).getValue();
+    }
+
+    public long getStartEntryId(long ledgerId) {
+        return this.indexEntries.get(ledgerId).firstEntry().getValue().getEntryId();
+    }
+
+    @Override
+    public int getEntryCount() {
+        int ans = 0;
+        for (TreeMap<Long, OffloadIndexEntryImpl> v : this.indexEntries.values()) {
+            ans += v.size();
+        }
+
+        return ans;
+    }
+
+    @Override
+    public LedgerMetadata getLedgerMetadata(long ledgerId) {
+        if (compatibleMetadata.containsKey(ledgerId)) {
+            return compatibleMetadata.get(ledgerId);
+        } else if (segmentMetadata.containsKey(ledgerId)) {
+            final CompatibleMetadata result = new CompatibleMetadata(segmentMetadata.get(ledgerId));
+            compatibleMetadata.put(ledgerId, result);
+            return result;
+        } else {
+            return null;
+        }
+    }
+
+    @Override
+    public long getDataObjectLength() {
+        return this.dataObjectLength;
+    }
+
+    @Override
+    public long getDataBlockHeaderLength() {
+        return this.dataHeaderLength;
+    }
+
+    /**
+     * Get the content of the index block as InputStream.
+     * Read out in format:
+     *   | index_magic_header | index_block_len | data_object_len | data_header_len |
+     *   | index_entry_count  | segment_metadata_len | segment metadata | index entries... |
+     */
+    @Override
+    public IndexInputStream toStream() throws IOException {
+
+        int indexBlockLength = 4 /* magic header */
+                + 4 /* index block length */
+                + 8 /* data object length */
+                + 8; /* data header length */
+
+        Map<Long, byte[]> metaBytesMap = new HashMap<>();
+        for (Map.Entry<Long, TreeMap<Long, OffloadIndexEntryImpl>> e : this.indexEntries.entrySet()) {
+            Long ledgerId = e.getKey();
+            TreeMap<Long, OffloadIndexEntryImpl> ledgerIndexEntries = e.getValue();
+            int indexEntryCount = ledgerIndexEntries.size();
+            byte[] ledgerMetadataByte = this.segmentMetadata.get(ledgerId).toByteArray();
+            int segmentMetadataLength = ledgerMetadataByte.length;
+            indexBlockLength += 8 /* ledger id length */
+                    + 4 /* index entry count */
+                    + 4 /* segment metadata length */
+                    + segmentMetadataLength
+                    + indexEntryCount * (8 + 4 + 8);
+            metaBytesMap.put(ledgerId, ledgerMetadataByte);
+        }
+
+        ByteBuf out = PulsarByteBufAllocator.DEFAULT.buffer(indexBlockLength, indexBlockLength);
+
+        out.writeInt(INDEX_MAGIC_WORD)
+                .writeInt(indexBlockLength)
+                .writeLong(dataObjectLength)
+                .writeLong(dataHeaderLength);
+
+        for (Map.Entry<Long, TreeMap<Long, OffloadIndexEntryImpl>> e : this.indexEntries.entrySet()) {
+            Long ledgerId = e.getKey();
+            TreeMap<Long, OffloadIndexEntryImpl> ledgerIndexEntries = e.getValue();
+            int indexEntryCount = ledgerIndexEntries.size();
+            byte[] ledgerMetadataByte = metaBytesMap.get(ledgerId);
+            out.writeLong(ledgerId)
+                    .writeInt(indexEntryCount)
+                    .writeInt(ledgerMetadataByte.length)
+                    .writeBytes(ledgerMetadataByte);
+            ledgerIndexEntries.values().forEach(idxEntry -> {
+                out.writeLong(idxEntry.getEntryId())
+                        .writeInt(idxEntry.getPartId())
+                        .writeLong(idxEntry.getOffset());
+            });
+        }
+
+        return new IndexInputStream(new ByteBufInputStream(out, true), indexBlockLength);
+    }
+
+    private static LedgerInfo parseLedgerInfo(byte[] bytes) throws IOException {
+        return LedgerInfo.newBuilder().mergeFrom(bytes).build();
+    }
+
+    private OffloadIndexBlockV2 fromStream(DataInputStream dis) throws IOException {
+
+        dis.readInt(); // no used index block length
+        this.dataObjectLength = dis.readLong();
+        this.dataHeaderLength = dis.readLong();
+        while (dis.available() > 0) {
+            long ledgerId = dis.readLong();
+            int indexEntryCount = dis.readInt();
+            int segmentMetadataLength = dis.readInt();
+
+            byte[] metadataBytes = new byte[segmentMetadataLength];
+
+            if (segmentMetadataLength != dis.read(metadataBytes)) {
+                log.error("Read ledgerMetadata from bytes failed");
+                throw new IOException("Read ledgerMetadata from bytes failed");
+            }
+            final LedgerInfo ledgerInfo = parseLedgerInfo(metadataBytes);
+            this.segmentMetadata.put(ledgerId, ledgerInfo);
+            final TreeMap<Long, OffloadIndexEntryImpl> indexEntries = new TreeMap<>();
+
+            for (int i = 0; i < indexEntryCount; i++) {
+                long entryId = dis.readLong();
+                indexEntries.putIfAbsent(entryId, OffloadIndexEntryImpl.of(entryId, dis.readInt(),
+                        dis.readLong(), dataHeaderLength));
+            }
+            this.indexEntries.put(ledgerId, indexEntries);
+        }
+
+        return this;
+    }
+
+    public static int getIndexMagicWord() {
+        return INDEX_MAGIC_WORD;
+    }
+
+    @Override
+    public void close() {
+        recycle();
+    }
+
+    @VisibleForTesting
+    static class CompatibleMetadata implements LedgerMetadata {
+        LedgerInfo ledgerInfo;
+
+        public CompatibleMetadata(LedgerInfo ledgerInfo) {
+            this.ledgerInfo = ledgerInfo;
+        }
+
+        @Override
+        public long getLedgerId() {
+            return ledgerInfo.getLedgerId();
+        }
+
+        @Override
+        public int getEnsembleSize() {
+            return 0;
+        }
+
+        @Override
+        public int getWriteQuorumSize() {
+            return 0;
+        }
+
+        @Override
+        public int getAckQuorumSize() {
+            return 0;
+        }
+
+        @Override
+        public long getLastEntryId() {
+            return ledgerInfo.getEntries() - 1;
+        }
+
+        @Override
+        public long getLength() {
+            return ledgerInfo.getSize();
+        }
+
+        @Override
+        public boolean hasPassword() {
+            return false;
+        }
+
+        @Override
+        public byte[] getPassword() {
+            return new byte[0];
+        }
+
+        @Override
+        public DigestType getDigestType() {
+            return null;
+        }
+
+        @Override
+        public long getCtime() {
+            return 0;
+        }
+
+        @Override
+        public boolean isClosed() {
+            return true;
+        }
+
+        @Override
+        public Map<String, byte[]> getCustomMetadata() {
+            return null;
+        }
+
+        @Override
+        public List<BookieId> getEnsembleAt(long entryId) {
+            return null;
+        }
+
+        @Override
+        public NavigableMap<Long, ? extends List<BookieId>> getAllEnsembles() {
+            return null;
+        }
+
+        @Override
+        public State getState() {
+            return null;
+        }
+
+        @Override
+        public String toSafeString() {
+            return null;
+        }
+
+        @Override
+        public int getMetadataFormatVersion() {
+            return 0;
+        }
+
+        @Override
+        public long getCToken() {
+            return 0;
+        }
+
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) {
+                return true;
+            }
+            if (o == null || getClass() != o.getClass()) {
+                return false;
+            }
+            CompatibleMetadata that = (CompatibleMetadata) o;
+            return ledgerInfo.equals(that.ledgerInfo);
+        }
+
+        @Override
+        public int hashCode() {
+            return Objects.hash(ledgerInfo);
+        }
+    }
+}
+
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexEntryImpl.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexEntryImpl.java
index 2faffa7e25..16f7b0be41 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexEntryImpl.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexEntryImpl.java
@@ -1,84 +1,84 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import java.util.Objects;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;
-
-/**
- * The Index Entry in OffloadIndexBlock.
- */
-public class OffloadIndexEntryImpl implements OffloadIndexEntry {
-    public static OffloadIndexEntryImpl of(long entryId, int partId, long offset, long blockHeaderSize) {
-        return new OffloadIndexEntryImpl(entryId, partId, offset, blockHeaderSize);
-    }
-
-    private final long entryId;
-    private final int partId;
-    private final long offset;
-    private final long blockHeaderSize;
-
-    @Override
-    public long getEntryId() {
-        return entryId;
-    }
-
-    @Override
-    public int getPartId() {
-        return partId;
-    }
-
-    @Override
-    public long getOffset() {
-        return offset;
-    }
-
-    @Override
-    public long getDataOffset() {
-        return offset + blockHeaderSize;
-    }
-
-    private OffloadIndexEntryImpl(long entryId, int partId, long offset, long blockHeaderSize) {
-        this.entryId = entryId;
-        this.partId = partId;
-        this.offset = offset;
-        this.blockHeaderSize = blockHeaderSize;
-    }
-
-    @Override
-    public String toString() {
-        return String.format("[eid:%d, part:%d, offset:%d, doffset:%d]",
-                entryId, partId, offset, getDataOffset());
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        if (!(o instanceof OffloadIndexEntryImpl that)) {
-            return false;
-        }
-        return entryId == that.entryId && partId == that.partId && offset == that.offset
-                && blockHeaderSize == that.blockHeaderSize;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(entryId, partId, offset, blockHeaderSize);
-    }
-}
-
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import java.util.Objects;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;
+
+/**
+ * The Index Entry in OffloadIndexBlock.
+ */
+public class OffloadIndexEntryImpl implements OffloadIndexEntry {
+    public static OffloadIndexEntryImpl of(long entryId, int partId, long offset, long blockHeaderSize) {
+        return new OffloadIndexEntryImpl(entryId, partId, offset, blockHeaderSize);
+    }
+
+    private final long entryId;
+    private final int partId;
+    private final long offset;
+    private final long blockHeaderSize;
+
+    @Override
+    public long getEntryId() {
+        return entryId;
+    }
+
+    @Override
+    public int getPartId() {
+        return partId;
+    }
+
+    @Override
+    public long getOffset() {
+        return offset;
+    }
+
+    @Override
+    public long getDataOffset() {
+        return offset + blockHeaderSize;
+    }
+
+    private OffloadIndexEntryImpl(long entryId, int partId, long offset, long blockHeaderSize) {
+        this.entryId = entryId;
+        this.partId = partId;
+        this.offset = offset;
+        this.blockHeaderSize = blockHeaderSize;
+    }
+
+    @Override
+    public String toString() {
+        return String.format("[eid:%d, part:%d, offset:%d, doffset:%d]",
+                entryId, partId, offset, getDataOffset());
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (!(o instanceof OffloadIndexEntryImpl that)) {
+            return false;
+        }
+        return entryId == that.entryId && partId == that.partId && offset == that.offset
+                && blockHeaderSize == that.blockHeaderSize;
+    }
+
+    @Override
+    public int hashCode() {
+        return Objects.hash(entryId, partId, offset, blockHeaderSize);
+    }
+}
+
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffsetsCache.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffsetsCache.java
index 6651b199e4..278f6b3209 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffsetsCache.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffsetsCache.java
@@ -1,85 +1,85 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import com.google.common.cache.Cache;
-import com.google.common.cache.CacheBuilder;
-import com.google.common.util.concurrent.ThreadFactoryBuilder;
-import java.util.concurrent.Executors;
-import java.util.concurrent.ScheduledExecutorService;
-import java.util.concurrent.TimeUnit;
-
-public class OffsetsCache implements AutoCloseable {
-    private static final int CACHE_TTL_SECONDS =
-            Integer.getInteger("pulsar.jclouds.readhandleimpl.offsetsscache.ttl.seconds", 5 * 60);
-    // limit the cache size to avoid OOM
-    // 1 million entries consumes about 60MB of heap space
-    private static final int CACHE_MAX_SIZE =
-            Integer.getInteger("pulsar.jclouds.readhandleimpl.offsetsscache.max.size", 1_000_000);
-    private final ScheduledExecutorService cacheEvictionExecutor;
-
-    record Key(long ledgerId, long entryId) {
-
-    }
-
-    private final Cache<OffsetsCache.Key, Long> entryOffsetsCache;
-
-    public OffsetsCache() {
-        if (CACHE_MAX_SIZE > 0) {
-            entryOffsetsCache = CacheBuilder
-                    .newBuilder()
-                    .expireAfterAccess(CACHE_TTL_SECONDS, TimeUnit.SECONDS)
-                    .maximumSize(CACHE_MAX_SIZE)
-                    .build();
-            cacheEvictionExecutor =
-                    Executors.newSingleThreadScheduledExecutor(
-                            new ThreadFactoryBuilder().setNameFormat("jcloud-offsets-cache-eviction").build());
-            int period = Math.max(CACHE_TTL_SECONDS / 2, 1);
-            cacheEvictionExecutor.scheduleAtFixedRate(() -> {
-                entryOffsetsCache.cleanUp();
-            }, period, period, TimeUnit.SECONDS);
-        } else {
-            cacheEvictionExecutor = null;
-            entryOffsetsCache = null;
-        }
-    }
-
-    public void put(long ledgerId, long entryId, long currentPosition) {
-        if (entryOffsetsCache != null) {
-            entryOffsetsCache.put(new Key(ledgerId, entryId), currentPosition);
-        }
-    }
-
-    public Long getIfPresent(long ledgerId, long entryId) {
-        return entryOffsetsCache != null ? entryOffsetsCache.getIfPresent(new Key(ledgerId, entryId)) : null;
-    }
-
-    public void clear() {
-        if (entryOffsetsCache != null) {
-            entryOffsetsCache.invalidateAll();
-        }
-    }
-
-    @Override
-    public void close() {
-        if (cacheEvictionExecutor != null) {
-            cacheEvictionExecutor.shutdownNow();
-        }
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import com.google.common.cache.Cache;
+import com.google.common.cache.CacheBuilder;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import java.util.concurrent.Executors;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.TimeUnit;
+
+public class OffsetsCache implements AutoCloseable {
+    private static final int CACHE_TTL_SECONDS =
+            Integer.getInteger("pulsar.jclouds.readhandleimpl.offsetsscache.ttl.seconds", 5 * 60);
+    // limit the cache size to avoid OOM
+    // 1 million entries consumes about 60MB of heap space
+    private static final int CACHE_MAX_SIZE =
+            Integer.getInteger("pulsar.jclouds.readhandleimpl.offsetsscache.max.size", 1_000_000);
+    private final ScheduledExecutorService cacheEvictionExecutor;
+
+    record Key(long ledgerId, long entryId) {
+
+    }
+
+    private final Cache<OffsetsCache.Key, Long> entryOffsetsCache;
+
+    public OffsetsCache() {
+        if (CACHE_MAX_SIZE > 0) {
+            entryOffsetsCache = CacheBuilder
+                    .newBuilder()
+                    .expireAfterAccess(CACHE_TTL_SECONDS, TimeUnit.SECONDS)
+                    .maximumSize(CACHE_MAX_SIZE)
+                    .build();
+            cacheEvictionExecutor =
+                    Executors.newSingleThreadScheduledExecutor(
+                            new ThreadFactoryBuilder().setNameFormat("jcloud-offsets-cache-eviction").build());
+            int period = Math.max(CACHE_TTL_SECONDS / 2, 1);
+            cacheEvictionExecutor.scheduleAtFixedRate(() -> {
+                entryOffsetsCache.cleanUp();
+            }, period, period, TimeUnit.SECONDS);
+        } else {
+            cacheEvictionExecutor = null;
+            entryOffsetsCache = null;
+        }
+    }
+
+    public void put(long ledgerId, long entryId, long currentPosition) {
+        if (entryOffsetsCache != null) {
+            entryOffsetsCache.put(new Key(ledgerId, entryId), currentPosition);
+        }
+    }
+
+    public Long getIfPresent(long ledgerId, long entryId) {
+        return entryOffsetsCache != null ? entryOffsetsCache.getIfPresent(new Key(ledgerId, entryId)) : null;
+    }
+
+    public void clear() {
+        if (entryOffsetsCache != null) {
+            entryOffsetsCache.invalidateAll();
+        }
+    }
+
+    @Override
+    public void close() {
+        if (cacheEvictionExecutor != null) {
+            cacheEvictionExecutor.shutdownNow();
+        }
+    }
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/StreamingDataBlockHeaderImpl.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/StreamingDataBlockHeaderImpl.java
index 3ce1e1fecf..cb8d2f52cf 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/StreamingDataBlockHeaderImpl.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/StreamingDataBlockHeaderImpl.java
@@ -1,139 +1,139 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import com.google.common.io.CountingInputStream;
-import io.netty.buffer.ByteBuf;
-import io.netty.buffer.ByteBufInputStream;
-import java.io.DataInputStream;
-import java.io.EOFException;
-import java.io.IOException;
-import java.io.InputStream;
-import org.apache.bookkeeper.mledger.offload.jcloud.DataBlockHeader;
-import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
-
-/**
- * The data block header in tiered storage for each data block.
- */
-public class StreamingDataBlockHeaderImpl implements DataBlockHeader {
-    // Magic Word for streaming data block.
-    // It is a sequence of bytes used to identify the start of a block.
-    static final int MAGIC_WORD = 0x26A66D32;
-    // This is bigger than header size. Leaving some place for alignment and future enhancement.
-    // Payload use this as the start offset.
-    public static final int HEADER_MAX_SIZE = 128;
-    private static final int HEADER_BYTES_USED = 4 /* magic */
-            + 8 /* header len */
-            + 8 /* block len */
-            + 8 /* first entry id */
-            + 8 /* ledger id */;
-    private static final byte[] PADDING = new byte[HEADER_MAX_SIZE - HEADER_BYTES_USED];
-
-    public long getLedgerId() {
-        return ledgerId;
-    }
-
-    private final long ledgerId;
-
-    public static StreamingDataBlockHeaderImpl of(int blockLength, long ledgerId, long firstEntryId) {
-        return new StreamingDataBlockHeaderImpl(HEADER_MAX_SIZE, blockLength, ledgerId, firstEntryId);
-    }
-
-    private final long headerLength;
-    private final long blockLength;
-    private final long firstEntryId;
-
-    public static int getBlockMagicWord() {
-        return MAGIC_WORD;
-    }
-
-    public static int getDataStartOffset() {
-        return HEADER_MAX_SIZE;
-    }
-
-    @Override
-    public long getBlockLength() {
-        return this.blockLength;
-    }
-
-    @Override
-    public long getHeaderLength() {
-        return this.headerLength;
-    }
-
-    @Override
-    public long getFirstEntryId() {
-        return this.firstEntryId;
-    }
-
-    public StreamingDataBlockHeaderImpl(long headerLength, long blockLength, long ledgerId, long firstEntryId) {
-        this.headerLength = headerLength;
-        this.blockLength = blockLength;
-        this.firstEntryId = firstEntryId;
-        this.ledgerId = ledgerId;
-    }
-
-    // Construct DataBlockHeader from InputStream, which contains `HEADER_MAX_SIZE` bytes readable.
-    public static StreamingDataBlockHeaderImpl fromStream(InputStream stream) throws IOException {
-        CountingInputStream countingStream = new CountingInputStream(stream);
-        DataInputStream dis = new DataInputStream(countingStream);
-        int magic = dis.readInt();
-        if (magic != MAGIC_WORD) {
-            throw new IOException("Data block header magic word not match. read: " + magic
-                    + " expected: " + MAGIC_WORD);
-        }
-
-        long headerLen = dis.readLong();
-        long blockLen = dis.readLong();
-        long firstEntryId = dis.readLong();
-        long ledgerId = dis.readLong();
-        long toSkip = headerLen - countingStream.getCount();
-        if (dis.skip(toSkip) != toSkip) {
-            throw new EOFException("Header was too small");
-        }
-
-        return new StreamingDataBlockHeaderImpl(headerLen, blockLen, ledgerId, firstEntryId);
-    }
-
-    /**
-     * Get the content of the data block header as InputStream.
-     * Read out in format:
-     *   [ magic_word -- int ][ block_len -- int ][ first_entry_id  -- long] [padding zeros]
-     */
-    @Override
-    public InputStream toStream() {
-        ByteBuf out = PulsarByteBufAllocator.DEFAULT.buffer(HEADER_MAX_SIZE, HEADER_MAX_SIZE);
-        out.writeInt(MAGIC_WORD)
-                .writeLong(headerLength)
-                .writeLong(blockLength)
-                .writeLong(firstEntryId)
-                .writeLong(ledgerId)
-                .writeBytes(PADDING);
-
-        // true means the input stream will release the ByteBuf on close
-        return new ByteBufInputStream(out, true);
-    }
-
-    @Override
-    public String toString() {
-        return String.format("StreamingDataBlockHeader(len:%d,hlen:%d,firstEntry:%d,ledger:%d)",
-                blockLength, headerLength, firstEntryId, ledgerId);
-    }
-}
-
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import com.google.common.io.CountingInputStream;
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.ByteBufInputStream;
+import java.io.DataInputStream;
+import java.io.EOFException;
+import java.io.IOException;
+import java.io.InputStream;
+import org.apache.bookkeeper.mledger.offload.jcloud.DataBlockHeader;
+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;
+
+/**
+ * The data block header in tiered storage for each data block.
+ */
+public class StreamingDataBlockHeaderImpl implements DataBlockHeader {
+    // Magic Word for streaming data block.
+    // It is a sequence of bytes used to identify the start of a block.
+    static final int MAGIC_WORD = 0x26A66D32;
+    // This is bigger than header size. Leaving some place for alignment and future enhancement.
+    // Payload use this as the start offset.
+    public static final int HEADER_MAX_SIZE = 128;
+    private static final int HEADER_BYTES_USED = 4 /* magic */
+            + 8 /* header len */
+            + 8 /* block len */
+            + 8 /* first entry id */
+            + 8 /* ledger id */;
+    private static final byte[] PADDING = new byte[HEADER_MAX_SIZE - HEADER_BYTES_USED];
+
+    public long getLedgerId() {
+        return ledgerId;
+    }
+
+    private final long ledgerId;
+
+    public static StreamingDataBlockHeaderImpl of(int blockLength, long ledgerId, long firstEntryId) {
+        return new StreamingDataBlockHeaderImpl(HEADER_MAX_SIZE, blockLength, ledgerId, firstEntryId);
+    }
+
+    private final long headerLength;
+    private final long blockLength;
+    private final long firstEntryId;
+
+    public static int getBlockMagicWord() {
+        return MAGIC_WORD;
+    }
+
+    public static int getDataStartOffset() {
+        return HEADER_MAX_SIZE;
+    }
+
+    @Override
+    public long getBlockLength() {
+        return this.blockLength;
+    }
+
+    @Override
+    public long getHeaderLength() {
+        return this.headerLength;
+    }
+
+    @Override
+    public long getFirstEntryId() {
+        return this.firstEntryId;
+    }
+
+    public StreamingDataBlockHeaderImpl(long headerLength, long blockLength, long ledgerId, long firstEntryId) {
+        this.headerLength = headerLength;
+        this.blockLength = blockLength;
+        this.firstEntryId = firstEntryId;
+        this.ledgerId = ledgerId;
+    }
+
+    // Construct DataBlockHeader from InputStream, which contains `HEADER_MAX_SIZE` bytes readable.
+    public static StreamingDataBlockHeaderImpl fromStream(InputStream stream) throws IOException {
+        CountingInputStream countingStream = new CountingInputStream(stream);
+        DataInputStream dis = new DataInputStream(countingStream);
+        int magic = dis.readInt();
+        if (magic != MAGIC_WORD) {
+            throw new IOException("Data block header magic word not match. read: " + magic
+                    + " expected: " + MAGIC_WORD);
+        }
+
+        long headerLen = dis.readLong();
+        long blockLen = dis.readLong();
+        long firstEntryId = dis.readLong();
+        long ledgerId = dis.readLong();
+        long toSkip = headerLen - countingStream.getCount();
+        if (dis.skip(toSkip) != toSkip) {
+            throw new EOFException("Header was too small");
+        }
+
+        return new StreamingDataBlockHeaderImpl(headerLen, blockLen, ledgerId, firstEntryId);
+    }
+
+    /**
+     * Get the content of the data block header as InputStream.
+     * Read out in format:
+     *   [ magic_word -- int ][ block_len -- int ][ first_entry_id  -- long] [padding zeros]
+     */
+    @Override
+    public InputStream toStream() {
+        ByteBuf out = PulsarByteBufAllocator.DEFAULT.buffer(HEADER_MAX_SIZE, HEADER_MAX_SIZE);
+        out.writeInt(MAGIC_WORD)
+                .writeLong(headerLength)
+                .writeLong(blockLength)
+                .writeLong(firstEntryId)
+                .writeLong(ledgerId)
+                .writeBytes(PADDING);
+
+        // true means the input stream will release the ByteBuf on close
+        return new ByteBufInputStream(out, true);
+    }
+
+    @Override
+    public String toString() {
+        return String.format("StreamingDataBlockHeader(len:%d,hlen:%d,firstEntry:%d,ledger:%d)",
+                blockLength, headerLength, firstEntryId, ledgerId);
+    }
+}
+
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/package-info.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/package-info.java
index 628a0b9117..e868c0f314 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/package-info.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/package-info.java
@@ -1,19 +1,19 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
 package org.apache.bookkeeper.mledger.offload.jcloud.impl;
\ No newline at end of file
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/package-info.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/package-info.java
index 4667971f6a..cd43d87805 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/package-info.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/package-info.java
@@ -1,19 +1,19 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
 package org.apache.bookkeeper.mledger.offload.jcloud;
\ No newline at end of file
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/BlobStoreLocation.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/BlobStoreLocation.java
index c7cc6b4617..d18bc1d630 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/BlobStoreLocation.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/BlobStoreLocation.java
@@ -1,62 +1,62 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.provider;
-
-import java.io.Serializable;
-import java.util.Map;
-import lombok.Data;
-import lombok.EqualsAndHashCode;
-
-/**
- * Tiered storage blob storage location metadata.
- */
-@Data
-@EqualsAndHashCode
-public class BlobStoreLocation implements Serializable {
-
-    private static final long serialVersionUID = 1L;
-
-    private final String providerName;
-    private final String region;
-    private final String bucket;
-    private final String endpoint;
-
-    public BlobStoreLocation(Map<String, String> metadata) {
-        this.providerName = getProvider(metadata);
-        this.region = getRegion(metadata);
-        this.bucket = getBucket(metadata);
-        this.endpoint = getEndpoint(metadata);
-    }
-
-    public String getProvider(Map<String, String> offloadDriverMetadata) {
-        return offloadDriverMetadata.get(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY);
-    }
-
-    public String getRegion(Map<String, String> offloadDriverMetadata) {
-        return offloadDriverMetadata.getOrDefault(TieredStorageConfiguration.METADATA_FIELD_REGION, "");
-    }
-
-    public String getBucket(Map<String, String> offloadDriverMetadata) {
-        return offloadDriverMetadata.get(TieredStorageConfiguration.METADATA_FIELD_BUCKET);
-    }
-
-    public String getEndpoint(Map<String, String> offloadDriverMetadata) {
-        return offloadDriverMetadata.getOrDefault(TieredStorageConfiguration.METADATA_FIELD_ENDPOINT, "");
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.provider;
+
+import java.io.Serializable;
+import java.util.Map;
+import lombok.Data;
+import lombok.EqualsAndHashCode;
+
+/**
+ * Tiered storage blob storage location metadata.
+ */
+@Data
+@EqualsAndHashCode
+public class BlobStoreLocation implements Serializable {
+
+    private static final long serialVersionUID = 1L;
+
+    private final String providerName;
+    private final String region;
+    private final String bucket;
+    private final String endpoint;
+
+    public BlobStoreLocation(Map<String, String> metadata) {
+        this.providerName = getProvider(metadata);
+        this.region = getRegion(metadata);
+        this.bucket = getBucket(metadata);
+        this.endpoint = getEndpoint(metadata);
+    }
+
+    public String getProvider(Map<String, String> offloadDriverMetadata) {
+        return offloadDriverMetadata.get(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY);
+    }
+
+    public String getRegion(Map<String, String> offloadDriverMetadata) {
+        return offloadDriverMetadata.getOrDefault(TieredStorageConfiguration.METADATA_FIELD_REGION, "");
+    }
+
+    public String getBucket(Map<String, String> offloadDriverMetadata) {
+        return offloadDriverMetadata.get(TieredStorageConfiguration.METADATA_FIELD_BUCKET);
+    }
+
+    public String getEndpoint(Map<String, String> offloadDriverMetadata) {
+        return offloadDriverMetadata.getOrDefault(TieredStorageConfiguration.METADATA_FIELD_ENDPOINT, "");
+    }
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/JCloudBlobStoreProvider.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/JCloudBlobStoreProvider.java
index f859f2b732..7c1fe587c5 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/JCloudBlobStoreProvider.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/JCloudBlobStoreProvider.java
@@ -1,452 +1,452 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.provider;
-
-import static org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.GCS_ACCOUNT_KEY_FILE_FIELD;
-import static org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.S3_ID_FIELD;
-import static org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.S3_ROLE_FIELD;
-import static org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.S3_ROLE_SESSION_NAME_FIELD;
-import static org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.S3_SECRET_FIELD;
-import com.amazonaws.auth.AWSCredentials;
-import com.amazonaws.auth.AWSCredentialsProvider;
-import com.amazonaws.auth.AWSSessionCredentials;
-import com.amazonaws.auth.AWSStaticCredentialsProvider;
-import com.amazonaws.auth.BasicAWSCredentials;
-import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;
-import com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider;
-import com.google.common.base.Strings;
-import com.google.common.io.Files;
-import java.io.File;
-import java.io.IOException;
-import java.io.Serializable;
-import java.nio.charset.Charset;
-import java.util.Properties;
-import java.util.UUID;
-import lombok.extern.slf4j.Slf4j;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.BlobStoreBuilder;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.ConfigValidation;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.CredentialBuilder;
-import org.apache.commons.lang3.StringUtils;
-import org.apache.pulsar.jclouds.ShadedJCloudsUtils;
-import org.jclouds.ContextBuilder;
-import org.jclouds.aws.domain.SessionCredentials;
-import org.jclouds.aws.s3.AWSS3ProviderMetadata;
-import org.jclouds.azureblob.AzureBlobProviderMetadata;
-import org.jclouds.blobstore.BlobStore;
-import org.jclouds.blobstore.BlobStoreContext;
-import org.jclouds.blobstore.TransientApiMetadata;
-import org.jclouds.domain.Credentials;
-import org.jclouds.domain.Location;
-import org.jclouds.domain.LocationBuilder;
-import org.jclouds.domain.LocationScope;
-import org.jclouds.googlecloud.GoogleCredentialsFromJson;
-import org.jclouds.googlecloudstorage.GoogleCloudStorageProviderMetadata;
-import org.jclouds.providers.AnonymousProviderMetadata;
-import org.jclouds.providers.ProviderMetadata;
-import org.jclouds.s3.S3ApiMetadata;
-import org.jclouds.s3.reference.S3Constants;
-
-/**
- * Enumeration of the supported JCloud Blob Store Providers.
- * <p>
- * Each Enumeration is responsible for implementation of its own validation,
- * service authentication, and factory method for creating and instance of the
- * JClod BlobStore type.
- *
- * Additional enumerations can be added in the future support other JCloud Providers,
- * currently JClouds supports the following:
- *
- *   - providers=[aws-s3, azureblob, b2, google-cloud-storage, rackspace-cloudfiles-us, rackspace-cloudfiles-uk]
- *   - apis=[s3, sts, transient, atmos, openstack-swift, openstack-keystone, openstack-keystone-3,
- *           rackspace-cloudfiles, rackspace-cloudidentity, filesystem]
- *
- * Note: The driver name associated with each Enum MUST match one of the above vaules, as it is used to instantiate the
- * org.jclouds.ContextBuilder used to create the BlobStore.
- *</p>
- */
-@Slf4j
-public enum JCloudBlobStoreProvider implements Serializable, ConfigValidation, BlobStoreBuilder, CredentialBuilder  {
-
-    AWS_S3("aws-s3", new AWSS3ProviderMetadata()) {
-        @Override
-        public void validate(TieredStorageConfiguration config) throws IllegalArgumentException {
-            VALIDATION.validate(config);
-        }
-
-        @Override
-        public BlobStore getBlobStore(TieredStorageConfiguration config) {
-            return BLOB_STORE_BUILDER.getBlobStore(config);
-        }
-
-        @Override
-        public void buildCredentials(TieredStorageConfiguration config) {
-            AWS_CREDENTIAL_BUILDER.buildCredentials(config);
-        }
-    },
-
-    GOOGLE_CLOUD_STORAGE("google-cloud-storage", new GoogleCloudStorageProviderMetadata()) {
-        @Override
-        public void validate(TieredStorageConfiguration config) throws IllegalArgumentException {
-            VALIDATION.validate(config);
-        }
-
-        @Override
-        public BlobStore getBlobStore(TieredStorageConfiguration config) {
-            return BLOB_STORE_BUILDER.getBlobStore(config);
-        }
-
-        @Override
-        public void buildCredentials(TieredStorageConfiguration config) {
-            if (config.getCredentials() == null) {
-                try {
-                    String gcsKeyContent = Files.asCharSource(
-                            new File(config.getConfigProperty(GCS_ACCOUNT_KEY_FILE_FIELD)),
-                            Charset.defaultCharset()).read();
-                    config.setProviderCredentials(() -> new GoogleCredentialsFromJson(gcsKeyContent).get());
-                } catch (IOException ioe) {
-                    log.error("Cannot read GCS service account credentials file: {}",
-                            config.getConfigProperty("gcsManagedLedgerOffloadServiceAccountKeyFile"));
-                    throw new IllegalArgumentException(ioe);
-                }
-            }
-        }
-    },
-
-    AZURE_BLOB("azureblob", new AzureBlobProviderMetadata()) {
-        @Override
-        public void validate(TieredStorageConfiguration config) throws IllegalArgumentException {
-            VALIDATION.validate(config);
-        }
-
-        @Override
-        public BlobStore getBlobStore(TieredStorageConfiguration config) {
-            ContextBuilder contextBuilder = ContextBuilder.newBuilder(config.getProviderMetadata());
-            ShadedJCloudsUtils.addStandardModules(contextBuilder);
-            contextBuilder.overrides(config.getOverrides());
-
-            if (config.getProviderCredentials() != null) {
-                Credentials credentials = config.getProviderCredentials().get();
-                return contextBuilder
-                        .credentials(credentials.identity, credentials.credential)
-                        .buildView(BlobStoreContext.class)
-                        .getBlobStore();
-            } else {
-                log.warn("The credentials is null. driver: {}, bucket: {}", config.getDriver(), config.getBucket());
-                return contextBuilder
-                        .buildView(BlobStoreContext.class)
-                        .getBlobStore();
-            }
-        }
-
-        @Override
-        public void buildCredentials(TieredStorageConfiguration config) {
-            String accountName = System.getenv("AZURE_STORAGE_ACCOUNT");
-            if (StringUtils.isEmpty(accountName)) {
-                throw new IllegalArgumentException("Couldn't get the azure storage account.");
-            }
-            String accountKey = System.getenv("AZURE_STORAGE_ACCESS_KEY");
-            if (StringUtils.isEmpty(accountKey)) {
-                throw new IllegalArgumentException("Couldn't get the azure storage access key.");
-            }
-            config.setProviderCredentials(() -> new Credentials(accountName, accountKey));
-        }
-    },
-
-
-    /**
-     * Aliyun OSS is compatible with the S3 API.
-     * https://www.alibabacloud.com/help/doc-detail/64919.htm
-     */
-    ALIYUN_OSS("aliyun-oss", new AnonymousProviderMetadata(new S3ApiMetadata(), "")) {
-        @Override
-        public void validate(TieredStorageConfiguration config) throws IllegalArgumentException {
-            S3_VALIDATION.validate(config);
-        }
-
-        @Override
-        public BlobStore getBlobStore(TieredStorageConfiguration config) {
-            return S3_BLOB_STORE_BUILDER.getBlobStore(config);
-        }
-
-        @Override
-        public void buildCredentials(TieredStorageConfiguration config) {
-            S3_CREDENTIAL_BUILDER.buildCredentials(config);
-        }
-    },
-
-    S3("S3", new AnonymousProviderMetadata(new S3ApiMetadata(), "")) {
-        @Override
-        public BlobStore getBlobStore(TieredStorageConfiguration config) {
-            return S3_BLOB_STORE_BUILDER.getBlobStore(config);
-        }
-
-        @Override
-        public void buildCredentials(TieredStorageConfiguration config) {
-            S3_CREDENTIAL_BUILDER.buildCredentials(config);
-        }
-
-        @Override
-        public void validate(TieredStorageConfiguration config) throws IllegalArgumentException {
-            S3_VALIDATION.validate(config);
-        }
-    },
-
-    TRANSIENT("transient", new AnonymousProviderMetadata(new TransientApiMetadata(), "")) {
-        @Override
-        public void validate(TieredStorageConfiguration config) throws IllegalArgumentException {
-            if (Strings.isNullOrEmpty(config.getBucket())) {
-                throw new IllegalArgumentException(
-                    "Bucket cannot be empty for Local offload");
-            }
-        }
-
-        @Override
-        public BlobStore getBlobStore(TieredStorageConfiguration config) {
-
-            ContextBuilder contextBuilder =  ContextBuilder.newBuilder("transient");
-            ShadedJCloudsUtils.addStandardModules(contextBuilder);
-            BlobStoreContext ctx = contextBuilder
-                    .buildView(BlobStoreContext.class);
-
-            BlobStore bs = ctx.getBlobStore();
-
-            if (!bs.containerExists(config.getBucket())) {
-                Location loc = new LocationBuilder()
-                        .scope(LocationScope.HOST)
-                        .id(UUID.randomUUID() + "")
-                        .description("Transient " + config.getBucket())
-                        .build();
-
-                bs.createContainerInLocation(loc, config.getBucket());
-            }
-            System.out.println("Returning " + bs);
-            return bs;
-        }
-
-        @Override
-        public void buildCredentials(TieredStorageConfiguration config) {
-            // No-op
-        }
-    };
-
-    public static JCloudBlobStoreProvider getProvider(String driver) {
-        if (StringUtils.isEmpty(driver)) {
-            return null;
-        }
-        for (JCloudBlobStoreProvider provider : JCloudBlobStoreProvider.values()) {
-            if (provider.driver.equalsIgnoreCase(driver)) {
-                return provider;
-            }
-        }
-        return null;
-    }
-
-    public static boolean driverSupported(String driverName) {
-        for (JCloudBlobStoreProvider provider: JCloudBlobStoreProvider.values()) {
-            if (provider.getDriver().equalsIgnoreCase(driverName)) {
-                return true;
-            }
-        }
-        return false;
-    }
-
-    private String driver;
-    private ProviderMetadata providerMetadata;
-
-    JCloudBlobStoreProvider(String s, ProviderMetadata providerMetadata) {
-        this.driver = s;
-        this.providerMetadata = providerMetadata;
-    }
-
-    public String getDriver() {
-        return driver;
-    }
-
-    public ProviderMetadata getProviderMetadata() {
-        return providerMetadata;
-    }
-
-    // Constants for reuse across AWS, GCS, and Azure, etc.
-    static final ConfigValidation VALIDATION = (TieredStorageConfiguration config) -> {
-        if (Strings.isNullOrEmpty(config.getRegion()) && Strings.isNullOrEmpty(config.getServiceEndpoint())) {
-            throw new IllegalArgumentException(
-                "Either Region or ServiceEndpoint must specified for " + config.getDriver() + " offload");
-        }
-
-        if (Strings.isNullOrEmpty(config.getBucket())) {
-            throw new IllegalArgumentException(
-                "Bucket cannot be empty for " + config.getDriver() + " offload");
-        }
-
-        if (config.getMaxBlockSizeInBytes() < (5 * 1024 * 1024)) {
-            throw new IllegalArgumentException(
-                "ManagedLedgerOffloadMaxBlockSizeInBytes cannot be less than 5MB for "
-                + config.getDriver() + " offload");
-        }
-    };
-
-    static final BlobStoreBuilder BLOB_STORE_BUILDER = (TieredStorageConfiguration config) -> {
-        ContextBuilder contextBuilder = ContextBuilder.newBuilder(config.getProviderMetadata());
-        ShadedJCloudsUtils.addStandardModules(contextBuilder);
-        contextBuilder.overrides(config.getOverrides());
-
-        if (StringUtils.isNotEmpty(config.getServiceEndpoint())) {
-            contextBuilder.endpoint(config.getServiceEndpoint());
-        }
-
-        if (config.getProviderCredentials() != null) {
-                return contextBuilder
-                        .credentialsSupplier(config.getCredentials()::get)
-                        .buildView(BlobStoreContext.class)
-                        .getBlobStore();
-        } else {
-            log.warn("The credentials is null. driver: {}, bucket: {}", config.getDriver(), config.getBucket());
-            return contextBuilder
-                    .buildView(BlobStoreContext.class)
-                    .getBlobStore();
-        }
-
-    };
-
-    static final CredentialBuilder AWS_CREDENTIAL_BUILDER = (TieredStorageConfiguration config) -> {
-        if (config.getCredentials() == null) {
-            final AWSCredentialsProvider authChain;
-            try {
-                if (!Strings.isNullOrEmpty(config.getConfigProperty(S3_ID_FIELD))
-                    && !Strings.isNullOrEmpty(config.getConfigProperty(S3_SECRET_FIELD))) {
-                    AWSCredentials awsCredentials = new AWSCredentials() {
-                        @Override
-                        public String getAWSAccessKeyId() {
-                            return config.getConfigProperty(S3_ID_FIELD);
-                        }
-
-                        @Override
-                        public String getAWSSecretKey() {
-                            return config.getConfigProperty(S3_SECRET_FIELD);
-                        }
-                    };
-                    authChain = new AWSStaticCredentialsProvider(
-                            new BasicAWSCredentials(
-                                config.getConfigProperty(S3_ID_FIELD),
-                                config.getConfigProperty(S3_SECRET_FIELD)));
-                } else if (Strings.isNullOrEmpty(config.getConfigProperty(S3_ROLE_FIELD))) {
-                    authChain = DefaultAWSCredentialsProviderChain.getInstance();
-                } else {
-                    authChain =
-                            new STSAssumeRoleSessionCredentialsProvider.Builder(
-                                    config.getConfigProperty(S3_ROLE_FIELD),
-                                    config.getConfigProperty(S3_ROLE_SESSION_NAME_FIELD)
-                            ).build();
-                }
-
-                // Important! Delay the building of actual credentials
-                // until later to support tokens that may be refreshed
-                // such as all session tokens
-                config.setProviderCredentials(() -> {
-                    AWSCredentials newCreds = authChain.getCredentials();
-                    Credentials jcloudCred = null;
-
-                    if (newCreds instanceof AWSSessionCredentials) {
-                        // if we have session credentials, we need to send the session token
-                        // this allows us to support EC2 metadata credentials
-                        jcloudCred = SessionCredentials.builder()
-                                .accessKeyId(newCreds.getAWSAccessKeyId())
-                                .secretAccessKey(newCreds.getAWSSecretKey())
-                                .sessionToken(((AWSSessionCredentials) newCreds).getSessionToken())
-                                .build();
-                    } else {
-                        // in the event we hit this branch, we likely don't have expiring
-                        // credentials, however, this still allows for the user to update
-                        // profiles creds or some other mechanism
-                        jcloudCred = new Credentials(
-                                newCreds.getAWSAccessKeyId(), newCreds.getAWSSecretKey());
-                    }
-                    return jcloudCred;
-                });
-            } catch (Exception e) {
-                // allowed, some mock s3 service do not need credential
-                log.warn("Exception when get credentials for s3 ", e);
-            }
-        }
-    };
-
-    static final BlobStoreBuilder S3_BLOB_STORE_BUILDER = (TieredStorageConfiguration config) -> {
-        ContextBuilder contextBuilder = ContextBuilder.newBuilder(config.getProviderMetadata());
-        ShadedJCloudsUtils.addStandardModules(contextBuilder);
-        Properties overrides = config.getOverrides();
-        if (ALIYUN_OSS.getDriver().equals(config.getDriver())) {
-            // For security reasons, OSS supports only virtual hosted style access.
-            overrides.setProperty(S3Constants.PROPERTY_S3_VIRTUAL_HOST_BUCKETS, "true");
-        }
-        contextBuilder.overrides(overrides);
-        contextBuilder.endpoint(config.getServiceEndpoint());
-
-        if (config.getProviderCredentials() != null) {
-            return contextBuilder
-                    .credentialsSupplier(config.getCredentials()::get)
-                    .buildView(BlobStoreContext.class)
-                    .getBlobStore();
-        } else {
-            log.warn("The credentials is null. driver: {}, bucket: {}", config.getDriver(), config.getBucket());
-            return contextBuilder
-                    .buildView(BlobStoreContext.class)
-                    .getBlobStore();
-        }
-    };
-
-    static final ConfigValidation S3_VALIDATION = (TieredStorageConfiguration config) -> {
-        if (Strings.isNullOrEmpty(config.getServiceEndpoint())) {
-            throw new IllegalArgumentException(
-                    "ServiceEndpoint must specified for " + config.getDriver() + " offload");
-        }
-
-        if (Strings.isNullOrEmpty(config.getBucket())) {
-            throw new IllegalArgumentException(
-                    "Bucket cannot be empty for " + config.getDriver() + " offload");
-        }
-
-        if (config.getMaxBlockSizeInBytes() < (5 * 1024 * 1024)) {
-            throw new IllegalArgumentException(
-                    "ManagedLedgerOffloadMaxBlockSizeInBytes cannot be less than 5MB for "
-                            + config.getDriver() + " offload");
-        }
-    };
-
-    static final CredentialBuilder S3_CREDENTIAL_BUILDER = (TieredStorageConfiguration config) -> {
-        String accountName = System.getenv().getOrDefault("ACCESS_KEY_ID", "");
-        // For forward compatibility
-        if (StringUtils.isEmpty(accountName.trim())) {
-            accountName = System.getenv().getOrDefault("ALIYUN_OSS_ACCESS_KEY_ID", "");
-        }
-        if (StringUtils.isEmpty(accountName.trim())) {
-            throw new IllegalArgumentException("Couldn't get the access key id.");
-        }
-        String accountKey = System.getenv().getOrDefault("ACCESS_KEY_SECRET", "");
-        if (StringUtils.isEmpty(accountKey.trim())) {
-            accountKey = System.getenv().getOrDefault("ALIYUN_OSS_ACCESS_KEY_SECRET", "");
-        }
-        if (StringUtils.isEmpty(accountKey.trim())) {
-            throw new IllegalArgumentException("Couldn't get the access key secret.");
-        }
-        Credentials credentials = new Credentials(
-                accountName, accountKey);
-        config.setProviderCredentials(() -> credentials);
-    };
-
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.provider;
+
+import static org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.GCS_ACCOUNT_KEY_FILE_FIELD;
+import static org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.S3_ID_FIELD;
+import static org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.S3_ROLE_FIELD;
+import static org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.S3_ROLE_SESSION_NAME_FIELD;
+import static org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.S3_SECRET_FIELD;
+import com.amazonaws.auth.AWSCredentials;
+import com.amazonaws.auth.AWSCredentialsProvider;
+import com.amazonaws.auth.AWSSessionCredentials;
+import com.amazonaws.auth.AWSStaticCredentialsProvider;
+import com.amazonaws.auth.BasicAWSCredentials;
+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;
+import com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider;
+import com.google.common.base.Strings;
+import com.google.common.io.Files;
+import java.io.File;
+import java.io.IOException;
+import java.io.Serializable;
+import java.nio.charset.Charset;
+import java.util.Properties;
+import java.util.UUID;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.BlobStoreBuilder;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.ConfigValidation;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration.CredentialBuilder;
+import org.apache.commons.lang3.StringUtils;
+import org.apache.pulsar.jclouds.ShadedJCloudsUtils;
+import org.jclouds.ContextBuilder;
+import org.jclouds.aws.domain.SessionCredentials;
+import org.jclouds.aws.s3.AWSS3ProviderMetadata;
+import org.jclouds.azureblob.AzureBlobProviderMetadata;
+import org.jclouds.blobstore.BlobStore;
+import org.jclouds.blobstore.BlobStoreContext;
+import org.jclouds.blobstore.TransientApiMetadata;
+import org.jclouds.domain.Credentials;
+import org.jclouds.domain.Location;
+import org.jclouds.domain.LocationBuilder;
+import org.jclouds.domain.LocationScope;
+import org.jclouds.googlecloud.GoogleCredentialsFromJson;
+import org.jclouds.googlecloudstorage.GoogleCloudStorageProviderMetadata;
+import org.jclouds.providers.AnonymousProviderMetadata;
+import org.jclouds.providers.ProviderMetadata;
+import org.jclouds.s3.S3ApiMetadata;
+import org.jclouds.s3.reference.S3Constants;
+
+/**
+ * Enumeration of the supported JCloud Blob Store Providers.
+ * <p>
+ * Each Enumeration is responsible for implementation of its own validation,
+ * service authentication, and factory method for creating and instance of the
+ * JClod BlobStore type.
+ *
+ * Additional enumerations can be added in the future support other JCloud Providers,
+ * currently JClouds supports the following:
+ *
+ *   - providers=[aws-s3, azureblob, b2, google-cloud-storage, rackspace-cloudfiles-us, rackspace-cloudfiles-uk]
+ *   - apis=[s3, sts, transient, atmos, openstack-swift, openstack-keystone, openstack-keystone-3,
+ *           rackspace-cloudfiles, rackspace-cloudidentity, filesystem]
+ *
+ * Note: The driver name associated with each Enum MUST match one of the above vaules, as it is used to instantiate the
+ * org.jclouds.ContextBuilder used to create the BlobStore.
+ *</p>
+ */
+@Slf4j
+public enum JCloudBlobStoreProvider implements Serializable, ConfigValidation, BlobStoreBuilder, CredentialBuilder  {
+
+    AWS_S3("aws-s3", new AWSS3ProviderMetadata()) {
+        @Override
+        public void validate(TieredStorageConfiguration config) throws IllegalArgumentException {
+            VALIDATION.validate(config);
+        }
+
+        @Override
+        public BlobStore getBlobStore(TieredStorageConfiguration config) {
+            return BLOB_STORE_BUILDER.getBlobStore(config);
+        }
+
+        @Override
+        public void buildCredentials(TieredStorageConfiguration config) {
+            AWS_CREDENTIAL_BUILDER.buildCredentials(config);
+        }
+    },
+
+    GOOGLE_CLOUD_STORAGE("google-cloud-storage", new GoogleCloudStorageProviderMetadata()) {
+        @Override
+        public void validate(TieredStorageConfiguration config) throws IllegalArgumentException {
+            VALIDATION.validate(config);
+        }
+
+        @Override
+        public BlobStore getBlobStore(TieredStorageConfiguration config) {
+            return BLOB_STORE_BUILDER.getBlobStore(config);
+        }
+
+        @Override
+        public void buildCredentials(TieredStorageConfiguration config) {
+            if (config.getCredentials() == null) {
+                try {
+                    String gcsKeyContent = Files.asCharSource(
+                            new File(config.getConfigProperty(GCS_ACCOUNT_KEY_FILE_FIELD)),
+                            Charset.defaultCharset()).read();
+                    config.setProviderCredentials(() -> new GoogleCredentialsFromJson(gcsKeyContent).get());
+                } catch (IOException ioe) {
+                    log.error("Cannot read GCS service account credentials file: {}",
+                            config.getConfigProperty("gcsManagedLedgerOffloadServiceAccountKeyFile"));
+                    throw new IllegalArgumentException(ioe);
+                }
+            }
+        }
+    },
+
+    AZURE_BLOB("azureblob", new AzureBlobProviderMetadata()) {
+        @Override
+        public void validate(TieredStorageConfiguration config) throws IllegalArgumentException {
+            VALIDATION.validate(config);
+        }
+
+        @Override
+        public BlobStore getBlobStore(TieredStorageConfiguration config) {
+            ContextBuilder contextBuilder = ContextBuilder.newBuilder(config.getProviderMetadata());
+            ShadedJCloudsUtils.addStandardModules(contextBuilder);
+            contextBuilder.overrides(config.getOverrides());
+
+            if (config.getProviderCredentials() != null) {
+                Credentials credentials = config.getProviderCredentials().get();
+                return contextBuilder
+                        .credentials(credentials.identity, credentials.credential)
+                        .buildView(BlobStoreContext.class)
+                        .getBlobStore();
+            } else {
+                log.warn("The credentials is null. driver: {}, bucket: {}", config.getDriver(), config.getBucket());
+                return contextBuilder
+                        .buildView(BlobStoreContext.class)
+                        .getBlobStore();
+            }
+        }
+
+        @Override
+        public void buildCredentials(TieredStorageConfiguration config) {
+            String accountName = System.getenv("AZURE_STORAGE_ACCOUNT");
+            if (StringUtils.isEmpty(accountName)) {
+                throw new IllegalArgumentException("Couldn't get the azure storage account.");
+            }
+            String accountKey = System.getenv("AZURE_STORAGE_ACCESS_KEY");
+            if (StringUtils.isEmpty(accountKey)) {
+                throw new IllegalArgumentException("Couldn't get the azure storage access key.");
+            }
+            config.setProviderCredentials(() -> new Credentials(accountName, accountKey));
+        }
+    },
+
+
+    /**
+     * Aliyun OSS is compatible with the S3 API.
+     * https://www.alibabacloud.com/help/doc-detail/64919.htm
+     */
+    ALIYUN_OSS("aliyun-oss", new AnonymousProviderMetadata(new S3ApiMetadata(), "")) {
+        @Override
+        public void validate(TieredStorageConfiguration config) throws IllegalArgumentException {
+            S3_VALIDATION.validate(config);
+        }
+
+        @Override
+        public BlobStore getBlobStore(TieredStorageConfiguration config) {
+            return S3_BLOB_STORE_BUILDER.getBlobStore(config);
+        }
+
+        @Override
+        public void buildCredentials(TieredStorageConfiguration config) {
+            S3_CREDENTIAL_BUILDER.buildCredentials(config);
+        }
+    },
+
+    S3("S3", new AnonymousProviderMetadata(new S3ApiMetadata(), "")) {
+        @Override
+        public BlobStore getBlobStore(TieredStorageConfiguration config) {
+            return S3_BLOB_STORE_BUILDER.getBlobStore(config);
+        }
+
+        @Override
+        public void buildCredentials(TieredStorageConfiguration config) {
+            S3_CREDENTIAL_BUILDER.buildCredentials(config);
+        }
+
+        @Override
+        public void validate(TieredStorageConfiguration config) throws IllegalArgumentException {
+            S3_VALIDATION.validate(config);
+        }
+    },
+
+    TRANSIENT("transient", new AnonymousProviderMetadata(new TransientApiMetadata(), "")) {
+        @Override
+        public void validate(TieredStorageConfiguration config) throws IllegalArgumentException {
+            if (Strings.isNullOrEmpty(config.getBucket())) {
+                throw new IllegalArgumentException(
+                    "Bucket cannot be empty for Local offload");
+            }
+        }
+
+        @Override
+        public BlobStore getBlobStore(TieredStorageConfiguration config) {
+
+            ContextBuilder contextBuilder =  ContextBuilder.newBuilder("transient");
+            ShadedJCloudsUtils.addStandardModules(contextBuilder);
+            BlobStoreContext ctx = contextBuilder
+                    .buildView(BlobStoreContext.class);
+
+            BlobStore bs = ctx.getBlobStore();
+
+            if (!bs.containerExists(config.getBucket())) {
+                Location loc = new LocationBuilder()
+                        .scope(LocationScope.HOST)
+                        .id(UUID.randomUUID() + "")
+                        .description("Transient " + config.getBucket())
+                        .build();
+
+                bs.createContainerInLocation(loc, config.getBucket());
+            }
+            System.out.println("Returning " + bs);
+            return bs;
+        }
+
+        @Override
+        public void buildCredentials(TieredStorageConfiguration config) {
+            // No-op
+        }
+    };
+
+    public static JCloudBlobStoreProvider getProvider(String driver) {
+        if (StringUtils.isEmpty(driver)) {
+            return null;
+        }
+        for (JCloudBlobStoreProvider provider : JCloudBlobStoreProvider.values()) {
+            if (provider.driver.equalsIgnoreCase(driver)) {
+                return provider;
+            }
+        }
+        return null;
+    }
+
+    public static boolean driverSupported(String driverName) {
+        for (JCloudBlobStoreProvider provider: JCloudBlobStoreProvider.values()) {
+            if (provider.getDriver().equalsIgnoreCase(driverName)) {
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private String driver;
+    private ProviderMetadata providerMetadata;
+
+    JCloudBlobStoreProvider(String s, ProviderMetadata providerMetadata) {
+        this.driver = s;
+        this.providerMetadata = providerMetadata;
+    }
+
+    public String getDriver() {
+        return driver;
+    }
+
+    public ProviderMetadata getProviderMetadata() {
+        return providerMetadata;
+    }
+
+    // Constants for reuse across AWS, GCS, and Azure, etc.
+    static final ConfigValidation VALIDATION = (TieredStorageConfiguration config) -> {
+        if (Strings.isNullOrEmpty(config.getRegion()) && Strings.isNullOrEmpty(config.getServiceEndpoint())) {
+            throw new IllegalArgumentException(
+                "Either Region or ServiceEndpoint must specified for " + config.getDriver() + " offload");
+        }
+
+        if (Strings.isNullOrEmpty(config.getBucket())) {
+            throw new IllegalArgumentException(
+                "Bucket cannot be empty for " + config.getDriver() + " offload");
+        }
+
+        if (config.getMaxBlockSizeInBytes() < (5 * 1024 * 1024)) {
+            throw new IllegalArgumentException(
+                "ManagedLedgerOffloadMaxBlockSizeInBytes cannot be less than 5MB for "
+                + config.getDriver() + " offload");
+        }
+    };
+
+    static final BlobStoreBuilder BLOB_STORE_BUILDER = (TieredStorageConfiguration config) -> {
+        ContextBuilder contextBuilder = ContextBuilder.newBuilder(config.getProviderMetadata());
+        ShadedJCloudsUtils.addStandardModules(contextBuilder);
+        contextBuilder.overrides(config.getOverrides());
+
+        if (StringUtils.isNotEmpty(config.getServiceEndpoint())) {
+            contextBuilder.endpoint(config.getServiceEndpoint());
+        }
+
+        if (config.getProviderCredentials() != null) {
+                return contextBuilder
+                        .credentialsSupplier(config.getCredentials()::get)
+                        .buildView(BlobStoreContext.class)
+                        .getBlobStore();
+        } else {
+            log.warn("The credentials is null. driver: {}, bucket: {}", config.getDriver(), config.getBucket());
+            return contextBuilder
+                    .buildView(BlobStoreContext.class)
+                    .getBlobStore();
+        }
+
+    };
+
+    static final CredentialBuilder AWS_CREDENTIAL_BUILDER = (TieredStorageConfiguration config) -> {
+        if (config.getCredentials() == null) {
+            final AWSCredentialsProvider authChain;
+            try {
+                if (!Strings.isNullOrEmpty(config.getConfigProperty(S3_ID_FIELD))
+                    && !Strings.isNullOrEmpty(config.getConfigProperty(S3_SECRET_FIELD))) {
+                    AWSCredentials awsCredentials = new AWSCredentials() {
+                        @Override
+                        public String getAWSAccessKeyId() {
+                            return config.getConfigProperty(S3_ID_FIELD);
+                        }
+
+                        @Override
+                        public String getAWSSecretKey() {
+                            return config.getConfigProperty(S3_SECRET_FIELD);
+                        }
+                    };
+                    authChain = new AWSStaticCredentialsProvider(
+                            new BasicAWSCredentials(
+                                config.getConfigProperty(S3_ID_FIELD),
+                                config.getConfigProperty(S3_SECRET_FIELD)));
+                } else if (Strings.isNullOrEmpty(config.getConfigProperty(S3_ROLE_FIELD))) {
+                    authChain = DefaultAWSCredentialsProviderChain.getInstance();
+                } else {
+                    authChain =
+                            new STSAssumeRoleSessionCredentialsProvider.Builder(
+                                    config.getConfigProperty(S3_ROLE_FIELD),
+                                    config.getConfigProperty(S3_ROLE_SESSION_NAME_FIELD)
+                            ).build();
+                }
+
+                // Important! Delay the building of actual credentials
+                // until later to support tokens that may be refreshed
+                // such as all session tokens
+                config.setProviderCredentials(() -> {
+                    AWSCredentials newCreds = authChain.getCredentials();
+                    Credentials jcloudCred = null;
+
+                    if (newCreds instanceof AWSSessionCredentials) {
+                        // if we have session credentials, we need to send the session token
+                        // this allows us to support EC2 metadata credentials
+                        jcloudCred = SessionCredentials.builder()
+                                .accessKeyId(newCreds.getAWSAccessKeyId())
+                                .secretAccessKey(newCreds.getAWSSecretKey())
+                                .sessionToken(((AWSSessionCredentials) newCreds).getSessionToken())
+                                .build();
+                    } else {
+                        // in the event we hit this branch, we likely don't have expiring
+                        // credentials, however, this still allows for the user to update
+                        // profiles creds or some other mechanism
+                        jcloudCred = new Credentials(
+                                newCreds.getAWSAccessKeyId(), newCreds.getAWSSecretKey());
+                    }
+                    return jcloudCred;
+                });
+            } catch (Exception e) {
+                // allowed, some mock s3 service do not need credential
+                log.warn("Exception when get credentials for s3 ", e);
+            }
+        }
+    };
+
+    static final BlobStoreBuilder S3_BLOB_STORE_BUILDER = (TieredStorageConfiguration config) -> {
+        ContextBuilder contextBuilder = ContextBuilder.newBuilder(config.getProviderMetadata());
+        ShadedJCloudsUtils.addStandardModules(contextBuilder);
+        Properties overrides = config.getOverrides();
+        if (ALIYUN_OSS.getDriver().equals(config.getDriver())) {
+            // For security reasons, OSS supports only virtual hosted style access.
+            overrides.setProperty(S3Constants.PROPERTY_S3_VIRTUAL_HOST_BUCKETS, "true");
+        }
+        contextBuilder.overrides(overrides);
+        contextBuilder.endpoint(config.getServiceEndpoint());
+
+        if (config.getProviderCredentials() != null) {
+            return contextBuilder
+                    .credentialsSupplier(config.getCredentials()::get)
+                    .buildView(BlobStoreContext.class)
+                    .getBlobStore();
+        } else {
+            log.warn("The credentials is null. driver: {}, bucket: {}", config.getDriver(), config.getBucket());
+            return contextBuilder
+                    .buildView(BlobStoreContext.class)
+                    .getBlobStore();
+        }
+    };
+
+    static final ConfigValidation S3_VALIDATION = (TieredStorageConfiguration config) -> {
+        if (Strings.isNullOrEmpty(config.getServiceEndpoint())) {
+            throw new IllegalArgumentException(
+                    "ServiceEndpoint must specified for " + config.getDriver() + " offload");
+        }
+
+        if (Strings.isNullOrEmpty(config.getBucket())) {
+            throw new IllegalArgumentException(
+                    "Bucket cannot be empty for " + config.getDriver() + " offload");
+        }
+
+        if (config.getMaxBlockSizeInBytes() < (5 * 1024 * 1024)) {
+            throw new IllegalArgumentException(
+                    "ManagedLedgerOffloadMaxBlockSizeInBytes cannot be less than 5MB for "
+                            + config.getDriver() + " offload");
+        }
+    };
+
+    static final CredentialBuilder S3_CREDENTIAL_BUILDER = (TieredStorageConfiguration config) -> {
+        String accountName = System.getenv().getOrDefault("ACCESS_KEY_ID", "");
+        // For forward compatibility
+        if (StringUtils.isEmpty(accountName.trim())) {
+            accountName = System.getenv().getOrDefault("ALIYUN_OSS_ACCESS_KEY_ID", "");
+        }
+        if (StringUtils.isEmpty(accountName.trim())) {
+            throw new IllegalArgumentException("Couldn't get the access key id.");
+        }
+        String accountKey = System.getenv().getOrDefault("ACCESS_KEY_SECRET", "");
+        if (StringUtils.isEmpty(accountKey.trim())) {
+            accountKey = System.getenv().getOrDefault("ALIYUN_OSS_ACCESS_KEY_SECRET", "");
+        }
+        if (StringUtils.isEmpty(accountKey.trim())) {
+            throw new IllegalArgumentException("Couldn't get the access key secret.");
+        }
+        Credentials credentials = new Credentials(
+                accountName, accountKey);
+        config.setProviderCredentials(() -> credentials);
+    };
+
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TieredStorageConfiguration.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TieredStorageConfiguration.java
index d7a4e4d449..57295f6604 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TieredStorageConfiguration.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TieredStorageConfiguration.java
@@ -1,376 +1,376 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.provider;
-
-import static org.apache.bookkeeper.mledger.offload.jcloud.provider.JCloudBlobStoreProvider.AWS_S3;
-import static org.apache.bookkeeper.mledger.offload.jcloud.provider.JCloudBlobStoreProvider.GOOGLE_CLOUD_STORAGE;
-import com.google.common.collect.ImmutableMap;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-import java.util.function.Supplier;
-import java.util.stream.Collectors;
-import lombok.Getter;
-import lombok.extern.slf4j.Slf4j;
-import org.apache.commons.lang3.StringUtils;
-import org.jclouds.Constants;
-import org.jclouds.aws.s3.AWSS3ProviderMetadata;
-import org.jclouds.blobstore.BlobStore;
-import org.jclouds.domain.Credentials;
-import org.jclouds.googlecloudstorage.GoogleCloudStorageProviderMetadata;
-import org.jclouds.osgi.ApiRegistry;
-import org.jclouds.osgi.ProviderRegistry;
-import org.jclouds.providers.ProviderMetadata;
-import org.jclouds.s3.S3ApiMetadata;
-import org.jclouds.s3.reference.S3Constants;
-
-/**
- * Class responsible for holding all of the tiered storage configuration data
- * that is set in the global Pulsar broker.conf file.
- * <p>
- * This class is used by the BlobStoreManagedLedgerOffloader to determine which
- * JCloud provider to use for Tiered Storage offloand, along with the associated
- * properties such as region, bucket, user credentials, etc.
- * </p>
- */
-@Slf4j
-public class TieredStorageConfiguration {
-
-    private static final long serialVersionUID = 1L;
-    public static final String BLOB_STORE_PROVIDER_KEY = "managedLedgerOffloadDriver";
-    public static final String METADATA_FIELD_BUCKET = "bucket";
-    public static final String METADATA_FIELD_REGION = "region";
-    public static final String METADATA_FIELD_ENDPOINT = "serviceEndpoint";
-    public static final String METADATA_FIELD_MAX_BLOCK_SIZE = "maxBlockSizeInBytes";
-    public static final String METADATA_FIELD_MIN_BLOCK_SIZE = "minBlockSizeInBytes";
-    public static final String METADATA_FIELD_READ_BUFFER_SIZE = "readBufferSizeInBytes";
-    public static final String METADATA_FIELD_WRITE_BUFFER_SIZE = "writeBufferSizeInBytes";
-    public static final String OFFLOADER_PROPERTY_PREFIX = "managedLedgerOffload";
-    public static final String MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC = "maxOffloadSegmentRolloverTimeInSeconds";
-    public static final String MIN_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC = "minOffloadSegmentRolloverTimeInSeconds";
-    public static final long DEFAULT_MAX_SEGMENT_TIME_IN_SECOND = 600;
-    public static final long DEFAULT_MIN_SEGMENT_TIME_IN_SECOND = 0;
-    public static final String MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES = "maxOffloadSegmentSizeInBytes";
-    public static final long DEFAULT_MAX_SEGMENT_SIZE_IN_BYTES = 1024 * 1024 * 1024;
-
-    protected static final int MB = 1024 * 1024;
-
-    public static final String GCS_ACCOUNT_KEY_FILE_FIELD = "gcsManagedLedgerOffloadServiceAccountKeyFile";
-    public static final String S3_ID_FIELD = "s3ManagedLedgerOffloadCredentialId";
-    public static final String S3_SECRET_FIELD = "s3ManagedLedgerOffloadCredentialSecret";
-    public static final String S3_ROLE_FIELD = "s3ManagedLedgerOffloadRole";
-    public static final String S3_ROLE_SESSION_NAME_FIELD = "s3ManagedLedgerOffloadRoleSessionName";
-
-    public static TieredStorageConfiguration create(Properties props) throws IOException {
-        Map<String, String> map = new HashMap<String, String>();
-        map.putAll(props.entrySet()
-                .stream()
-                .collect(Collectors.toMap(e -> e.getKey().toString(),
-                                          e -> e.getValue().toString())));
-
-        return new TieredStorageConfiguration(map);
-    }
-
-    public static TieredStorageConfiguration create(Map<String, String> props) {
-        return new TieredStorageConfiguration(props);
-    }
-
-    @Getter
-    private final Map<String, String> configProperties;
-    @Getter
-    private Supplier<Credentials> credentials;
-    private JCloudBlobStoreProvider provider;
-
-    public TieredStorageConfiguration(Map<String, String> configProperties) {
-        if (configProperties != null) {
-            this.configProperties = configProperties;
-        } else {
-            throw new IllegalArgumentException("configProperties cannot be null");
-        }
-    }
-
-    public List<String> getKeys(String property) {
-        List<String> keys = new ArrayList<String> ();
-
-        String bc = getBackwardCompatibleKey(property);
-        if (StringUtils.isNotBlank(bc)) {
-            keys.add(bc);
-        }
-
-        String key = getKeyName(property);
-        if (StringUtils.isNotBlank(key)) {
-            keys.add(key);
-        }
-        return keys;
-    }
-
-    private String getKeyName(String property) {
-        StringBuilder sb = new StringBuilder();
-        sb.append(OFFLOADER_PROPERTY_PREFIX)
-          .append(StringUtils.capitalize(property));
-
-        return sb.toString();
-    }
-
-    private String getBackwardCompatibleKey(String property) {
-        switch (getProvider()) {
-            case AWS_S3:
-                return new StringBuilder().append("s3ManagedLedgerOffload")
-                                          .append(StringUtils.capitalize(property))
-                                          .toString();
-
-            case GOOGLE_CLOUD_STORAGE:
-                return new StringBuilder().append("gcsManagedLedgerOffload")
-                                          .append(StringUtils.capitalize(property))
-                                          .toString();
-
-            default:
-                return null;
-        }
-    }
-
-    public String getBlobStoreProviderKey() {
-        return configProperties.getOrDefault(BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
-    }
-
-    public String getDriver() {
-        return getProvider().getDriver();
-    }
-
-    public String getRegion() {
-        for (String key : getKeys(METADATA_FIELD_REGION)) {
-            if (configProperties.containsKey(key)) {
-                return configProperties.get(key);
-            }
-        }
-        return null;
-    }
-
-    public void setRegion(String s) {
-        configProperties.put(getKeyName(METADATA_FIELD_REGION), s);
-    }
-
-    public String getBucket() {
-        for (String key : getKeys(METADATA_FIELD_BUCKET)) {
-            if (configProperties.containsKey(key)) {
-                return configProperties.get(key);
-            }
-        }
-        return null;
-    }
-
-    public String getServiceEndpoint() {
-        for (String key : getKeys(METADATA_FIELD_ENDPOINT)) {
-            if (configProperties.containsKey(key)) {
-                return configProperties.get(key);
-            }
-        }
-        return null;
-    }
-
-    public long getMaxSegmentTimeInSecond() {
-        if (configProperties.containsKey(MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC)) {
-            return Long.parseLong(configProperties.get(MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC));
-        } else {
-            return DEFAULT_MAX_SEGMENT_TIME_IN_SECOND;
-        }
-    }
-
-    public long getMinSegmentTimeInSecond() {
-        if (configProperties.containsKey(MIN_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC)) {
-            return Long.parseLong(configProperties.get(MIN_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC));
-        } else {
-            return DEFAULT_MIN_SEGMENT_TIME_IN_SECOND;
-        }
-    }
-
-    public long getMaxSegmentSizeInBytes() {
-        if (configProperties.containsKey(MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES)) {
-            return Long.parseLong(configProperties.get(MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES));
-        } else {
-            return DEFAULT_MAX_SEGMENT_SIZE_IN_BYTES;
-        }
-    }
-
-    public void setServiceEndpoint(String s) {
-        configProperties.put(getKeyName(METADATA_FIELD_ENDPOINT), s);
-    }
-
-    /**
-     * Used to find a specific configuration property other than
-     * one of the predefined ones. This allows for any number of
-     * provider specific, or new properties to added in the future.
-     *
-     * @param propertyName
-     * @return
-     */
-    public String getConfigProperty(String propertyName) {
-        return configProperties.get(propertyName);
-    }
-
-    public JCloudBlobStoreProvider getProvider() {
-        if (provider == null) {
-            provider = JCloudBlobStoreProvider.getProvider(getBlobStoreProviderKey());
-        }
-        return provider;
-    }
-
-    public void setProvider(JCloudBlobStoreProvider provider) {
-        this.provider = provider;
-    }
-
-    public Integer getMaxBlockSizeInBytes() {
-        for (String key : getKeys(METADATA_FIELD_MAX_BLOCK_SIZE)) {
-            if (configProperties.containsKey(key)) {
-                return Integer.valueOf(configProperties.get(key));
-            }
-        }
-        return 64 * MB;
-    }
-
-    public Integer getMinBlockSizeInBytes() {
-        for (String key : getKeys(METADATA_FIELD_MIN_BLOCK_SIZE)) {
-            if (configProperties.containsKey(key)) {
-                return Integer.valueOf(configProperties.get(key));
-            }
-        }
-        return 5 * MB;
-    }
-
-    public Integer getReadBufferSizeInBytes() {
-        for (String key : getKeys(METADATA_FIELD_READ_BUFFER_SIZE)) {
-            if (configProperties.containsKey(key)) {
-                return Integer.valueOf(configProperties.get(key));
-            }
-        }
-        return MB;
-    }
-
-    public Integer getWriteBufferSizeInBytes() {
-        for (String key : getKeys(METADATA_FIELD_WRITE_BUFFER_SIZE)) {
-            if (configProperties.containsKey(key)) {
-                return Integer.valueOf(configProperties.get(key));
-            }
-        }
-        return 10 * MB;
-    }
-
-    public Supplier<Credentials> getProviderCredentials() {
-        if (credentials == null) {
-            getProvider().buildCredentials(this);
-        }
-        return credentials;
-    }
-
-    public void setProviderCredentials(Supplier<Credentials> credentials) {
-        this.credentials = credentials;
-    }
-
-    public void validate() {
-        getProvider().validate(this);
-    }
-
-    public ProviderMetadata getProviderMetadata() {
-        return getProvider().getProviderMetadata();
-    }
-
-    public BlobStoreLocation getBlobStoreLocation() {
-        return new BlobStoreLocation(getOffloadDriverMetadata());
-    }
-
-    public BlobStore getBlobStore() {
-        return getProvider().getBlobStore(this);
-    }
-
-    public Map<String, String> getOffloadDriverMetadata() {
-        return ImmutableMap.of(
-                BLOB_STORE_PROVIDER_KEY, (getProvider() != null) ? getProvider().toString() : "",
-                METADATA_FIELD_BUCKET,  (getBucket() != null) ?  getBucket() : "",
-                METADATA_FIELD_REGION, (getRegion() != null) ? getRegion() : "",
-                METADATA_FIELD_ENDPOINT, (getServiceEndpoint() != null) ? getServiceEndpoint() : ""
-             );
-    }
-
-    protected Properties getOverrides() {
-        Properties overrides = new Properties();
-        // This property controls the number of parts being uploaded in parallel.
-        overrides.setProperty("jclouds.mpu.parallel.degree", "1");
-        overrides.setProperty("jclouds.mpu.parts.size", Integer.toString(getMaxBlockSizeInBytes()));
-        overrides.setProperty(Constants.PROPERTY_SO_TIMEOUT, "25000");
-        overrides.setProperty(Constants.PROPERTY_MAX_RETRIES, Integer.toString(100));
-
-        if (getDriver().equalsIgnoreCase(AWS_S3.getDriver())) {
-            ApiRegistry.registerApi(new S3ApiMetadata());
-            ProviderRegistry.registerProvider(new AWSS3ProviderMetadata());
-        } else if (getDriver().equalsIgnoreCase(GOOGLE_CLOUD_STORAGE.getDriver())) {
-            ProviderRegistry.registerProvider(new GoogleCloudStorageProviderMetadata());
-        }
-
-        if (StringUtils.isNotEmpty(getServiceEndpoint())) {
-            overrides.setProperty(S3Constants.PROPERTY_S3_VIRTUAL_HOST_BUCKETS, "false");
-        }
-
-        // load more jclouds properties into the overrides
-        System.getProperties().entrySet().stream()
-            .filter(p -> p.getKey().toString().startsWith("jclouds"))
-            .forEach(jcloudsProp -> {
-                overrides.setProperty(jcloudsProp.getKey().toString(), jcloudsProp.getValue().toString());
-            });
-
-        System.getenv().entrySet().stream()
-            .filter(p -> p.getKey().toString().startsWith("jclouds"))
-            .forEach(jcloudsProp -> {
-                overrides.setProperty(jcloudsProp.getKey().toString(), jcloudsProp.getValue().toString());
-            });
-
-        log.info("getOverrides: {}", overrides.toString());
-        return overrides;
-    }
-
-    /*
-     * Interfaces for the JCloudBlobStoreProvider's to implement
-     */
-    /**
-     * Checks the given TieredStorageConfiguration to see if all of the
-     * required properties are set, and that all properties are valid.
-     */
-    public interface ConfigValidation {
-        void validate(TieredStorageConfiguration config) throws IllegalArgumentException;
-    }
-
-    /**
-     * Constructs the proper credentials for the given JCloud provider
-     * from the given TieredStorageConfiguration.
-     */
-    public interface CredentialBuilder {
-        void buildCredentials(TieredStorageConfiguration config);
-    }
-
-    /**
-     * Builds a JCloudprovider BlobStore from the TieredStorageConfiguration,
-     * which allows us to publish and retrieve data blocks from the external
-     * storage system.
-     */
-    public interface BlobStoreBuilder {
-        BlobStore getBlobStore(TieredStorageConfiguration config);
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.provider;
+
+import static org.apache.bookkeeper.mledger.offload.jcloud.provider.JCloudBlobStoreProvider.AWS_S3;
+import static org.apache.bookkeeper.mledger.offload.jcloud.provider.JCloudBlobStoreProvider.GOOGLE_CLOUD_STORAGE;
+import com.google.common.collect.ImmutableMap;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+import java.util.function.Supplier;
+import java.util.stream.Collectors;
+import lombok.Getter;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.commons.lang3.StringUtils;
+import org.jclouds.Constants;
+import org.jclouds.aws.s3.AWSS3ProviderMetadata;
+import org.jclouds.blobstore.BlobStore;
+import org.jclouds.domain.Credentials;
+import org.jclouds.googlecloudstorage.GoogleCloudStorageProviderMetadata;
+import org.jclouds.osgi.ApiRegistry;
+import org.jclouds.osgi.ProviderRegistry;
+import org.jclouds.providers.ProviderMetadata;
+import org.jclouds.s3.S3ApiMetadata;
+import org.jclouds.s3.reference.S3Constants;
+
+/**
+ * Class responsible for holding all of the tiered storage configuration data
+ * that is set in the global Pulsar broker.conf file.
+ * <p>
+ * This class is used by the BlobStoreManagedLedgerOffloader to determine which
+ * JCloud provider to use for Tiered Storage offloand, along with the associated
+ * properties such as region, bucket, user credentials, etc.
+ * </p>
+ */
+@Slf4j
+public class TieredStorageConfiguration {
+
+    private static final long serialVersionUID = 1L;
+    public static final String BLOB_STORE_PROVIDER_KEY = "managedLedgerOffloadDriver";
+    public static final String METADATA_FIELD_BUCKET = "bucket";
+    public static final String METADATA_FIELD_REGION = "region";
+    public static final String METADATA_FIELD_ENDPOINT = "serviceEndpoint";
+    public static final String METADATA_FIELD_MAX_BLOCK_SIZE = "maxBlockSizeInBytes";
+    public static final String METADATA_FIELD_MIN_BLOCK_SIZE = "minBlockSizeInBytes";
+    public static final String METADATA_FIELD_READ_BUFFER_SIZE = "readBufferSizeInBytes";
+    public static final String METADATA_FIELD_WRITE_BUFFER_SIZE = "writeBufferSizeInBytes";
+    public static final String OFFLOADER_PROPERTY_PREFIX = "managedLedgerOffload";
+    public static final String MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC = "maxOffloadSegmentRolloverTimeInSeconds";
+    public static final String MIN_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC = "minOffloadSegmentRolloverTimeInSeconds";
+    public static final long DEFAULT_MAX_SEGMENT_TIME_IN_SECOND = 600;
+    public static final long DEFAULT_MIN_SEGMENT_TIME_IN_SECOND = 0;
+    public static final String MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES = "maxOffloadSegmentSizeInBytes";
+    public static final long DEFAULT_MAX_SEGMENT_SIZE_IN_BYTES = 1024 * 1024 * 1024;
+
+    protected static final int MB = 1024 * 1024;
+
+    public static final String GCS_ACCOUNT_KEY_FILE_FIELD = "gcsManagedLedgerOffloadServiceAccountKeyFile";
+    public static final String S3_ID_FIELD = "s3ManagedLedgerOffloadCredentialId";
+    public static final String S3_SECRET_FIELD = "s3ManagedLedgerOffloadCredentialSecret";
+    public static final String S3_ROLE_FIELD = "s3ManagedLedgerOffloadRole";
+    public static final String S3_ROLE_SESSION_NAME_FIELD = "s3ManagedLedgerOffloadRoleSessionName";
+
+    public static TieredStorageConfiguration create(Properties props) throws IOException {
+        Map<String, String> map = new HashMap<String, String>();
+        map.putAll(props.entrySet()
+                .stream()
+                .collect(Collectors.toMap(e -> e.getKey().toString(),
+                                          e -> e.getValue().toString())));
+
+        return new TieredStorageConfiguration(map);
+    }
+
+    public static TieredStorageConfiguration create(Map<String, String> props) {
+        return new TieredStorageConfiguration(props);
+    }
+
+    @Getter
+    private final Map<String, String> configProperties;
+    @Getter
+    private Supplier<Credentials> credentials;
+    private JCloudBlobStoreProvider provider;
+
+    public TieredStorageConfiguration(Map<String, String> configProperties) {
+        if (configProperties != null) {
+            this.configProperties = configProperties;
+        } else {
+            throw new IllegalArgumentException("configProperties cannot be null");
+        }
+    }
+
+    public List<String> getKeys(String property) {
+        List<String> keys = new ArrayList<String> ();
+
+        String bc = getBackwardCompatibleKey(property);
+        if (StringUtils.isNotBlank(bc)) {
+            keys.add(bc);
+        }
+
+        String key = getKeyName(property);
+        if (StringUtils.isNotBlank(key)) {
+            keys.add(key);
+        }
+        return keys;
+    }
+
+    private String getKeyName(String property) {
+        StringBuilder sb = new StringBuilder();
+        sb.append(OFFLOADER_PROPERTY_PREFIX)
+          .append(StringUtils.capitalize(property));
+
+        return sb.toString();
+    }
+
+    private String getBackwardCompatibleKey(String property) {
+        switch (getProvider()) {
+            case AWS_S3:
+                return new StringBuilder().append("s3ManagedLedgerOffload")
+                                          .append(StringUtils.capitalize(property))
+                                          .toString();
+
+            case GOOGLE_CLOUD_STORAGE:
+                return new StringBuilder().append("gcsManagedLedgerOffload")
+                                          .append(StringUtils.capitalize(property))
+                                          .toString();
+
+            default:
+                return null;
+        }
+    }
+
+    public String getBlobStoreProviderKey() {
+        return configProperties.getOrDefault(BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
+    }
+
+    public String getDriver() {
+        return getProvider().getDriver();
+    }
+
+    public String getRegion() {
+        for (String key : getKeys(METADATA_FIELD_REGION)) {
+            if (configProperties.containsKey(key)) {
+                return configProperties.get(key);
+            }
+        }
+        return null;
+    }
+
+    public void setRegion(String s) {
+        configProperties.put(getKeyName(METADATA_FIELD_REGION), s);
+    }
+
+    public String getBucket() {
+        for (String key : getKeys(METADATA_FIELD_BUCKET)) {
+            if (configProperties.containsKey(key)) {
+                return configProperties.get(key);
+            }
+        }
+        return null;
+    }
+
+    public String getServiceEndpoint() {
+        for (String key : getKeys(METADATA_FIELD_ENDPOINT)) {
+            if (configProperties.containsKey(key)) {
+                return configProperties.get(key);
+            }
+        }
+        return null;
+    }
+
+    public long getMaxSegmentTimeInSecond() {
+        if (configProperties.containsKey(MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC)) {
+            return Long.parseLong(configProperties.get(MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC));
+        } else {
+            return DEFAULT_MAX_SEGMENT_TIME_IN_SECOND;
+        }
+    }
+
+    public long getMinSegmentTimeInSecond() {
+        if (configProperties.containsKey(MIN_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC)) {
+            return Long.parseLong(configProperties.get(MIN_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC));
+        } else {
+            return DEFAULT_MIN_SEGMENT_TIME_IN_SECOND;
+        }
+    }
+
+    public long getMaxSegmentSizeInBytes() {
+        if (configProperties.containsKey(MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES)) {
+            return Long.parseLong(configProperties.get(MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES));
+        } else {
+            return DEFAULT_MAX_SEGMENT_SIZE_IN_BYTES;
+        }
+    }
+
+    public void setServiceEndpoint(String s) {
+        configProperties.put(getKeyName(METADATA_FIELD_ENDPOINT), s);
+    }
+
+    /**
+     * Used to find a specific configuration property other than
+     * one of the predefined ones. This allows for any number of
+     * provider specific, or new properties to added in the future.
+     *
+     * @param propertyName
+     * @return
+     */
+    public String getConfigProperty(String propertyName) {
+        return configProperties.get(propertyName);
+    }
+
+    public JCloudBlobStoreProvider getProvider() {
+        if (provider == null) {
+            provider = JCloudBlobStoreProvider.getProvider(getBlobStoreProviderKey());
+        }
+        return provider;
+    }
+
+    public void setProvider(JCloudBlobStoreProvider provider) {
+        this.provider = provider;
+    }
+
+    public Integer getMaxBlockSizeInBytes() {
+        for (String key : getKeys(METADATA_FIELD_MAX_BLOCK_SIZE)) {
+            if (configProperties.containsKey(key)) {
+                return Integer.valueOf(configProperties.get(key));
+            }
+        }
+        return 64 * MB;
+    }
+
+    public Integer getMinBlockSizeInBytes() {
+        for (String key : getKeys(METADATA_FIELD_MIN_BLOCK_SIZE)) {
+            if (configProperties.containsKey(key)) {
+                return Integer.valueOf(configProperties.get(key));
+            }
+        }
+        return 5 * MB;
+    }
+
+    public Integer getReadBufferSizeInBytes() {
+        for (String key : getKeys(METADATA_FIELD_READ_BUFFER_SIZE)) {
+            if (configProperties.containsKey(key)) {
+                return Integer.valueOf(configProperties.get(key));
+            }
+        }
+        return MB;
+    }
+
+    public Integer getWriteBufferSizeInBytes() {
+        for (String key : getKeys(METADATA_FIELD_WRITE_BUFFER_SIZE)) {
+            if (configProperties.containsKey(key)) {
+                return Integer.valueOf(configProperties.get(key));
+            }
+        }
+        return 10 * MB;
+    }
+
+    public Supplier<Credentials> getProviderCredentials() {
+        if (credentials == null) {
+            getProvider().buildCredentials(this);
+        }
+        return credentials;
+    }
+
+    public void setProviderCredentials(Supplier<Credentials> credentials) {
+        this.credentials = credentials;
+    }
+
+    public void validate() {
+        getProvider().validate(this);
+    }
+
+    public ProviderMetadata getProviderMetadata() {
+        return getProvider().getProviderMetadata();
+    }
+
+    public BlobStoreLocation getBlobStoreLocation() {
+        return new BlobStoreLocation(getOffloadDriverMetadata());
+    }
+
+    public BlobStore getBlobStore() {
+        return getProvider().getBlobStore(this);
+    }
+
+    public Map<String, String> getOffloadDriverMetadata() {
+        return ImmutableMap.of(
+                BLOB_STORE_PROVIDER_KEY, (getProvider() != null) ? getProvider().toString() : "",
+                METADATA_FIELD_BUCKET,  (getBucket() != null) ?  getBucket() : "",
+                METADATA_FIELD_REGION, (getRegion() != null) ? getRegion() : "",
+                METADATA_FIELD_ENDPOINT, (getServiceEndpoint() != null) ? getServiceEndpoint() : ""
+             );
+    }
+
+    protected Properties getOverrides() {
+        Properties overrides = new Properties();
+        // This property controls the number of parts being uploaded in parallel.
+        overrides.setProperty("jclouds.mpu.parallel.degree", "1");
+        overrides.setProperty("jclouds.mpu.parts.size", Integer.toString(getMaxBlockSizeInBytes()));
+        overrides.setProperty(Constants.PROPERTY_SO_TIMEOUT, "25000");
+        overrides.setProperty(Constants.PROPERTY_MAX_RETRIES, Integer.toString(100));
+
+        if (getDriver().equalsIgnoreCase(AWS_S3.getDriver())) {
+            ApiRegistry.registerApi(new S3ApiMetadata());
+            ProviderRegistry.registerProvider(new AWSS3ProviderMetadata());
+        } else if (getDriver().equalsIgnoreCase(GOOGLE_CLOUD_STORAGE.getDriver())) {
+            ProviderRegistry.registerProvider(new GoogleCloudStorageProviderMetadata());
+        }
+
+        if (StringUtils.isNotEmpty(getServiceEndpoint())) {
+            overrides.setProperty(S3Constants.PROPERTY_S3_VIRTUAL_HOST_BUCKETS, "false");
+        }
+
+        // load more jclouds properties into the overrides
+        System.getProperties().entrySet().stream()
+            .filter(p -> p.getKey().toString().startsWith("jclouds"))
+            .forEach(jcloudsProp -> {
+                overrides.setProperty(jcloudsProp.getKey().toString(), jcloudsProp.getValue().toString());
+            });
+
+        System.getenv().entrySet().stream()
+            .filter(p -> p.getKey().toString().startsWith("jclouds"))
+            .forEach(jcloudsProp -> {
+                overrides.setProperty(jcloudsProp.getKey().toString(), jcloudsProp.getValue().toString());
+            });
+
+        log.info("getOverrides: {}", overrides.toString());
+        return overrides;
+    }
+
+    /*
+     * Interfaces for the JCloudBlobStoreProvider's to implement
+     */
+    /**
+     * Checks the given TieredStorageConfiguration to see if all of the
+     * required properties are set, and that all properties are valid.
+     */
+    public interface ConfigValidation {
+        void validate(TieredStorageConfiguration config) throws IllegalArgumentException;
+    }
+
+    /**
+     * Constructs the proper credentials for the given JCloud provider
+     * from the given TieredStorageConfiguration.
+     */
+    public interface CredentialBuilder {
+        void buildCredentials(TieredStorageConfiguration config);
+    }
+
+    /**
+     * Builds a JCloudprovider BlobStore from the TieredStorageConfiguration,
+     * which allows us to publish and retrieve data blocks from the external
+     * storage system.
+     */
+    public interface BlobStoreBuilder {
+        BlobStore getBlobStore(TieredStorageConfiguration config);
+    }
+}
diff --git a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/package-info.java b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/package-info.java
index 6cd2f3a86c..c35163e7b6 100644
--- a/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/package-info.java
+++ b/tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/package-info.java
@@ -1,19 +1,19 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
 package org.apache.bookkeeper.mledger.offload.jcloud.provider;
\ No newline at end of file
diff --git a/tiered-storage/jcloud/src/main/resources/META-INF/services/pulsar-offloader.yaml b/tiered-storage/jcloud/src/main/resources/META-INF/services/pulsar-offloader.yaml
index 8f97287a1c..fd053c8836 100644
--- a/tiered-storage/jcloud/src/main/resources/META-INF/services/pulsar-offloader.yaml
+++ b/tiered-storage/jcloud/src/main/resources/META-INF/services/pulsar-offloader.yaml
@@ -1,22 +1,22 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-#
-
-name: jcloud
-description: JCloud based offloader implementation
-offloaderFactoryClass: org.apache.bookkeeper.mledger.offload.jcloud.JCloudLedgerOffloaderFactory
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+name: jcloud
+description: JCloud based offloader implementation
+offloaderFactoryClass: org.apache.bookkeeper.mledger.offload.jcloud.JCloudLedgerOffloaderFactory
diff --git a/tiered-storage/jcloud/src/main/resources/findbugsExclude.xml b/tiered-storage/jcloud/src/main/resources/findbugsExclude.xml
index a8a9b9aae9..da588f4fbc 100644
--- a/tiered-storage/jcloud/src/main/resources/findbugsExclude.xml
+++ b/tiered-storage/jcloud/src/main/resources/findbugsExclude.xml
@@ -1,48 +1,48 @@
-<!--
-
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-
--->
-<FindBugsFilter>
-  <!-- Ignore violations that were present when the rule was enabled -->
-  <Match>
-    <Class name="org.apache.bookkeeper.mledger.offload.jcloud.impl.BlobStoreBackedInputStreamImpl"/>
-    <Method name="&lt;init&gt;"/>
-    <Bug pattern="EI_EXPOSE_REP2"/>
-  </Match>
-  <Match>
-    <Class name="org.apache.bookkeeper.mledger.offload.jcloud.impl.BlobStoreManagedLedgerOffloader"/>
-    <Method name="streamingOffload"/>
-    <Bug pattern="EI_EXPOSE_REP2"/>
-  </Match>
-  <Match>
-    <Class name="org.apache.bookkeeper.mledger.offload.jcloud.impl.BufferedOffloadStream"/>
-    <Method name="&lt;init&gt;"/>
-    <Bug pattern="EI_EXPOSE_REP2"/>
-  </Match>
-  <Match>
-    <Class name="org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration"/>
-    <Method name="getConfigProperties"/>
-    <Bug pattern="EI_EXPOSE_REP"/>
-  </Match>
-  <Match>
-    <Class name="org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration"/>
-    <Method name="&lt;init&gt;"/>
-    <Bug pattern="EI_EXPOSE_REP2"/>
-  </Match>
-</FindBugsFilter>
+<!--
+
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+
+-->
+<FindBugsFilter>
+  <!-- Ignore violations that were present when the rule was enabled -->
+  <Match>
+    <Class name="org.apache.bookkeeper.mledger.offload.jcloud.impl.BlobStoreBackedInputStreamImpl"/>
+    <Method name="&lt;init&gt;"/>
+    <Bug pattern="EI_EXPOSE_REP2"/>
+  </Match>
+  <Match>
+    <Class name="org.apache.bookkeeper.mledger.offload.jcloud.impl.BlobStoreManagedLedgerOffloader"/>
+    <Method name="streamingOffload"/>
+    <Bug pattern="EI_EXPOSE_REP2"/>
+  </Match>
+  <Match>
+    <Class name="org.apache.bookkeeper.mledger.offload.jcloud.impl.BufferedOffloadStream"/>
+    <Method name="&lt;init&gt;"/>
+    <Bug pattern="EI_EXPOSE_REP2"/>
+  </Match>
+  <Match>
+    <Class name="org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration"/>
+    <Method name="getConfigProperties"/>
+    <Bug pattern="EI_EXPOSE_REP"/>
+  </Match>
+  <Match>
+    <Class name="org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration"/>
+    <Method name="&lt;init&gt;"/>
+    <Bug pattern="EI_EXPOSE_REP2"/>
+  </Match>
+</FindBugsFilter>
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/BlobStoreBackedInputStreamTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/BlobStoreBackedInputStreamTest.java
index b84f7c7a00..69e04f26ae 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/BlobStoreBackedInputStreamTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/BlobStoreBackedInputStreamTest.java
@@ -1,306 +1,306 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud;
-
-import static org.mockito.AdditionalAnswers.delegatesTo;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.times;
-import static org.mockito.Mockito.verify;
-import static org.testng.Assert.assertEquals;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Random;
-import lombok.Cleanup;
-import lombok.extern.slf4j.Slf4j;
-import org.apache.bookkeeper.mledger.offload.jcloud.impl.BlobStoreBackedInputStreamImpl;
-import org.jclouds.blobstore.BlobStore;
-import org.jclouds.blobstore.KeyNotFoundException;
-import org.jclouds.blobstore.domain.Blob;
-import org.jclouds.io.Payload;
-import org.jclouds.io.Payloads;
-import org.mockito.ArgumentMatchers;
-import org.mockito.Mockito;
-import org.testng.Assert;
-import org.testng.annotations.Test;
-
-@Slf4j
-public class BlobStoreBackedInputStreamTest extends BlobStoreTestBase {
-
-    class RandomInputStream extends InputStream {
-        final Random r;
-        int bytesRemaining;
-
-        RandomInputStream(int seed, int bytesRemaining) {
-            this.r = new Random(seed);
-            this.bytesRemaining = bytesRemaining;
-        }
-
-        @Override
-        public int read() {
-            if (bytesRemaining-- > 0) {
-                return r.nextInt() & 0xFF;
-            } else {
-                return -1;
-            }
-        }
-    }
-
-    private void assertStreamsMatch(BackedInputStream a, InputStream b, long initialPosition) throws Exception {
-        assertEquals(initialPosition, a.getCurrentPosition());
-        int ret = 0;
-        long expectedPosition = initialPosition;
-        while (ret >= 0) {
-            ret = a.read();
-            assertEquals(ret, b.read());
-            if (ret != -1) {
-                // reached end of the stream, so read() did not advance the position
-                expectedPosition++;
-            }
-            assertEquals(a.getCurrentPosition(), expectedPosition);
-        }
-        assertEquals(-1, a.read());
-        assertEquals(-1, b.read());
-    }
-
-    private void assertStreamsMatchByBytes(BackedInputStream a, InputStream b) throws Exception {
-        byte[] bytesA = new byte[100];
-        byte[] bytesB = new byte[100];
-
-        int retA = 0;
-        long expectedPosition = 0;
-        while (retA >= 0) {
-            retA = a.read(bytesA, 0, 100);
-            int retB = b.read(bytesB, 0, 100);
-            assertEquals(retA, retB);
-            assertEquals(bytesA, bytesB);
-            if (retA != -1) {
-                // reached end of the stream, so read() did not advance the position
-                expectedPosition += retA;
-            }
-            assertEquals(a.getCurrentPosition(), expectedPosition);
-        }
-    }
-
-    @Test
-    public void testReadingFullObject() throws Exception {
-        String objectKey = "testReadingFull";
-        int objectSize = 12345;
-        RandomInputStream toWrite = new RandomInputStream(0, objectSize);
-        RandomInputStream toCompare = new RandomInputStream(0, objectSize);
-
-        Payload payload = Payloads.newInputStreamPayload(toWrite);
-        payload.getContentMetadata().setContentLength((long) objectSize);
-        Blob blob = blobStore.blobBuilder(objectKey)
-            .payload(payload)
-            .contentLength((long) objectSize)
-            .build();
-        String ret = blobStore.putBlob(BUCKET, blob);
-        log.debug("put blob: {} in Bucket: {}, in blobStore, result: {}", objectKey, BUCKET, ret);
-
-        @Cleanup
-        BackedInputStream toTest = new BlobStoreBackedInputStreamImpl(blobStore, BUCKET, objectKey,
-                                                                 (key, md) -> {},
-                                                                 objectSize, 1000);
-        assertStreamsMatch(toTest, toCompare, 0);
-    }
-
-    @Test
-    public void testReadingFullObjectByBytes() throws Exception {
-        String objectKey = "testReadingFull2";
-        int objectSize = 12345;
-        RandomInputStream toWrite = new RandomInputStream(0, objectSize);
-        RandomInputStream toCompare = new RandomInputStream(0, objectSize);
-
-        Payload payload = Payloads.newInputStreamPayload(toWrite);
-        payload.getContentMetadata().setContentLength((long) objectSize);
-        Blob blob = blobStore.blobBuilder(objectKey)
-            .payload(payload)
-            .contentLength((long) objectSize)
-            .build();
-        String ret = blobStore.putBlob(BUCKET, blob);
-        log.debug("put blob: {} in Bucket: {}, in blobStore, result: {}", objectKey, BUCKET, ret);
-
-        @Cleanup
-        BackedInputStream toTest = new BlobStoreBackedInputStreamImpl(blobStore, BUCKET, objectKey,
-                                                                 (key, md) -> {},
-                                                                 objectSize, 1000);
-        assertStreamsMatchByBytes(toTest, toCompare);
-    }
-
-    @Test(expectedExceptions = KeyNotFoundException.class)
-    public void testNotFoundOnRead() throws Exception {
-        @Cleanup
-        BackedInputStream toTest = new BlobStoreBackedInputStreamImpl(blobStore, BUCKET, "doesn't exist",
-                                                                 (key, md) -> {},
-                                                                 1234, 1000);
-        toTest.read();
-    }
-
-
-    @Test
-    public void testSeek() throws Exception {
-        String objectKey = "testSeek";
-        int objectSize = 12345;
-        RandomInputStream toWrite = new RandomInputStream(0, objectSize);
-
-        Map<Integer, InputStream> seeks = new HashMap<>();
-        Random r = new Random(12345);
-        for (int i = 0; i < 20; i++) {
-            int seek = r.nextInt(objectSize + 1);
-            RandomInputStream stream = new RandomInputStream(0, objectSize);
-            stream.skip(seek);
-            seeks.put(seek, stream);
-        }
-
-        Payload payload = Payloads.newInputStreamPayload(toWrite);
-        payload.getContentMetadata().setContentLength((long) objectSize);
-        Blob blob = blobStore.blobBuilder(objectKey)
-            .payload(payload)
-            .contentLength((long) objectSize)
-            .build();
-        String ret = blobStore.putBlob(BUCKET, blob);
-        log.debug("put blob: {} in Bucket: {}, in blobStore, result: {}", objectKey, BUCKET, ret);
-
-        @Cleanup
-        BackedInputStream toTest = new BlobStoreBackedInputStreamImpl(blobStore, BUCKET, objectKey,
-                                                                 (key, md) -> {},
-                                                                 objectSize, 1000);
-        for (Map.Entry<Integer, InputStream> e : seeks.entrySet()) {
-            toTest.seek(e.getKey());
-            assertStreamsMatch(toTest, e.getValue(), e.getKey().longValue());
-        }
-    }
-
-    @Test
-    public void testSeekWithinCurrent() throws Exception {
-        String objectKey = "testSeekWithinCurrent";
-        int objectSize = 12345;
-        RandomInputStream toWrite = new RandomInputStream(0, objectSize);
-
-        Payload payload = Payloads.newInputStreamPayload(toWrite);
-        payload.getContentMetadata().setContentLength((long) objectSize);
-        Blob blob = blobStore.blobBuilder(objectKey)
-            .payload(payload)
-            .contentLength((long) objectSize)
-            .build();
-        String ret = blobStore.putBlob(BUCKET, blob);
-        log.debug("put blob: {} in Bucket: {}, in blobStore, result: {}", objectKey, BUCKET, ret);
-
-        //BlobStore spiedBlobStore = spy(blobStore);
-        BlobStore spiedBlobStore = mock(BlobStore.class, delegatesTo(blobStore));
-
-        @Cleanup
-        BackedInputStream toTest = new BlobStoreBackedInputStreamImpl(spiedBlobStore, BUCKET, objectKey,
-                                                                 (key, md) -> {},
-                                                                 objectSize, 1000);
-
-        // seek forward
-        RandomInputStream firstSeek = new RandomInputStream(0, objectSize);
-        toTest.seek(100);
-        firstSeek.skip(100);
-        for (int i = 0; i < 100; i++) {
-            assertEquals(firstSeek.read(), toTest.read());
-        }
-
-        // seek forward a bit more, but in same block
-        RandomInputStream secondSeek = new RandomInputStream(0, objectSize);
-        toTest.seek(600);
-        secondSeek.skip(600);
-        for (int i = 0; i < 100; i++) {
-            assertEquals(secondSeek.read(), toTest.read());
-        }
-
-        // seek back
-        RandomInputStream thirdSeek = new RandomInputStream(0, objectSize);
-        toTest.seek(200);
-        thirdSeek.skip(200);
-        for (int i = 0; i < 100; i++) {
-            assertEquals(thirdSeek.read(), toTest.read());
-        }
-
-        verify(spiedBlobStore, times(1))
-            .getBlob(Mockito.eq(BUCKET), Mockito.eq(objectKey), ArgumentMatchers.any());
-    }
-
-    @Test
-    public void testSeekForward() throws Exception {
-        String objectKey = "testSeekForward";
-        int objectSize = 12345;
-        RandomInputStream toWrite = new RandomInputStream(0, objectSize);
-
-        Payload payload = Payloads.newInputStreamPayload(toWrite);
-        payload.getContentMetadata().setContentLength((long) objectSize);
-        Blob blob = blobStore.blobBuilder(objectKey)
-            .payload(payload)
-            .contentLength((long) objectSize)
-            .build();
-        String ret = blobStore.putBlob(BUCKET, blob);
-        log.debug("put blob: {} in Bucket: {}, in blobStore, result: {}", objectKey, BUCKET, ret);
-
-        @Cleanup
-        BackedInputStream toTest = new BlobStoreBackedInputStreamImpl(blobStore, BUCKET, objectKey,
-                                                                 (key, md) -> {},
-                                                                 objectSize, 1000);
-
-        // seek forward to middle
-        long middle = objectSize / 2;
-        toTest.seekForward(middle);
-
-        try {
-            long before = middle - objectSize / 4;
-            toTest.seekForward(before);
-            Assert.fail("Shound't be able to seek backwards");
-        } catch (IOException ioe) {
-            // correct
-        }
-
-        long after = middle + objectSize / 4;
-        RandomInputStream toCompare = new RandomInputStream(0, objectSize);
-        toCompare.skip(after);
-
-        toTest.seekForward(after);
-        assertStreamsMatch(toTest, toCompare, after);
-    }
-
-    @Test
-    public void testAvailable() throws IOException {
-        String objectKey = "testAvailable";
-        int objectSize = 2048;
-        RandomInputStream toWrite = new RandomInputStream(0, objectSize);
-        Payload payload = Payloads.newInputStreamPayload(toWrite);
-        payload.getContentMetadata().setContentLength((long) objectSize);
-        Blob blob = blobStore.blobBuilder(objectKey)
-            .payload(payload)
-            .contentLength(objectSize)
-            .build();
-        String ret = blobStore.putBlob(BUCKET, blob);
-        @Cleanup
-        BackedInputStream bis = new BlobStoreBackedInputStreamImpl(
-            blobStore, BUCKET, objectKey, (k, md) -> {}, objectSize, 512);
-        assertEquals(bis.available(), objectSize);
-        bis.seek(500);
-        assertEquals(bis.available(), objectSize - 500);
-        bis.seek(1024);
-        assertEquals(bis.available(), 1024);
-        bis.seek(2048);
-        assertEquals(bis.available(), 0);
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud;
+
+import static org.mockito.AdditionalAnswers.delegatesTo;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.times;
+import static org.mockito.Mockito.verify;
+import static org.testng.Assert.assertEquals;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Random;
+import lombok.Cleanup;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.bookkeeper.mledger.offload.jcloud.impl.BlobStoreBackedInputStreamImpl;
+import org.jclouds.blobstore.BlobStore;
+import org.jclouds.blobstore.KeyNotFoundException;
+import org.jclouds.blobstore.domain.Blob;
+import org.jclouds.io.Payload;
+import org.jclouds.io.Payloads;
+import org.mockito.ArgumentMatchers;
+import org.mockito.Mockito;
+import org.testng.Assert;
+import org.testng.annotations.Test;
+
+@Slf4j
+public class BlobStoreBackedInputStreamTest extends BlobStoreTestBase {
+
+    class RandomInputStream extends InputStream {
+        final Random r;
+        int bytesRemaining;
+
+        RandomInputStream(int seed, int bytesRemaining) {
+            this.r = new Random(seed);
+            this.bytesRemaining = bytesRemaining;
+        }
+
+        @Override
+        public int read() {
+            if (bytesRemaining-- > 0) {
+                return r.nextInt() & 0xFF;
+            } else {
+                return -1;
+            }
+        }
+    }
+
+    private void assertStreamsMatch(BackedInputStream a, InputStream b, long initialPosition) throws Exception {
+        assertEquals(initialPosition, a.getCurrentPosition());
+        int ret = 0;
+        long expectedPosition = initialPosition;
+        while (ret >= 0) {
+            ret = a.read();
+            assertEquals(ret, b.read());
+            if (ret != -1) {
+                // reached end of the stream, so read() did not advance the position
+                expectedPosition++;
+            }
+            assertEquals(a.getCurrentPosition(), expectedPosition);
+        }
+        assertEquals(-1, a.read());
+        assertEquals(-1, b.read());
+    }
+
+    private void assertStreamsMatchByBytes(BackedInputStream a, InputStream b) throws Exception {
+        byte[] bytesA = new byte[100];
+        byte[] bytesB = new byte[100];
+
+        int retA = 0;
+        long expectedPosition = 0;
+        while (retA >= 0) {
+            retA = a.read(bytesA, 0, 100);
+            int retB = b.read(bytesB, 0, 100);
+            assertEquals(retA, retB);
+            assertEquals(bytesA, bytesB);
+            if (retA != -1) {
+                // reached end of the stream, so read() did not advance the position
+                expectedPosition += retA;
+            }
+            assertEquals(a.getCurrentPosition(), expectedPosition);
+        }
+    }
+
+    @Test
+    public void testReadingFullObject() throws Exception {
+        String objectKey = "testReadingFull";
+        int objectSize = 12345;
+        RandomInputStream toWrite = new RandomInputStream(0, objectSize);
+        RandomInputStream toCompare = new RandomInputStream(0, objectSize);
+
+        Payload payload = Payloads.newInputStreamPayload(toWrite);
+        payload.getContentMetadata().setContentLength((long) objectSize);
+        Blob blob = blobStore.blobBuilder(objectKey)
+            .payload(payload)
+            .contentLength((long) objectSize)
+            .build();
+        String ret = blobStore.putBlob(BUCKET, blob);
+        log.debug("put blob: {} in Bucket: {}, in blobStore, result: {}", objectKey, BUCKET, ret);
+
+        @Cleanup
+        BackedInputStream toTest = new BlobStoreBackedInputStreamImpl(blobStore, BUCKET, objectKey,
+                                                                 (key, md) -> {},
+                                                                 objectSize, 1000);
+        assertStreamsMatch(toTest, toCompare, 0);
+    }
+
+    @Test
+    public void testReadingFullObjectByBytes() throws Exception {
+        String objectKey = "testReadingFull2";
+        int objectSize = 12345;
+        RandomInputStream toWrite = new RandomInputStream(0, objectSize);
+        RandomInputStream toCompare = new RandomInputStream(0, objectSize);
+
+        Payload payload = Payloads.newInputStreamPayload(toWrite);
+        payload.getContentMetadata().setContentLength((long) objectSize);
+        Blob blob = blobStore.blobBuilder(objectKey)
+            .payload(payload)
+            .contentLength((long) objectSize)
+            .build();
+        String ret = blobStore.putBlob(BUCKET, blob);
+        log.debug("put blob: {} in Bucket: {}, in blobStore, result: {}", objectKey, BUCKET, ret);
+
+        @Cleanup
+        BackedInputStream toTest = new BlobStoreBackedInputStreamImpl(blobStore, BUCKET, objectKey,
+                                                                 (key, md) -> {},
+                                                                 objectSize, 1000);
+        assertStreamsMatchByBytes(toTest, toCompare);
+    }
+
+    @Test(expectedExceptions = KeyNotFoundException.class)
+    public void testNotFoundOnRead() throws Exception {
+        @Cleanup
+        BackedInputStream toTest = new BlobStoreBackedInputStreamImpl(blobStore, BUCKET, "doesn't exist",
+                                                                 (key, md) -> {},
+                                                                 1234, 1000);
+        toTest.read();
+    }
+
+
+    @Test
+    public void testSeek() throws Exception {
+        String objectKey = "testSeek";
+        int objectSize = 12345;
+        RandomInputStream toWrite = new RandomInputStream(0, objectSize);
+
+        Map<Integer, InputStream> seeks = new HashMap<>();
+        Random r = new Random(12345);
+        for (int i = 0; i < 20; i++) {
+            int seek = r.nextInt(objectSize + 1);
+            RandomInputStream stream = new RandomInputStream(0, objectSize);
+            stream.skip(seek);
+            seeks.put(seek, stream);
+        }
+
+        Payload payload = Payloads.newInputStreamPayload(toWrite);
+        payload.getContentMetadata().setContentLength((long) objectSize);
+        Blob blob = blobStore.blobBuilder(objectKey)
+            .payload(payload)
+            .contentLength((long) objectSize)
+            .build();
+        String ret = blobStore.putBlob(BUCKET, blob);
+        log.debug("put blob: {} in Bucket: {}, in blobStore, result: {}", objectKey, BUCKET, ret);
+
+        @Cleanup
+        BackedInputStream toTest = new BlobStoreBackedInputStreamImpl(blobStore, BUCKET, objectKey,
+                                                                 (key, md) -> {},
+                                                                 objectSize, 1000);
+        for (Map.Entry<Integer, InputStream> e : seeks.entrySet()) {
+            toTest.seek(e.getKey());
+            assertStreamsMatch(toTest, e.getValue(), e.getKey().longValue());
+        }
+    }
+
+    @Test
+    public void testSeekWithinCurrent() throws Exception {
+        String objectKey = "testSeekWithinCurrent";
+        int objectSize = 12345;
+        RandomInputStream toWrite = new RandomInputStream(0, objectSize);
+
+        Payload payload = Payloads.newInputStreamPayload(toWrite);
+        payload.getContentMetadata().setContentLength((long) objectSize);
+        Blob blob = blobStore.blobBuilder(objectKey)
+            .payload(payload)
+            .contentLength((long) objectSize)
+            .build();
+        String ret = blobStore.putBlob(BUCKET, blob);
+        log.debug("put blob: {} in Bucket: {}, in blobStore, result: {}", objectKey, BUCKET, ret);
+
+        //BlobStore spiedBlobStore = spy(blobStore);
+        BlobStore spiedBlobStore = mock(BlobStore.class, delegatesTo(blobStore));
+
+        @Cleanup
+        BackedInputStream toTest = new BlobStoreBackedInputStreamImpl(spiedBlobStore, BUCKET, objectKey,
+                                                                 (key, md) -> {},
+                                                                 objectSize, 1000);
+
+        // seek forward
+        RandomInputStream firstSeek = new RandomInputStream(0, objectSize);
+        toTest.seek(100);
+        firstSeek.skip(100);
+        for (int i = 0; i < 100; i++) {
+            assertEquals(firstSeek.read(), toTest.read());
+        }
+
+        // seek forward a bit more, but in same block
+        RandomInputStream secondSeek = new RandomInputStream(0, objectSize);
+        toTest.seek(600);
+        secondSeek.skip(600);
+        for (int i = 0; i < 100; i++) {
+            assertEquals(secondSeek.read(), toTest.read());
+        }
+
+        // seek back
+        RandomInputStream thirdSeek = new RandomInputStream(0, objectSize);
+        toTest.seek(200);
+        thirdSeek.skip(200);
+        for (int i = 0; i < 100; i++) {
+            assertEquals(thirdSeek.read(), toTest.read());
+        }
+
+        verify(spiedBlobStore, times(1))
+            .getBlob(Mockito.eq(BUCKET), Mockito.eq(objectKey), ArgumentMatchers.any());
+    }
+
+    @Test
+    public void testSeekForward() throws Exception {
+        String objectKey = "testSeekForward";
+        int objectSize = 12345;
+        RandomInputStream toWrite = new RandomInputStream(0, objectSize);
+
+        Payload payload = Payloads.newInputStreamPayload(toWrite);
+        payload.getContentMetadata().setContentLength((long) objectSize);
+        Blob blob = blobStore.blobBuilder(objectKey)
+            .payload(payload)
+            .contentLength((long) objectSize)
+            .build();
+        String ret = blobStore.putBlob(BUCKET, blob);
+        log.debug("put blob: {} in Bucket: {}, in blobStore, result: {}", objectKey, BUCKET, ret);
+
+        @Cleanup
+        BackedInputStream toTest = new BlobStoreBackedInputStreamImpl(blobStore, BUCKET, objectKey,
+                                                                 (key, md) -> {},
+                                                                 objectSize, 1000);
+
+        // seek forward to middle
+        long middle = objectSize / 2;
+        toTest.seekForward(middle);
+
+        try {
+            long before = middle - objectSize / 4;
+            toTest.seekForward(before);
+            Assert.fail("Shound't be able to seek backwards");
+        } catch (IOException ioe) {
+            // correct
+        }
+
+        long after = middle + objectSize / 4;
+        RandomInputStream toCompare = new RandomInputStream(0, objectSize);
+        toCompare.skip(after);
+
+        toTest.seekForward(after);
+        assertStreamsMatch(toTest, toCompare, after);
+    }
+
+    @Test
+    public void testAvailable() throws IOException {
+        String objectKey = "testAvailable";
+        int objectSize = 2048;
+        RandomInputStream toWrite = new RandomInputStream(0, objectSize);
+        Payload payload = Payloads.newInputStreamPayload(toWrite);
+        payload.getContentMetadata().setContentLength((long) objectSize);
+        Blob blob = blobStore.blobBuilder(objectKey)
+            .payload(payload)
+            .contentLength(objectSize)
+            .build();
+        String ret = blobStore.putBlob(BUCKET, blob);
+        @Cleanup
+        BackedInputStream bis = new BlobStoreBackedInputStreamImpl(
+            blobStore, BUCKET, objectKey, (k, md) -> {}, objectSize, 512);
+        assertEquals(bis.available(), objectSize);
+        bis.seek(500);
+        assertEquals(bis.available(), objectSize - 500);
+        bis.seek(1024);
+        assertEquals(bis.available(), 1024);
+        bis.seek(2048);
+        assertEquals(bis.available(), 0);
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/BlobStoreTestBase.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/BlobStoreTestBase.java
index c5508ba5f6..2d5990027f 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/BlobStoreTestBase.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/BlobStoreTestBase.java
@@ -1,83 +1,83 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud;
-
-import org.jclouds.ContextBuilder;
-import org.jclouds.blobstore.BlobStore;
-import org.jclouds.blobstore.BlobStoreContext;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.testng.annotations.AfterMethod;
-import org.testng.annotations.BeforeMethod;
-
-public abstract class BlobStoreTestBase {
-
-    private static final Logger log = LoggerFactory.getLogger(BlobStoreTestBase.class);
-    public static final String BUCKET = "pulsar-unittest";
-
-    protected BlobStoreContext context = null;
-    protected BlobStore blobStore = null;
-
-    @BeforeMethod(alwaysRun = true)
-    public void start() throws Exception {
-        if (Boolean.parseBoolean(System.getProperty("testRealAWS", "false"))) {
-            log.info("TestReal AWS S3, bucket: {}", BUCKET);
-            // To use this, must config credentials using "aws_access_key_id" as S3ID,
-            // and "aws_secret_access_key" as S3Key. And bucket should exist in default region. e.g.
-            //        props.setProperty("S3ID", "AXXXXXXQ");
-            //        props.setProperty("S3Key", "HXXXXX+");
-            context = ContextBuilder.newBuilder("aws-s3")
-                .credentials(System.getProperty("S3ID"), System.getProperty("S3Key"))
-                .build(BlobStoreContext.class);
-            blobStore = context.getBlobStore();
-            // To use this, ~/.aws must be configured with credentials and a default region
-            //s3client = AmazonS3ClientBuilder.standard().build();
-        } else if (Boolean.parseBoolean(System.getProperty("testRealGCS", "false"))) {
-            log.info("TestReal GCS, bucket: {}", BUCKET);
-            // To use this, must config credentials using "client_email" as GCSID and "private_key" as GCSKey.
-            // And bucket should exist in default region. e.g.
-            //        props.setProperty("GCSID", "5XXXXXXXXXX6-compute@developer.gserviceaccount.com");
-            //        props.setProperty("GCSKey", "XXXXXX");
-            context = ContextBuilder.newBuilder("google-cloud-storage")
-                .credentials(System.getProperty("GCSID"), System.getProperty("GCSKey"))
-                .build(BlobStoreContext.class);
-            blobStore = context.getBlobStore();
-        } else {
-            log.info("Test Transient, bucket: {}", BUCKET);
-            context = ContextBuilder.newBuilder("transient").build(BlobStoreContext.class);
-            blobStore = context.getBlobStore();
-            boolean create = blobStore.createContainerInLocation(null, BUCKET);
-            log.debug("TestBase Create Bucket: {}, in blobStore, result: {}", BUCKET, create);
-        }
-    }
-
-    @AfterMethod(alwaysRun = true)
-    public void tearDown() {
-        if (blobStore != null
-            && (!Boolean.parseBoolean(System.getProperty("testRealAWS", "false"))
-             && !Boolean.parseBoolean(System.getProperty("testRealGCS", "false")))) {
-            blobStore.deleteContainer(BUCKET);
-        }
-
-        if (context != null) {
-            context.close();
-        }
-    }
-
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud;
+
+import org.jclouds.ContextBuilder;
+import org.jclouds.blobstore.BlobStore;
+import org.jclouds.blobstore.BlobStoreContext;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.testng.annotations.AfterMethod;
+import org.testng.annotations.BeforeMethod;
+
+public abstract class BlobStoreTestBase {
+
+    private static final Logger log = LoggerFactory.getLogger(BlobStoreTestBase.class);
+    public static final String BUCKET = "pulsar-unittest";
+
+    protected BlobStoreContext context = null;
+    protected BlobStore blobStore = null;
+
+    @BeforeMethod(alwaysRun = true)
+    public void start() throws Exception {
+        if (Boolean.parseBoolean(System.getProperty("testRealAWS", "false"))) {
+            log.info("TestReal AWS S3, bucket: {}", BUCKET);
+            // To use this, must config credentials using "aws_access_key_id" as S3ID,
+            // and "aws_secret_access_key" as S3Key. And bucket should exist in default region. e.g.
+            //        props.setProperty("S3ID", "AXXXXXXQ");
+            //        props.setProperty("S3Key", "HXXXXX+");
+            context = ContextBuilder.newBuilder("aws-s3")
+                .credentials(System.getProperty("S3ID"), System.getProperty("S3Key"))
+                .build(BlobStoreContext.class);
+            blobStore = context.getBlobStore();
+            // To use this, ~/.aws must be configured with credentials and a default region
+            //s3client = AmazonS3ClientBuilder.standard().build();
+        } else if (Boolean.parseBoolean(System.getProperty("testRealGCS", "false"))) {
+            log.info("TestReal GCS, bucket: {}", BUCKET);
+            // To use this, must config credentials using "client_email" as GCSID and "private_key" as GCSKey.
+            // And bucket should exist in default region. e.g.
+            //        props.setProperty("GCSID", "5XXXXXXXXXX6-compute@developer.gserviceaccount.com");
+            //        props.setProperty("GCSKey", "XXXXXX");
+            context = ContextBuilder.newBuilder("google-cloud-storage")
+                .credentials(System.getProperty("GCSID"), System.getProperty("GCSKey"))
+                .build(BlobStoreContext.class);
+            blobStore = context.getBlobStore();
+        } else {
+            log.info("Test Transient, bucket: {}", BUCKET);
+            context = ContextBuilder.newBuilder("transient").build(BlobStoreContext.class);
+            blobStore = context.getBlobStore();
+            boolean create = blobStore.createContainerInLocation(null, BUCKET);
+            log.debug("TestBase Create Bucket: {}, in blobStore, result: {}", BUCKET, create);
+        }
+    }
+
+    @AfterMethod(alwaysRun = true)
+    public void tearDown() {
+        if (blobStore != null
+            && (!Boolean.parseBoolean(System.getProperty("testRealAWS", "false"))
+             && !Boolean.parseBoolean(System.getProperty("testRealGCS", "false")))) {
+            blobStore.deleteContainer(BUCKET);
+        }
+
+        if (context != null) {
+            context.close();
+        }
+    }
+
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedInputStreamTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedInputStreamTest.java
index 9c3a4fb8fe..b2c17bb12c 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedInputStreamTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedInputStreamTest.java
@@ -1,52 +1,52 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static org.testng.Assert.assertEquals;
-import java.io.IOException;
-import java.io.InputStream;
-import lombok.Cleanup;
-import org.apache.bookkeeper.mledger.offload.jcloud.BlobStoreTestBase;
-import org.testng.annotations.Test;
-
-public class BlobStoreBackedInputStreamTest extends BlobStoreTestBase {
-
-    @Test
-    public void testFillBuffer() throws Exception {
-        @Cleanup
-        BlobStoreBackedInputStreamImpl bis = new BlobStoreBackedInputStreamImpl(
-            blobStore, BUCKET, "testFillBuffer", (k, md) -> {
-        }, 2048, 512);
-
-        InputStream is = new InputStream() {
-            int count = 10;
-
-            @Override
-            public int read() throws IOException {
-                if (count-- > 0) {
-                    return 1;
-                } else {
-                    return -1;
-                }
-            }
-        };
-        bis.fillBuffer(is, 20);
-        assertEquals(bis.getBuffer().readableBytes(), 10);
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static org.testng.Assert.assertEquals;
+import java.io.IOException;
+import java.io.InputStream;
+import lombok.Cleanup;
+import org.apache.bookkeeper.mledger.offload.jcloud.BlobStoreTestBase;
+import org.testng.annotations.Test;
+
+public class BlobStoreBackedInputStreamTest extends BlobStoreTestBase {
+
+    @Test
+    public void testFillBuffer() throws Exception {
+        @Cleanup
+        BlobStoreBackedInputStreamImpl bis = new BlobStoreBackedInputStreamImpl(
+            blobStore, BUCKET, "testFillBuffer", (k, md) -> {
+        }, 2048, 512);
+
+        InputStream is = new InputStream() {
+            int count = 10;
+
+            @Override
+            public int read() throws IOException {
+                if (count-- > 0) {
+                    return 1;
+                } else {
+                    return -1;
+                }
+            }
+        };
+        bis.fillBuffer(is, 20);
+        assertEquals(bis.getBuffer().readableBytes(), 10);
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedReadHandleImplTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedReadHandleImplTest.java
index bf116e5aec..dfecc60fb0 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedReadHandleImplTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreBackedReadHandleImplTest.java
@@ -1,212 +1,212 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static java.nio.charset.StandardCharsets.UTF_8;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-import static org.testng.Assert.assertEquals;
-import io.netty.buffer.ByteBuf;
-import io.netty.buffer.ByteBufAllocator;
-import java.io.EOFException;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Iterator;
-import java.util.List;
-import java.util.concurrent.Executors;
-import java.util.concurrent.ScheduledExecutorService;
-import java.util.concurrent.TimeUnit;
-import org.apache.bookkeeper.client.LedgerMetadataBuilder;
-import org.apache.bookkeeper.client.api.DigestType;
-import org.apache.bookkeeper.client.api.LedgerEntries;
-import org.apache.bookkeeper.client.api.LedgerEntry;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock;
-import org.apache.bookkeeper.net.BookieId;
-import org.apache.commons.lang3.tuple.Pair;
-import org.testng.annotations.AfterClass;
-import org.testng.annotations.DataProvider;
-import org.testng.annotations.Test;
-
-public class BlobStoreBackedReadHandleImplTest {
-
-    private OffsetsCache offsetsCache = new OffsetsCache();
-
-    private ScheduledExecutorService executor = Executors.newScheduledThreadPool(2);
-
-    @AfterClass
-    public void tearDown() throws Exception {
-        if (executor != null) {
-            executor.shutdown();
-            executor.awaitTermination(5, TimeUnit.SECONDS);
-        }
-        if (offsetsCache != null) {
-            offsetsCache.close();
-        }
-    }
-
-    @AfterClass
-    public void clearCache() throws Exception {
-        offsetsCache.clear();
-    }
-
-    private String getExpectedEntryContent(int entryId) {
-        return "Entry " + entryId;
-    }
-
-    private Pair<BlobStoreBackedReadHandleImpl, ByteBuf> createReadHandle(
-            long ledgerId, int entries, boolean hasDirtyData) throws Exception {
-        // Build data.
-        List<Pair<Integer, Integer>> offsets = new ArrayList<>();
-        int totalLen = 0;
-        ByteBuf data = ByteBufAllocator.DEFAULT.heapBuffer(1024);
-        data.writeInt(0);
-        data.writerIndex(128);
-        //data.readerIndex(128);
-        for (int i = 0; i < entries; i++) {
-            if (hasDirtyData && i == 1) {
-                data.writeBytes("dirty data".getBytes(UTF_8));
-            }
-            offsets.add(Pair.of(i, data.writerIndex()));
-            offsetsCache.put(ledgerId, i, data.writerIndex());
-            byte[] entryContent = getExpectedEntryContent(i).getBytes(UTF_8);
-            totalLen += entryContent.length;
-            data.writeInt(entryContent.length);
-            data.writeLong(i);
-            data.writeBytes(entryContent);
-        }
-        // Build metadata.
-        LedgerMetadata metadata = LedgerMetadataBuilder.create()
-                .withId(ledgerId)
-                .withEnsembleSize(1)
-                .withWriteQuorumSize(1)
-                .withAckQuorumSize(1)
-                .withDigestType(DigestType.CRC32C)
-                .withPassword("pwd".getBytes(UTF_8))
-                .withClosedState()
-                .withLastEntryId(entries)
-                .withLength(totalLen)
-                .newEnsembleEntry(0L, Arrays.asList(BookieId.parse("127.0.0.1:3181")))
-                .build();
-        BackedInputStreamImpl inputStream = new BackedInputStreamImpl(data);
-        // Since we have written data to "offsetsCache", the index will never be used.
-        OffloadIndexBlock mockIndex = mock(OffloadIndexBlock.class);
-        when(mockIndex.getLedgerMetadata()).thenReturn(metadata);
-        for (Pair<Integer, Integer> pair : offsets) {
-            when(mockIndex.getIndexEntryForEntry(pair.getLeft())).thenReturn(
-                    OffloadIndexEntryImpl.of(pair.getLeft(), 0, pair.getRight(), 0));
-        }
-        // Build obj.
-        return Pair.of(new BlobStoreBackedReadHandleImpl(ledgerId, mockIndex, inputStream, executor, offsetsCache),
-                data);
-    }
-
-    private static class BackedInputStreamImpl extends BackedInputStream {
-
-        private ByteBuf data;
-
-        private BackedInputStreamImpl(ByteBuf data){
-            this.data = data;
-        }
-
-        @Override
-        public void seek(long position) {
-            data.readerIndex((int) position);
-        }
-
-        @Override
-        public void seekForward(long position) throws IOException {
-            data.readerIndex((int) position);
-        }
-
-        @Override
-        public long getCurrentPosition() {
-            return data.readerIndex();
-        }
-
-        @Override
-        public int read() throws IOException {
-            if (data.readableBytes() == 0) {
-                throw new EOFException("The input-stream has no bytes to read");
-            }
-            return data.readByte();
-        }
-
-        @Override
-        public int available() throws IOException {
-            return data.readableBytes();
-        }
-    }
-
-    @DataProvider
-    public Object[][] streamStartAt() {
-        return new Object[][] {
-            // It gives a 0 value of the entry length.
-            { 0, false },
-            // It gives a 0 value of the entry length.
-            { 1, false },
-            // The first entry starts at 128.
-            { 128, false },
-            // It gives a 0 value of the entry length.
-            { 0, true },
-            // It gives a 0 value of the entry length.
-            { 1, true },
-            // The first entry starts at 128.
-            { 128, true }
-        };
-    }
-
-    @Test(dataProvider = "streamStartAt")
-    public void testRead(int streamStartAt, boolean hasDirtyData) throws Exception {
-        int entryCount = 5;
-        Pair<BlobStoreBackedReadHandleImpl, ByteBuf> ledgerDataPair =
-                createReadHandle(1, entryCount, hasDirtyData);
-        BlobStoreBackedReadHandleImpl ledger = ledgerDataPair.getLeft();
-        ByteBuf data = ledgerDataPair.getRight();
-        data.readerIndex(streamStartAt);
-        // Teat read each entry.
-        for (int i = 0; i < 5; i++) {
-            LedgerEntries entries = ledger.read(i, i);
-            assertEquals(new String(entries.iterator().next().getEntryBytes()), getExpectedEntryContent(i));
-        }
-        // Test read all entries.
-        LedgerEntries entries1 = ledger.read(0, entryCount - 1);
-        Iterator<LedgerEntry> iterator1 = entries1.iterator();
-        for (int i = 0; i < entryCount; i++) {
-            assertEquals(new String(iterator1.next().getEntryBytes()), getExpectedEntryContent(i));
-        }
-        // Test a special case.
-        // 1. Read from 0 to "lac - 1".
-        // 2. Any reading.
-        LedgerEntries entries2 = ledger.read(0, entryCount - 2);
-        Iterator<LedgerEntry> iterator2 = entries2.iterator();
-        for (int i = 0; i < entryCount - 1; i++) {
-            assertEquals(new String(iterator2.next().getEntryBytes()), getExpectedEntryContent(i));
-        }
-        LedgerEntries entries3 = ledger.read(0, entryCount - 1);
-        Iterator<LedgerEntry> iterator3 = entries3.iterator();
-        for (int i = 0; i < entryCount; i++) {
-            assertEquals(new String(iterator3.next().getEntryBytes()), getExpectedEntryContent(i));
-        }
-        // cleanup.
-        ledger.close();
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+import static org.testng.Assert.assertEquals;
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.ByteBufAllocator;
+import java.io.EOFException;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Iterator;
+import java.util.List;
+import java.util.concurrent.Executors;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.TimeUnit;
+import org.apache.bookkeeper.client.LedgerMetadataBuilder;
+import org.apache.bookkeeper.client.api.DigestType;
+import org.apache.bookkeeper.client.api.LedgerEntries;
+import org.apache.bookkeeper.client.api.LedgerEntry;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock;
+import org.apache.bookkeeper.net.BookieId;
+import org.apache.commons.lang3.tuple.Pair;
+import org.testng.annotations.AfterClass;
+import org.testng.annotations.DataProvider;
+import org.testng.annotations.Test;
+
+public class BlobStoreBackedReadHandleImplTest {
+
+    private OffsetsCache offsetsCache = new OffsetsCache();
+
+    private ScheduledExecutorService executor = Executors.newScheduledThreadPool(2);
+
+    @AfterClass
+    public void tearDown() throws Exception {
+        if (executor != null) {
+            executor.shutdown();
+            executor.awaitTermination(5, TimeUnit.SECONDS);
+        }
+        if (offsetsCache != null) {
+            offsetsCache.close();
+        }
+    }
+
+    @AfterClass
+    public void clearCache() throws Exception {
+        offsetsCache.clear();
+    }
+
+    private String getExpectedEntryContent(int entryId) {
+        return "Entry " + entryId;
+    }
+
+    private Pair<BlobStoreBackedReadHandleImpl, ByteBuf> createReadHandle(
+            long ledgerId, int entries, boolean hasDirtyData) throws Exception {
+        // Build data.
+        List<Pair<Integer, Integer>> offsets = new ArrayList<>();
+        int totalLen = 0;
+        ByteBuf data = ByteBufAllocator.DEFAULT.heapBuffer(1024);
+        data.writeInt(0);
+        data.writerIndex(128);
+        //data.readerIndex(128);
+        for (int i = 0; i < entries; i++) {
+            if (hasDirtyData && i == 1) {
+                data.writeBytes("dirty data".getBytes(UTF_8));
+            }
+            offsets.add(Pair.of(i, data.writerIndex()));
+            offsetsCache.put(ledgerId, i, data.writerIndex());
+            byte[] entryContent = getExpectedEntryContent(i).getBytes(UTF_8);
+            totalLen += entryContent.length;
+            data.writeInt(entryContent.length);
+            data.writeLong(i);
+            data.writeBytes(entryContent);
+        }
+        // Build metadata.
+        LedgerMetadata metadata = LedgerMetadataBuilder.create()
+                .withId(ledgerId)
+                .withEnsembleSize(1)
+                .withWriteQuorumSize(1)
+                .withAckQuorumSize(1)
+                .withDigestType(DigestType.CRC32C)
+                .withPassword("pwd".getBytes(UTF_8))
+                .withClosedState()
+                .withLastEntryId(entries)
+                .withLength(totalLen)
+                .newEnsembleEntry(0L, Arrays.asList(BookieId.parse("127.0.0.1:3181")))
+                .build();
+        BackedInputStreamImpl inputStream = new BackedInputStreamImpl(data);
+        // Since we have written data to "offsetsCache", the index will never be used.
+        OffloadIndexBlock mockIndex = mock(OffloadIndexBlock.class);
+        when(mockIndex.getLedgerMetadata()).thenReturn(metadata);
+        for (Pair<Integer, Integer> pair : offsets) {
+            when(mockIndex.getIndexEntryForEntry(pair.getLeft())).thenReturn(
+                    OffloadIndexEntryImpl.of(pair.getLeft(), 0, pair.getRight(), 0));
+        }
+        // Build obj.
+        return Pair.of(new BlobStoreBackedReadHandleImpl(ledgerId, mockIndex, inputStream, executor, offsetsCache),
+                data);
+    }
+
+    private static class BackedInputStreamImpl extends BackedInputStream {
+
+        private ByteBuf data;
+
+        private BackedInputStreamImpl(ByteBuf data){
+            this.data = data;
+        }
+
+        @Override
+        public void seek(long position) {
+            data.readerIndex((int) position);
+        }
+
+        @Override
+        public void seekForward(long position) throws IOException {
+            data.readerIndex((int) position);
+        }
+
+        @Override
+        public long getCurrentPosition() {
+            return data.readerIndex();
+        }
+
+        @Override
+        public int read() throws IOException {
+            if (data.readableBytes() == 0) {
+                throw new EOFException("The input-stream has no bytes to read");
+            }
+            return data.readByte();
+        }
+
+        @Override
+        public int available() throws IOException {
+            return data.readableBytes();
+        }
+    }
+
+    @DataProvider
+    public Object[][] streamStartAt() {
+        return new Object[][] {
+            // It gives a 0 value of the entry length.
+            { 0, false },
+            // It gives a 0 value of the entry length.
+            { 1, false },
+            // The first entry starts at 128.
+            { 128, false },
+            // It gives a 0 value of the entry length.
+            { 0, true },
+            // It gives a 0 value of the entry length.
+            { 1, true },
+            // The first entry starts at 128.
+            { 128, true }
+        };
+    }
+
+    @Test(dataProvider = "streamStartAt")
+    public void testRead(int streamStartAt, boolean hasDirtyData) throws Exception {
+        int entryCount = 5;
+        Pair<BlobStoreBackedReadHandleImpl, ByteBuf> ledgerDataPair =
+                createReadHandle(1, entryCount, hasDirtyData);
+        BlobStoreBackedReadHandleImpl ledger = ledgerDataPair.getLeft();
+        ByteBuf data = ledgerDataPair.getRight();
+        data.readerIndex(streamStartAt);
+        // Teat read each entry.
+        for (int i = 0; i < 5; i++) {
+            LedgerEntries entries = ledger.read(i, i);
+            assertEquals(new String(entries.iterator().next().getEntryBytes()), getExpectedEntryContent(i));
+        }
+        // Test read all entries.
+        LedgerEntries entries1 = ledger.read(0, entryCount - 1);
+        Iterator<LedgerEntry> iterator1 = entries1.iterator();
+        for (int i = 0; i < entryCount; i++) {
+            assertEquals(new String(iterator1.next().getEntryBytes()), getExpectedEntryContent(i));
+        }
+        // Test a special case.
+        // 1. Read from 0 to "lac - 1".
+        // 2. Any reading.
+        LedgerEntries entries2 = ledger.read(0, entryCount - 2);
+        Iterator<LedgerEntry> iterator2 = entries2.iterator();
+        for (int i = 0; i < entryCount - 1; i++) {
+            assertEquals(new String(iterator2.next().getEntryBytes()), getExpectedEntryContent(i));
+        }
+        LedgerEntries entries3 = ledger.read(0, entryCount - 1);
+        Iterator<LedgerEntry> iterator3 = entries3.iterator();
+        for (int i = 0; i < entryCount; i++) {
+            assertEquals(new String(iterator3.next().getEntryBytes()), getExpectedEntryContent(i));
+        }
+        // cleanup.
+        ledger.close();
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderBase.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderBase.java
index ee7af53e96..a39a5efc52 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderBase.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderBase.java
@@ -1,171 +1,171 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.function.Supplier;
-import org.apache.bookkeeper.client.BookKeeper;
-import org.apache.bookkeeper.client.LedgerHandle;
-import org.apache.bookkeeper.client.PulsarMockBookKeeper;
-import org.apache.bookkeeper.client.api.DigestType;
-import org.apache.bookkeeper.client.api.ReadHandle;
-import org.apache.bookkeeper.common.util.OrderedScheduler;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.JCloudBlobStoreProvider;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration;
-import org.apache.commons.lang3.StringUtils;
-import org.jclouds.blobstore.BlobStore;
-import org.jclouds.domain.Credentials;
-import org.testng.Assert;
-import org.testng.annotations.AfterClass;
-import org.testng.annotations.AfterMethod;
-
-public abstract class BlobStoreManagedLedgerOffloaderBase {
-
-    public static final String BUCKET = "pulsar-unittest";
-    protected static final int DEFAULT_BLOCK_SIZE = 5 * 1024 * 1024;
-    protected static final int DEFAULT_READ_BUFFER_SIZE = 1 * 1024 * 1024;
-
-    protected final OrderedScheduler scheduler;
-    protected final PulsarMockBookKeeper bk;
-    protected final JCloudBlobStoreProvider provider;
-    protected TieredStorageConfiguration config;
-    protected BlobStore blobStore = null;
-    protected final OffsetsCache entryOffsetsCache = new OffsetsCache();
-
-    protected BlobStoreManagedLedgerOffloaderBase() throws Exception {
-        scheduler = OrderedScheduler.newSchedulerBuilder().numThreads(5).name("offloader").build();
-        bk = new PulsarMockBookKeeper(scheduler);
-        provider = getBlobStoreProvider();
-    }
-
-    @AfterMethod(alwaysRun = true)
-    public void cleanupMockBookKeeper() {
-        bk.getLedgerMap().clear();
-        entryOffsetsCache.clear();
-    }
-
-    @AfterClass(alwaysRun = true)
-    public void cleanup() throws Exception {
-        entryOffsetsCache.close();
-        scheduler.shutdownNow();
-    }
-
-    protected static MockManagedLedger createMockManagedLedger() {
-        return new MockManagedLedger();
-    }
-
-    /*
-     * Determine which BlobStore Provider to test based on the System properties
-     */
-    protected static JCloudBlobStoreProvider getBlobStoreProvider() {
-        if (Boolean.parseBoolean(System.getProperty("testRealAWS", "false"))) {
-            return JCloudBlobStoreProvider.AWS_S3;
-        } else if (Boolean.parseBoolean(System.getProperty("testRealGCS", "false"))) {
-            return JCloudBlobStoreProvider.GOOGLE_CLOUD_STORAGE;
-        } else {
-            return JCloudBlobStoreProvider.TRANSIENT;
-        }
-    }
-
-    /*
-     * Get the credentials to use for the JCloud provider
-     * based on the System properties.
-     */
-    protected static Supplier<Credentials> getBlobStoreCredentials() {
-        if (Boolean.parseBoolean(System.getProperty("testRealAWS", "false"))) {
-            /* To use this, must config credentials using "aws_access_key_id" as S3ID,
-             *  and "aws_secret_access_key" as S3Key. And bucket should exist in default region. e.g.
-             *      props.setProperty("S3ID", "AXXXXXXQ");
-             *      props.setProperty("S3Key", "HXXXXX+");
-             */
-            return () -> new Credentials(System.getProperty("S3ID"), System.getProperty("S3Key"));
-
-        } else if (Boolean.parseBoolean(System.getProperty("testRealGCS", "false"))) {
-            /*
-             * To use this, must config credentials using "client_email" as GCSID and "private_key" as GCSKey.
-             * And bucket should exist in default region. e.g.
-             *        props.setProperty("GCSID", "5XXXXXXXXXX6-compute@developer.gserviceaccount.com");
-             *        props.setProperty("GCSKey", "XXXXXX");
-             */
-            return () -> new Credentials(System.getProperty("GCSID"), System.getProperty("GCSKey"));
-        } else {
-            return null;
-        }
-    }
-
-    protected TieredStorageConfiguration getConfiguration(String bucket) {
-        return getConfiguration(bucket, null);
-    }
-
-    protected TieredStorageConfiguration getConfiguration(String bucket, Map<String, String> additionalConfig) {
-        Map<String, String> metaData = new HashMap<String, String>();
-        if (additionalConfig != null) {
-            metaData.putAll(additionalConfig);
-        }
-        metaData.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, provider.getDriver());
-        metaData.put(getConfigKey(TieredStorageConfiguration.METADATA_FIELD_REGION), "");
-        metaData.put(getConfigKey(TieredStorageConfiguration.METADATA_FIELD_BUCKET), bucket);
-        metaData.put(getConfigKey(TieredStorageConfiguration.METADATA_FIELD_ENDPOINT), "");
-
-        TieredStorageConfiguration config = TieredStorageConfiguration.create(metaData);
-        config.setProviderCredentials(getBlobStoreCredentials());
-
-        return config;
-    }
-
-    private String getConfigKey(String field) {
-        return TieredStorageConfiguration.OFFLOADER_PROPERTY_PREFIX + StringUtils.capitalize(field);
-    }
-
-    protected ReadHandle buildReadHandle() throws Exception {
-        return buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
-    }
-
-    protected ReadHandle buildReadHandle(int maxBlockSize, int blockCount) throws Exception {
-        Assert.assertTrue(maxBlockSize > DataBlockHeaderImpl.getDataStartOffset());
-
-        LedgerHandle lh = bk.createLedger(1, 1, 1, BookKeeper.DigestType.CRC32, "foobar".getBytes());
-
-        int i = 0;
-        int bytesWrittenCurrentBlock = DataBlockHeaderImpl.getDataStartOffset();
-        int blocksWritten = 1;
-
-        while (blocksWritten < blockCount
-               || bytesWrittenCurrentBlock < maxBlockSize / 2) {
-            byte[] entry = ("foobar" + i).getBytes();
-            int sizeInBlock = entry.length + 12 /* ENTRY_HEADER_SIZE */;
-
-            if (bytesWrittenCurrentBlock + sizeInBlock > maxBlockSize) {
-                bytesWrittenCurrentBlock = DataBlockHeaderImpl.getDataStartOffset();
-                blocksWritten++;
-            }
-
-            lh.addEntry(entry);
-            bytesWrittenCurrentBlock += sizeInBlock;
-            i++;
-        }
-
-        lh.close();
-
-        return bk.newOpenLedgerOp().withLedgerId(lh.getId())
-            .withPassword("foobar".getBytes()).withDigestType(DigestType.CRC32).execute().get();
-    }
-
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.function.Supplier;
+import org.apache.bookkeeper.client.BookKeeper;
+import org.apache.bookkeeper.client.LedgerHandle;
+import org.apache.bookkeeper.client.PulsarMockBookKeeper;
+import org.apache.bookkeeper.client.api.DigestType;
+import org.apache.bookkeeper.client.api.ReadHandle;
+import org.apache.bookkeeper.common.util.OrderedScheduler;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.JCloudBlobStoreProvider;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration;
+import org.apache.commons.lang3.StringUtils;
+import org.jclouds.blobstore.BlobStore;
+import org.jclouds.domain.Credentials;
+import org.testng.Assert;
+import org.testng.annotations.AfterClass;
+import org.testng.annotations.AfterMethod;
+
+public abstract class BlobStoreManagedLedgerOffloaderBase {
+
+    public static final String BUCKET = "pulsar-unittest";
+    protected static final int DEFAULT_BLOCK_SIZE = 5 * 1024 * 1024;
+    protected static final int DEFAULT_READ_BUFFER_SIZE = 1 * 1024 * 1024;
+
+    protected final OrderedScheduler scheduler;
+    protected final PulsarMockBookKeeper bk;
+    protected final JCloudBlobStoreProvider provider;
+    protected TieredStorageConfiguration config;
+    protected BlobStore blobStore = null;
+    protected final OffsetsCache entryOffsetsCache = new OffsetsCache();
+
+    protected BlobStoreManagedLedgerOffloaderBase() throws Exception {
+        scheduler = OrderedScheduler.newSchedulerBuilder().numThreads(5).name("offloader").build();
+        bk = new PulsarMockBookKeeper(scheduler);
+        provider = getBlobStoreProvider();
+    }
+
+    @AfterMethod(alwaysRun = true)
+    public void cleanupMockBookKeeper() {
+        bk.getLedgerMap().clear();
+        entryOffsetsCache.clear();
+    }
+
+    @AfterClass(alwaysRun = true)
+    public void cleanup() throws Exception {
+        entryOffsetsCache.close();
+        scheduler.shutdownNow();
+    }
+
+    protected static MockManagedLedger createMockManagedLedger() {
+        return new MockManagedLedger();
+    }
+
+    /*
+     * Determine which BlobStore Provider to test based on the System properties
+     */
+    protected static JCloudBlobStoreProvider getBlobStoreProvider() {
+        if (Boolean.parseBoolean(System.getProperty("testRealAWS", "false"))) {
+            return JCloudBlobStoreProvider.AWS_S3;
+        } else if (Boolean.parseBoolean(System.getProperty("testRealGCS", "false"))) {
+            return JCloudBlobStoreProvider.GOOGLE_CLOUD_STORAGE;
+        } else {
+            return JCloudBlobStoreProvider.TRANSIENT;
+        }
+    }
+
+    /*
+     * Get the credentials to use for the JCloud provider
+     * based on the System properties.
+     */
+    protected static Supplier<Credentials> getBlobStoreCredentials() {
+        if (Boolean.parseBoolean(System.getProperty("testRealAWS", "false"))) {
+            /* To use this, must config credentials using "aws_access_key_id" as S3ID,
+             *  and "aws_secret_access_key" as S3Key. And bucket should exist in default region. e.g.
+             *      props.setProperty("S3ID", "AXXXXXXQ");
+             *      props.setProperty("S3Key", "HXXXXX+");
+             */
+            return () -> new Credentials(System.getProperty("S3ID"), System.getProperty("S3Key"));
+
+        } else if (Boolean.parseBoolean(System.getProperty("testRealGCS", "false"))) {
+            /*
+             * To use this, must config credentials using "client_email" as GCSID and "private_key" as GCSKey.
+             * And bucket should exist in default region. e.g.
+             *        props.setProperty("GCSID", "5XXXXXXXXXX6-compute@developer.gserviceaccount.com");
+             *        props.setProperty("GCSKey", "XXXXXX");
+             */
+            return () -> new Credentials(System.getProperty("GCSID"), System.getProperty("GCSKey"));
+        } else {
+            return null;
+        }
+    }
+
+    protected TieredStorageConfiguration getConfiguration(String bucket) {
+        return getConfiguration(bucket, null);
+    }
+
+    protected TieredStorageConfiguration getConfiguration(String bucket, Map<String, String> additionalConfig) {
+        Map<String, String> metaData = new HashMap<String, String>();
+        if (additionalConfig != null) {
+            metaData.putAll(additionalConfig);
+        }
+        metaData.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, provider.getDriver());
+        metaData.put(getConfigKey(TieredStorageConfiguration.METADATA_FIELD_REGION), "");
+        metaData.put(getConfigKey(TieredStorageConfiguration.METADATA_FIELD_BUCKET), bucket);
+        metaData.put(getConfigKey(TieredStorageConfiguration.METADATA_FIELD_ENDPOINT), "");
+
+        TieredStorageConfiguration config = TieredStorageConfiguration.create(metaData);
+        config.setProviderCredentials(getBlobStoreCredentials());
+
+        return config;
+    }
+
+    private String getConfigKey(String field) {
+        return TieredStorageConfiguration.OFFLOADER_PROPERTY_PREFIX + StringUtils.capitalize(field);
+    }
+
+    protected ReadHandle buildReadHandle() throws Exception {
+        return buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
+    }
+
+    protected ReadHandle buildReadHandle(int maxBlockSize, int blockCount) throws Exception {
+        Assert.assertTrue(maxBlockSize > DataBlockHeaderImpl.getDataStartOffset());
+
+        LedgerHandle lh = bk.createLedger(1, 1, 1, BookKeeper.DigestType.CRC32, "foobar".getBytes());
+
+        int i = 0;
+        int bytesWrittenCurrentBlock = DataBlockHeaderImpl.getDataStartOffset();
+        int blocksWritten = 1;
+
+        while (blocksWritten < blockCount
+               || bytesWrittenCurrentBlock < maxBlockSize / 2) {
+            byte[] entry = ("foobar" + i).getBytes();
+            int sizeInBlock = entry.length + 12 /* ENTRY_HEADER_SIZE */;
+
+            if (bytesWrittenCurrentBlock + sizeInBlock > maxBlockSize) {
+                bytesWrittenCurrentBlock = DataBlockHeaderImpl.getDataStartOffset();
+                blocksWritten++;
+            }
+
+            lh.addEntry(entry);
+            bytesWrittenCurrentBlock += sizeInBlock;
+            i++;
+        }
+
+        lh.close();
+
+        return bk.newOpenLedgerOp().withLedgerId(lh.getId())
+            .withPassword("foobar".getBytes()).withDigestType(DigestType.CRC32).execute().get();
+    }
+
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderStreamingTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderStreamingTest.java
index 85889cde90..537eaec15d 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderStreamingTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderStreamingTest.java
@@ -1,534 +1,534 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static org.apache.bookkeeper.client.api.BKException.Code.NoSuchLedgerExistsException;
-import static org.mockito.AdditionalAnswers.delegatesTo;
-import static org.mockito.Mockito.mock;
-import static org.testng.Assert.assertEquals;
-import static org.testng.Assert.assertNotNull;
-import static org.testng.Assert.fail;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.LinkedList;
-import java.util.Map;
-import java.util.Random;
-import java.util.UUID;
-import lombok.Cleanup;
-import org.apache.bookkeeper.client.BKException;
-import org.apache.bookkeeper.client.api.LedgerEntries;
-import org.apache.bookkeeper.client.api.LedgerEntry;
-import org.apache.bookkeeper.client.api.ReadHandle;
-import org.apache.bookkeeper.mledger.Entry;
-import org.apache.bookkeeper.mledger.LedgerOffloader;
-import org.apache.bookkeeper.mledger.LedgerOffloader.OffloadHandle;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.bookkeeper.mledger.ManagedLedger;
-import org.apache.bookkeeper.mledger.impl.EntryImpl;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.JCloudBlobStoreProvider;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration;
-import org.apache.bookkeeper.mledger.proto.MLDataFormats;
-import org.apache.bookkeeper.mledger.proto.MLDataFormats.OffloadContext;
-import org.jclouds.blobstore.BlobStore;
-import org.mockito.Mockito;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.testng.Assert;
-import org.testng.annotations.Test;
-
-public class BlobStoreManagedLedgerOffloaderStreamingTest extends BlobStoreManagedLedgerOffloaderBase {
-
-    private static final Logger log = LoggerFactory.getLogger(BlobStoreManagedLedgerOffloaderStreamingTest.class);
-    private TieredStorageConfiguration mockedConfig;
-    private static final Random random = new Random();
-    private final LedgerOffloaderStats offloaderStats;
-
-    BlobStoreManagedLedgerOffloaderStreamingTest() throws Exception {
-        super();
-        config = getConfiguration(BUCKET);
-        JCloudBlobStoreProvider provider = getBlobStoreProvider();
-        assertNotNull(provider);
-        provider.validate(config);
-        blobStore = provider.getBlobStore(config);
-        this.offloaderStats = LedgerOffloaderStats.create(false, false, null, 0);
-    }
-
-    private BlobStoreManagedLedgerOffloader getOffloader(Map<String, String> additionalConfig) throws IOException {
-        return getOffloader(BUCKET, additionalConfig);
-    }
-
-    private BlobStoreManagedLedgerOffloader getOffloader(BlobStore mockedBlobStore,
-                                                         Map<String, String> additionalConfig) throws IOException {
-        return getOffloader(BUCKET, mockedBlobStore, additionalConfig);
-    }
-
-    private BlobStoreManagedLedgerOffloader getOffloader(String bucket, Map<String, String> additionalConfig) throws
-            IOException {
-        mockedConfig = mock(TieredStorageConfiguration.class, delegatesTo(getConfiguration(bucket, additionalConfig)));
-        Mockito.doReturn(blobStore).when(mockedConfig).getBlobStore(); // Use the REAL blobStore
-        BlobStoreManagedLedgerOffloader offloader = BlobStoreManagedLedgerOffloader
-                .create(mockedConfig, new HashMap<String, String>(), scheduler, scheduler,
-                        this.offloaderStats, entryOffsetsCache);
-        return offloader;
-    }
-
-    private BlobStoreManagedLedgerOffloader getOffloader(String bucket, BlobStore mockedBlobStore,
-                                                         Map<String, String> additionalConfig) throws IOException {
-        mockedConfig = mock(TieredStorageConfiguration.class, delegatesTo(getConfiguration(bucket, additionalConfig)));
-        Mockito.doReturn(mockedBlobStore).when(mockedConfig).getBlobStore();
-        BlobStoreManagedLedgerOffloader offloader = BlobStoreManagedLedgerOffloader
-                .create(mockedConfig, new HashMap<String, String>(), scheduler, scheduler,
-                        this.offloaderStats, entryOffsetsCache);
-        return offloader;
-    }
-
-    @Test
-    public void testHappyCase() throws Exception {
-        @Cleanup
-        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
-            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
-        }});
-        ManagedLedger ml = createMockManagedLedger();
-        UUID uuid = UUID.randomUUID();
-        long beginLedger = 0;
-        long beginEntry = 0;
-        log.error("try begin offload");
-        @Cleanup
-        OffloadHandle offloadHandle = offloader
-                .streamingOffload(ml, uuid, beginLedger, beginEntry, new HashMap<>()).get();
-        //Segment should closed because size in bytes full
-        for (int i = 0; i < 10; i++) {
-            final byte[] data = new byte[100];
-            random.nextBytes(data);
-            final OffloadHandle.OfferEntryResult offerEntryResult = offloadHandle
-                    .offerEntry(EntryImpl.create(0, i, data));
-            log.info("offer result: {}", offerEntryResult);
-        }
-        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
-        log.info("Offload reasult: {}", offloadResult);
-    }
-
-    @Test
-    public void testReadAndWrite() throws Exception {
-        @Cleanup
-        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
-            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
-        }});
-        ManagedLedger ml = createMockManagedLedger();
-        UUID uuid = UUID.randomUUID();
-        long beginLedger = 0;
-        long beginEntry = 0;
-
-        Map<String, String> driverMeta = new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.METADATA_FIELD_BUCKET, BUCKET);
-        }};
-        @Cleanup
-        OffloadHandle offloadHandle = offloader
-                .streamingOffload(ml, uuid, beginLedger, beginEntry, driverMeta).get();
-
-        //Segment should closed because size in bytes full
-        final LinkedList<Entry> entries = new LinkedList<>();
-        for (int i = 0; i < 10; i++) {
-            final byte[] data = new byte[100];
-            random.nextBytes(data);
-            final EntryImpl entry = EntryImpl.create(0, i, data);
-            offloadHandle.offerEntry(entry);
-            entries.add(entry);
-        }
-
-        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
-        assertEquals(offloadResult.endLedger, 0);
-        assertEquals(offloadResult.endEntry, 9);
-        final OffloadContext.Builder contextBuilder = OffloadContext.newBuilder();
-        contextBuilder.addOffloadSegment(
-                MLDataFormats.OffloadSegment.newBuilder()
-                        .setUidLsb(uuid.getLeastSignificantBits())
-                        .setUidMsb(uuid.getMostSignificantBits())
-                        .setComplete(true).setEndEntryId(9).build());
-
-        @Cleanup
-        final ReadHandle readHandle = offloader.readOffloaded(0, contextBuilder.build(), driverMeta).get();
-        @Cleanup
-        final LedgerEntries ledgerEntries = readHandle.readAsync(0, 9).get();
-
-        for (LedgerEntry ledgerEntry : ledgerEntries) {
-            final EntryImpl storedEntry = (EntryImpl) entries.get((int) ledgerEntry.getEntryId());
-            final byte[] storedData = storedEntry.getData();
-            final byte[] entryBytes = ledgerEntry.getEntryBytes();
-            assertEquals(storedData, entryBytes);
-        }
-    }
-
-    @Test
-    public void testReadAndWriteAcrossLedger() throws Exception {
-        @Cleanup
-        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "2000");
-            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
-        }});
-        ManagedLedger ml = createMockManagedLedger();
-        UUID uuid = UUID.randomUUID();
-        long beginLedger = 0;
-        long beginEntry = 0;
-
-        Map<String, String> driverMeta = new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.METADATA_FIELD_BUCKET, BUCKET);
-        }};
-        @Cleanup
-        OffloadHandle offloadHandle = offloader
-                .streamingOffload(ml, uuid, beginLedger, beginEntry, driverMeta).get();
-
-        //Segment should closed because size in bytes full
-        final LinkedList<Entry> entries = new LinkedList<>();
-        final LinkedList<Entry> ledger2Entries = new LinkedList<>();
-        for (int i = 0; i < 10; i++) {
-            final byte[] data = new byte[100];
-            random.nextBytes(data);
-            final EntryImpl entry = EntryImpl.create(0, i, data);
-            offloadHandle.offerEntry(entry);
-            entries.add(entry);
-        }
-        for (int i = 0; i < 10; i++) {
-            final byte[] data = new byte[100];
-            random.nextBytes(data);
-            final EntryImpl entry = EntryImpl.create(1, i, data);
-            offloadHandle.offerEntry(entry);
-            ledger2Entries.add(entry);
-        }
-
-        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
-        assertEquals(offloadResult.endLedger, 1);
-        assertEquals(offloadResult.endEntry, 9);
-        final OffloadContext.Builder contextBuilder = OffloadContext.newBuilder();
-        contextBuilder.addOffloadSegment(
-                MLDataFormats.OffloadSegment.newBuilder()
-                        .setUidLsb(uuid.getLeastSignificantBits())
-                        .setUidMsb(uuid.getMostSignificantBits())
-                        .setComplete(true).setEndEntryId(9).build());
-
-        @Cleanup
-        final ReadHandle readHandle = offloader.readOffloaded(0, contextBuilder.build(), driverMeta).get();
-        @Cleanup
-        final LedgerEntries ledgerEntries = readHandle.readAsync(0, 9).get();
-
-        for (LedgerEntry ledgerEntry : ledgerEntries) {
-            final EntryImpl storedEntry = (EntryImpl) entries.get((int) ledgerEntry.getEntryId());
-            final byte[] storedData = storedEntry.getData();
-            final byte[] entryBytes = ledgerEntry.getEntryBytes();
-            assertEquals(storedData, entryBytes);
-        }
-
-        @Cleanup
-        final ReadHandle readHandle2 = offloader.readOffloaded(1, contextBuilder.build(), driverMeta).get();
-        @Cleanup
-        final LedgerEntries ledgerEntries2 = readHandle2.readAsync(0, 9).get();
-
-        for (LedgerEntry ledgerEntry : ledgerEntries2) {
-            final EntryImpl storedEntry = (EntryImpl) ledger2Entries.get((int) ledgerEntry.getEntryId());
-            final byte[] storedData = storedEntry.getData();
-            final byte[] entryBytes = ledgerEntry.getEntryBytes();
-            assertEquals(storedData, entryBytes);
-        }
-    }
-
-    @Test
-    public void testReadAndWriteAcrossSegment() throws Exception {
-        @Cleanup
-        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
-            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
-        }});
-        @Cleanup
-        LedgerOffloader offloader2 = getOffloader(new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
-            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
-        }});
-        ManagedLedger ml = createMockManagedLedger();
-        UUID uuid = UUID.randomUUID();
-        UUID uuid2 = UUID.randomUUID();
-        long beginLedger = 0;
-        long beginEntry = 0;
-
-        Map<String, String> driverMeta = new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.METADATA_FIELD_BUCKET, BUCKET);
-        }};
-        @Cleanup
-        OffloadHandle offloadHandle = offloader
-                .streamingOffload(ml, uuid, beginLedger, beginEntry, driverMeta).get();
-
-        //Segment should closed because size in bytes full
-        final LinkedList<Entry> entries = new LinkedList<>();
-        for (int i = 0; i < 10; i++) {
-            final byte[] data = new byte[100];
-            random.nextBytes(data);
-            final EntryImpl entry = EntryImpl.create(0, i, data);
-            offloadHandle.offerEntry(entry);
-            entries.add(entry);
-        }
-
-        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
-        assertEquals(offloadResult.endLedger, 0);
-        assertEquals(offloadResult.endEntry, 9);
-
-        //Segment should closed because size in bytes full
-        @Cleanup
-        OffloadHandle offloadHandle2 = offloader2
-                .streamingOffload(ml, uuid2, beginLedger, 10, driverMeta).get();
-        for (int i = 0; i < 10; i++) {
-            final byte[] data = new byte[100];
-            random.nextBytes(data);
-            final EntryImpl entry = EntryImpl.create(0, i + 10, data);
-            offloadHandle2.offerEntry(entry);
-            entries.add(entry);
-        }
-        final LedgerOffloader.OffloadResult offloadResult2 = offloadHandle2.getOffloadResultAsync().get();
-        assertEquals(offloadResult2.endLedger, 0);
-        assertEquals(offloadResult2.endEntry, 19);
-
-        final OffloadContext.Builder contextBuilder = OffloadContext.newBuilder();
-        contextBuilder.addOffloadSegment(
-                MLDataFormats.OffloadSegment.newBuilder()
-                        .setUidLsb(uuid.getLeastSignificantBits())
-                        .setUidMsb(uuid.getMostSignificantBits())
-                        .setComplete(true).setEndEntryId(9).build()).addOffloadSegment(
-                MLDataFormats.OffloadSegment.newBuilder()
-                        .setUidLsb(uuid2.getLeastSignificantBits())
-                        .setUidMsb(uuid2.getMostSignificantBits())
-                        .setComplete(true).setEndEntryId(19).build()
-        );
-
-        @Cleanup
-        final ReadHandle readHandle = offloader.readOffloaded(0, contextBuilder.build(), driverMeta).get();
-        @Cleanup
-        final LedgerEntries ledgerEntries = readHandle.readAsync(0, 19).get();
-
-        for (LedgerEntry ledgerEntry : ledgerEntries) {
-            final EntryImpl storedEntry = (EntryImpl) entries.get((int) ledgerEntry.getEntryId());
-            final byte[] storedData = storedEntry.getData();
-            final byte[] entryBytes = ledgerEntry.getEntryBytes();
-            assertEquals(storedData, entryBytes);
-        }
-    }
-
-    @Test
-    public void testRandomRead() throws Exception {
-        @Cleanup
-        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
-            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
-        }});
-        LedgerOffloader offloader2 = getOffloader(new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
-            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
-        }});
-        ManagedLedger ml = createMockManagedLedger();
-        UUID uuid = UUID.randomUUID();
-        UUID uuid2 = UUID.randomUUID();
-        long beginLedger = 0;
-        long beginEntry = 0;
-
-        Map<String, String> driverMeta = new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.METADATA_FIELD_BUCKET, BUCKET);
-        }};
-        @Cleanup
-        OffloadHandle offloadHandle = offloader
-                .streamingOffload(ml, uuid, beginLedger, beginEntry, driverMeta).get();
-
-        //Segment should closed because size in bytes full
-        final LinkedList<Entry> entries = new LinkedList<>();
-        for (int i = 0; i < 10; i++) {
-            final byte[] data = new byte[100];
-            random.nextBytes(data);
-            final EntryImpl entry = EntryImpl.create(0, i, data);
-            offloadHandle.offerEntry(entry);
-            entries.add(entry);
-        }
-
-        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
-        assertEquals(offloadResult.endLedger, 0);
-        assertEquals(offloadResult.endEntry, 9);
-
-        //Segment should closed because size in bytes full
-        @Cleanup
-        OffloadHandle offloadHandle2 = offloader2
-                .streamingOffload(ml, uuid2, beginLedger, 10, driverMeta).get();
-        for (int i = 0; i < 10; i++) {
-            final byte[] data = new byte[100];
-            random.nextBytes(data);
-            final EntryImpl entry = EntryImpl.create(0, i + 10, data);
-            offloadHandle2.offerEntry(entry);
-            entries.add(entry);
-        }
-        final LedgerOffloader.OffloadResult offloadResult2 = offloadHandle2.getOffloadResultAsync().get();
-        assertEquals(offloadResult2.endLedger, 0);
-        assertEquals(offloadResult2.endEntry, 19);
-
-        final OffloadContext.Builder contextBuilder = OffloadContext.newBuilder();
-        contextBuilder.addOffloadSegment(
-                MLDataFormats.OffloadSegment.newBuilder()
-                        .setUidLsb(uuid.getLeastSignificantBits())
-                        .setUidMsb(uuid.getMostSignificantBits())
-                        .setComplete(true).setEndEntryId(9).build()).addOffloadSegment(
-                MLDataFormats.OffloadSegment.newBuilder()
-                        .setUidLsb(uuid2.getLeastSignificantBits())
-                        .setUidMsb(uuid2.getMostSignificantBits())
-                        .setComplete(true).setEndEntryId(19).build()
-        );
-
-        @Cleanup
-        final ReadHandle readHandle = offloader.readOffloaded(0, contextBuilder.build(), driverMeta).get();
-
-        for (int i = 0; i <= 19; i++) {
-            Random seed = new Random(0);
-            int begin = seed.nextInt(20);
-            int end = seed.nextInt(20);
-            if (begin >= end) {
-                int temp = begin;
-                begin = end;
-                end = temp;
-            }
-            @Cleanup
-            final LedgerEntries ledgerEntries = readHandle.readAsync(begin, end).get();
-            for (LedgerEntry ledgerEntry : ledgerEntries) {
-                final EntryImpl storedEntry = (EntryImpl) entries.get((int) ledgerEntry.getEntryId());
-                final byte[] storedData = storedEntry.getData();
-                final byte[] entryBytes = ledgerEntry.getEntryBytes();
-                assertEquals(storedData, entryBytes);
-            }
-        }
-    }
-
-    @Test
-    public void testInvalidEntryIds() throws Exception {
-        @Cleanup
-        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
-            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
-        }});
-        ManagedLedger ml = createMockManagedLedger();
-        UUID uuid = UUID.randomUUID();
-        long beginLedger = 0;
-        long beginEntry = 0;
-
-        Map<String, String> driverMeta = new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.METADATA_FIELD_BUCKET, BUCKET);
-        }};
-        @Cleanup
-        OffloadHandle offloadHandle = offloader
-                .streamingOffload(ml, uuid, beginLedger, beginEntry, driverMeta).get();
-
-        //Segment should closed because size in bytes full
-        final LinkedList<Entry> entries = new LinkedList<>();
-        for (int i = 0; i < 10; i++) {
-            final byte[] data = new byte[100];
-            random.nextBytes(data);
-            final EntryImpl entry = EntryImpl.create(0, i, data);
-            offloadHandle.offerEntry(entry);
-            entries.add(entry);
-        }
-
-        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
-        assertEquals(offloadResult.endLedger, 0);
-        assertEquals(offloadResult.endEntry, 9);
-        final OffloadContext.Builder contextBuilder = OffloadContext.newBuilder();
-        contextBuilder.addOffloadSegment(
-                MLDataFormats.OffloadSegment.newBuilder()
-                        .setUidLsb(uuid.getLeastSignificantBits())
-                        .setUidMsb(uuid.getMostSignificantBits())
-                        .setComplete(true).setEndEntryId(9).build());
-
-        @Cleanup
-        final ReadHandle readHandle = offloader.readOffloaded(0, contextBuilder.build(), driverMeta).get();
-        try {
-            readHandle.read(-1, -1);
-            Assert.fail("Shouldn't be able to read anything");
-        } catch (Exception e) {
-        }
-
-        try {
-            readHandle.read(0, 20);
-            Assert.fail("Shouldn't be able to read anything");
-        } catch (Exception e) {
-        }
-    }
-
-    @Test
-    public void testReadNotExistLedger() throws Exception {
-        @Cleanup
-        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
-            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
-            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
-        }});
-        ManagedLedger ml = createMockManagedLedger();
-        UUID uuid = UUID.randomUUID();
-        long beginLedger = 0;
-        long beginEntry = 0;
-
-        Map<String, String> driverMeta = new HashMap<String, String>() {{
-            put(TieredStorageConfiguration.METADATA_FIELD_BUCKET, BUCKET);
-        }};
-        @Cleanup
-        OffloadHandle offloadHandle = offloader
-                .streamingOffload(ml, uuid, beginLedger, beginEntry, driverMeta).get();
-
-        // Segment should closed because size in bytes full
-        final LinkedList<Entry> entries = new LinkedList<>();
-        for (int i = 0; i < 10; i++) {
-            final byte[] data = new byte[100];
-            random.nextBytes(data);
-            final EntryImpl entry = EntryImpl.create(0, i, data);
-            offloadHandle.offerEntry(entry);
-            entries.add(entry);
-        }
-
-        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
-        assertEquals(offloadResult.endLedger, 0);
-        assertEquals(offloadResult.endEntry, 9);
-        final OffloadContext.Builder contextBuilder = OffloadContext.newBuilder();
-        contextBuilder.addOffloadSegment(
-                MLDataFormats.OffloadSegment.newBuilder()
-                        .setUidLsb(uuid.getLeastSignificantBits())
-                        .setUidMsb(uuid.getMostSignificantBits())
-                        .setComplete(true).setEndEntryId(9).build());
-
-        @Cleanup
-        final ReadHandle readHandle = offloader.readOffloaded(0, contextBuilder.build(), driverMeta).get();
-
-        // delete blob(ledger)
-        blobStore.removeBlob(BUCKET, uuid.toString());
-
-        try {
-            readHandle.read(0, 9);
-            fail("Should be read fail");
-        } catch (BKException e) {
-            assertEquals(e.getCode(), NoSuchLedgerExistsException);
-        }
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static org.apache.bookkeeper.client.api.BKException.Code.NoSuchLedgerExistsException;
+import static org.mockito.AdditionalAnswers.delegatesTo;
+import static org.mockito.Mockito.mock;
+import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertNotNull;
+import static org.testng.Assert.fail;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.Random;
+import java.util.UUID;
+import lombok.Cleanup;
+import org.apache.bookkeeper.client.BKException;
+import org.apache.bookkeeper.client.api.LedgerEntries;
+import org.apache.bookkeeper.client.api.LedgerEntry;
+import org.apache.bookkeeper.client.api.ReadHandle;
+import org.apache.bookkeeper.mledger.Entry;
+import org.apache.bookkeeper.mledger.LedgerOffloader;
+import org.apache.bookkeeper.mledger.LedgerOffloader.OffloadHandle;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.bookkeeper.mledger.ManagedLedger;
+import org.apache.bookkeeper.mledger.impl.EntryImpl;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.JCloudBlobStoreProvider;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration;
+import org.apache.bookkeeper.mledger.proto.MLDataFormats;
+import org.apache.bookkeeper.mledger.proto.MLDataFormats.OffloadContext;
+import org.jclouds.blobstore.BlobStore;
+import org.mockito.Mockito;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.testng.Assert;
+import org.testng.annotations.Test;
+
+public class BlobStoreManagedLedgerOffloaderStreamingTest extends BlobStoreManagedLedgerOffloaderBase {
+
+    private static final Logger log = LoggerFactory.getLogger(BlobStoreManagedLedgerOffloaderStreamingTest.class);
+    private TieredStorageConfiguration mockedConfig;
+    private static final Random random = new Random();
+    private final LedgerOffloaderStats offloaderStats;
+
+    BlobStoreManagedLedgerOffloaderStreamingTest() throws Exception {
+        super();
+        config = getConfiguration(BUCKET);
+        JCloudBlobStoreProvider provider = getBlobStoreProvider();
+        assertNotNull(provider);
+        provider.validate(config);
+        blobStore = provider.getBlobStore(config);
+        this.offloaderStats = LedgerOffloaderStats.create(false, false, null, 0);
+    }
+
+    private BlobStoreManagedLedgerOffloader getOffloader(Map<String, String> additionalConfig) throws IOException {
+        return getOffloader(BUCKET, additionalConfig);
+    }
+
+    private BlobStoreManagedLedgerOffloader getOffloader(BlobStore mockedBlobStore,
+                                                         Map<String, String> additionalConfig) throws IOException {
+        return getOffloader(BUCKET, mockedBlobStore, additionalConfig);
+    }
+
+    private BlobStoreManagedLedgerOffloader getOffloader(String bucket, Map<String, String> additionalConfig) throws
+            IOException {
+        mockedConfig = mock(TieredStorageConfiguration.class, delegatesTo(getConfiguration(bucket, additionalConfig)));
+        Mockito.doReturn(blobStore).when(mockedConfig).getBlobStore(); // Use the REAL blobStore
+        BlobStoreManagedLedgerOffloader offloader = BlobStoreManagedLedgerOffloader
+                .create(mockedConfig, new HashMap<String, String>(), scheduler, scheduler,
+                        this.offloaderStats, entryOffsetsCache);
+        return offloader;
+    }
+
+    private BlobStoreManagedLedgerOffloader getOffloader(String bucket, BlobStore mockedBlobStore,
+                                                         Map<String, String> additionalConfig) throws IOException {
+        mockedConfig = mock(TieredStorageConfiguration.class, delegatesTo(getConfiguration(bucket, additionalConfig)));
+        Mockito.doReturn(mockedBlobStore).when(mockedConfig).getBlobStore();
+        BlobStoreManagedLedgerOffloader offloader = BlobStoreManagedLedgerOffloader
+                .create(mockedConfig, new HashMap<String, String>(), scheduler, scheduler,
+                        this.offloaderStats, entryOffsetsCache);
+        return offloader;
+    }
+
+    @Test
+    public void testHappyCase() throws Exception {
+        @Cleanup
+        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
+            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
+        }});
+        ManagedLedger ml = createMockManagedLedger();
+        UUID uuid = UUID.randomUUID();
+        long beginLedger = 0;
+        long beginEntry = 0;
+        log.error("try begin offload");
+        @Cleanup
+        OffloadHandle offloadHandle = offloader
+                .streamingOffload(ml, uuid, beginLedger, beginEntry, new HashMap<>()).get();
+        //Segment should closed because size in bytes full
+        for (int i = 0; i < 10; i++) {
+            final byte[] data = new byte[100];
+            random.nextBytes(data);
+            final OffloadHandle.OfferEntryResult offerEntryResult = offloadHandle
+                    .offerEntry(EntryImpl.create(0, i, data));
+            log.info("offer result: {}", offerEntryResult);
+        }
+        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
+        log.info("Offload reasult: {}", offloadResult);
+    }
+
+    @Test
+    public void testReadAndWrite() throws Exception {
+        @Cleanup
+        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
+            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
+        }});
+        ManagedLedger ml = createMockManagedLedger();
+        UUID uuid = UUID.randomUUID();
+        long beginLedger = 0;
+        long beginEntry = 0;
+
+        Map<String, String> driverMeta = new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.METADATA_FIELD_BUCKET, BUCKET);
+        }};
+        @Cleanup
+        OffloadHandle offloadHandle = offloader
+                .streamingOffload(ml, uuid, beginLedger, beginEntry, driverMeta).get();
+
+        //Segment should closed because size in bytes full
+        final LinkedList<Entry> entries = new LinkedList<>();
+        for (int i = 0; i < 10; i++) {
+            final byte[] data = new byte[100];
+            random.nextBytes(data);
+            final EntryImpl entry = EntryImpl.create(0, i, data);
+            offloadHandle.offerEntry(entry);
+            entries.add(entry);
+        }
+
+        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
+        assertEquals(offloadResult.endLedger, 0);
+        assertEquals(offloadResult.endEntry, 9);
+        final OffloadContext.Builder contextBuilder = OffloadContext.newBuilder();
+        contextBuilder.addOffloadSegment(
+                MLDataFormats.OffloadSegment.newBuilder()
+                        .setUidLsb(uuid.getLeastSignificantBits())
+                        .setUidMsb(uuid.getMostSignificantBits())
+                        .setComplete(true).setEndEntryId(9).build());
+
+        @Cleanup
+        final ReadHandle readHandle = offloader.readOffloaded(0, contextBuilder.build(), driverMeta).get();
+        @Cleanup
+        final LedgerEntries ledgerEntries = readHandle.readAsync(0, 9).get();
+
+        for (LedgerEntry ledgerEntry : ledgerEntries) {
+            final EntryImpl storedEntry = (EntryImpl) entries.get((int) ledgerEntry.getEntryId());
+            final byte[] storedData = storedEntry.getData();
+            final byte[] entryBytes = ledgerEntry.getEntryBytes();
+            assertEquals(storedData, entryBytes);
+        }
+    }
+
+    @Test
+    public void testReadAndWriteAcrossLedger() throws Exception {
+        @Cleanup
+        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "2000");
+            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
+        }});
+        ManagedLedger ml = createMockManagedLedger();
+        UUID uuid = UUID.randomUUID();
+        long beginLedger = 0;
+        long beginEntry = 0;
+
+        Map<String, String> driverMeta = new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.METADATA_FIELD_BUCKET, BUCKET);
+        }};
+        @Cleanup
+        OffloadHandle offloadHandle = offloader
+                .streamingOffload(ml, uuid, beginLedger, beginEntry, driverMeta).get();
+
+        //Segment should closed because size in bytes full
+        final LinkedList<Entry> entries = new LinkedList<>();
+        final LinkedList<Entry> ledger2Entries = new LinkedList<>();
+        for (int i = 0; i < 10; i++) {
+            final byte[] data = new byte[100];
+            random.nextBytes(data);
+            final EntryImpl entry = EntryImpl.create(0, i, data);
+            offloadHandle.offerEntry(entry);
+            entries.add(entry);
+        }
+        for (int i = 0; i < 10; i++) {
+            final byte[] data = new byte[100];
+            random.nextBytes(data);
+            final EntryImpl entry = EntryImpl.create(1, i, data);
+            offloadHandle.offerEntry(entry);
+            ledger2Entries.add(entry);
+        }
+
+        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
+        assertEquals(offloadResult.endLedger, 1);
+        assertEquals(offloadResult.endEntry, 9);
+        final OffloadContext.Builder contextBuilder = OffloadContext.newBuilder();
+        contextBuilder.addOffloadSegment(
+                MLDataFormats.OffloadSegment.newBuilder()
+                        .setUidLsb(uuid.getLeastSignificantBits())
+                        .setUidMsb(uuid.getMostSignificantBits())
+                        .setComplete(true).setEndEntryId(9).build());
+
+        @Cleanup
+        final ReadHandle readHandle = offloader.readOffloaded(0, contextBuilder.build(), driverMeta).get();
+        @Cleanup
+        final LedgerEntries ledgerEntries = readHandle.readAsync(0, 9).get();
+
+        for (LedgerEntry ledgerEntry : ledgerEntries) {
+            final EntryImpl storedEntry = (EntryImpl) entries.get((int) ledgerEntry.getEntryId());
+            final byte[] storedData = storedEntry.getData();
+            final byte[] entryBytes = ledgerEntry.getEntryBytes();
+            assertEquals(storedData, entryBytes);
+        }
+
+        @Cleanup
+        final ReadHandle readHandle2 = offloader.readOffloaded(1, contextBuilder.build(), driverMeta).get();
+        @Cleanup
+        final LedgerEntries ledgerEntries2 = readHandle2.readAsync(0, 9).get();
+
+        for (LedgerEntry ledgerEntry : ledgerEntries2) {
+            final EntryImpl storedEntry = (EntryImpl) ledger2Entries.get((int) ledgerEntry.getEntryId());
+            final byte[] storedData = storedEntry.getData();
+            final byte[] entryBytes = ledgerEntry.getEntryBytes();
+            assertEquals(storedData, entryBytes);
+        }
+    }
+
+    @Test
+    public void testReadAndWriteAcrossSegment() throws Exception {
+        @Cleanup
+        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
+            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
+        }});
+        @Cleanup
+        LedgerOffloader offloader2 = getOffloader(new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
+            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
+        }});
+        ManagedLedger ml = createMockManagedLedger();
+        UUID uuid = UUID.randomUUID();
+        UUID uuid2 = UUID.randomUUID();
+        long beginLedger = 0;
+        long beginEntry = 0;
+
+        Map<String, String> driverMeta = new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.METADATA_FIELD_BUCKET, BUCKET);
+        }};
+        @Cleanup
+        OffloadHandle offloadHandle = offloader
+                .streamingOffload(ml, uuid, beginLedger, beginEntry, driverMeta).get();
+
+        //Segment should closed because size in bytes full
+        final LinkedList<Entry> entries = new LinkedList<>();
+        for (int i = 0; i < 10; i++) {
+            final byte[] data = new byte[100];
+            random.nextBytes(data);
+            final EntryImpl entry = EntryImpl.create(0, i, data);
+            offloadHandle.offerEntry(entry);
+            entries.add(entry);
+        }
+
+        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
+        assertEquals(offloadResult.endLedger, 0);
+        assertEquals(offloadResult.endEntry, 9);
+
+        //Segment should closed because size in bytes full
+        @Cleanup
+        OffloadHandle offloadHandle2 = offloader2
+                .streamingOffload(ml, uuid2, beginLedger, 10, driverMeta).get();
+        for (int i = 0; i < 10; i++) {
+            final byte[] data = new byte[100];
+            random.nextBytes(data);
+            final EntryImpl entry = EntryImpl.create(0, i + 10, data);
+            offloadHandle2.offerEntry(entry);
+            entries.add(entry);
+        }
+        final LedgerOffloader.OffloadResult offloadResult2 = offloadHandle2.getOffloadResultAsync().get();
+        assertEquals(offloadResult2.endLedger, 0);
+        assertEquals(offloadResult2.endEntry, 19);
+
+        final OffloadContext.Builder contextBuilder = OffloadContext.newBuilder();
+        contextBuilder.addOffloadSegment(
+                MLDataFormats.OffloadSegment.newBuilder()
+                        .setUidLsb(uuid.getLeastSignificantBits())
+                        .setUidMsb(uuid.getMostSignificantBits())
+                        .setComplete(true).setEndEntryId(9).build()).addOffloadSegment(
+                MLDataFormats.OffloadSegment.newBuilder()
+                        .setUidLsb(uuid2.getLeastSignificantBits())
+                        .setUidMsb(uuid2.getMostSignificantBits())
+                        .setComplete(true).setEndEntryId(19).build()
+        );
+
+        @Cleanup
+        final ReadHandle readHandle = offloader.readOffloaded(0, contextBuilder.build(), driverMeta).get();
+        @Cleanup
+        final LedgerEntries ledgerEntries = readHandle.readAsync(0, 19).get();
+
+        for (LedgerEntry ledgerEntry : ledgerEntries) {
+            final EntryImpl storedEntry = (EntryImpl) entries.get((int) ledgerEntry.getEntryId());
+            final byte[] storedData = storedEntry.getData();
+            final byte[] entryBytes = ledgerEntry.getEntryBytes();
+            assertEquals(storedData, entryBytes);
+        }
+    }
+
+    @Test
+    public void testRandomRead() throws Exception {
+        @Cleanup
+        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
+            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
+        }});
+        LedgerOffloader offloader2 = getOffloader(new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
+            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
+        }});
+        ManagedLedger ml = createMockManagedLedger();
+        UUID uuid = UUID.randomUUID();
+        UUID uuid2 = UUID.randomUUID();
+        long beginLedger = 0;
+        long beginEntry = 0;
+
+        Map<String, String> driverMeta = new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.METADATA_FIELD_BUCKET, BUCKET);
+        }};
+        @Cleanup
+        OffloadHandle offloadHandle = offloader
+                .streamingOffload(ml, uuid, beginLedger, beginEntry, driverMeta).get();
+
+        //Segment should closed because size in bytes full
+        final LinkedList<Entry> entries = new LinkedList<>();
+        for (int i = 0; i < 10; i++) {
+            final byte[] data = new byte[100];
+            random.nextBytes(data);
+            final EntryImpl entry = EntryImpl.create(0, i, data);
+            offloadHandle.offerEntry(entry);
+            entries.add(entry);
+        }
+
+        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
+        assertEquals(offloadResult.endLedger, 0);
+        assertEquals(offloadResult.endEntry, 9);
+
+        //Segment should closed because size in bytes full
+        @Cleanup
+        OffloadHandle offloadHandle2 = offloader2
+                .streamingOffload(ml, uuid2, beginLedger, 10, driverMeta).get();
+        for (int i = 0; i < 10; i++) {
+            final byte[] data = new byte[100];
+            random.nextBytes(data);
+            final EntryImpl entry = EntryImpl.create(0, i + 10, data);
+            offloadHandle2.offerEntry(entry);
+            entries.add(entry);
+        }
+        final LedgerOffloader.OffloadResult offloadResult2 = offloadHandle2.getOffloadResultAsync().get();
+        assertEquals(offloadResult2.endLedger, 0);
+        assertEquals(offloadResult2.endEntry, 19);
+
+        final OffloadContext.Builder contextBuilder = OffloadContext.newBuilder();
+        contextBuilder.addOffloadSegment(
+                MLDataFormats.OffloadSegment.newBuilder()
+                        .setUidLsb(uuid.getLeastSignificantBits())
+                        .setUidMsb(uuid.getMostSignificantBits())
+                        .setComplete(true).setEndEntryId(9).build()).addOffloadSegment(
+                MLDataFormats.OffloadSegment.newBuilder()
+                        .setUidLsb(uuid2.getLeastSignificantBits())
+                        .setUidMsb(uuid2.getMostSignificantBits())
+                        .setComplete(true).setEndEntryId(19).build()
+        );
+
+        @Cleanup
+        final ReadHandle readHandle = offloader.readOffloaded(0, contextBuilder.build(), driverMeta).get();
+
+        for (int i = 0; i <= 19; i++) {
+            Random seed = new Random(0);
+            int begin = seed.nextInt(20);
+            int end = seed.nextInt(20);
+            if (begin >= end) {
+                int temp = begin;
+                begin = end;
+                end = temp;
+            }
+            @Cleanup
+            final LedgerEntries ledgerEntries = readHandle.readAsync(begin, end).get();
+            for (LedgerEntry ledgerEntry : ledgerEntries) {
+                final EntryImpl storedEntry = (EntryImpl) entries.get((int) ledgerEntry.getEntryId());
+                final byte[] storedData = storedEntry.getData();
+                final byte[] entryBytes = ledgerEntry.getEntryBytes();
+                assertEquals(storedData, entryBytes);
+            }
+        }
+    }
+
+    @Test
+    public void testInvalidEntryIds() throws Exception {
+        @Cleanup
+        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
+            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
+        }});
+        ManagedLedger ml = createMockManagedLedger();
+        UUID uuid = UUID.randomUUID();
+        long beginLedger = 0;
+        long beginEntry = 0;
+
+        Map<String, String> driverMeta = new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.METADATA_FIELD_BUCKET, BUCKET);
+        }};
+        @Cleanup
+        OffloadHandle offloadHandle = offloader
+                .streamingOffload(ml, uuid, beginLedger, beginEntry, driverMeta).get();
+
+        //Segment should closed because size in bytes full
+        final LinkedList<Entry> entries = new LinkedList<>();
+        for (int i = 0; i < 10; i++) {
+            final byte[] data = new byte[100];
+            random.nextBytes(data);
+            final EntryImpl entry = EntryImpl.create(0, i, data);
+            offloadHandle.offerEntry(entry);
+            entries.add(entry);
+        }
+
+        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
+        assertEquals(offloadResult.endLedger, 0);
+        assertEquals(offloadResult.endEntry, 9);
+        final OffloadContext.Builder contextBuilder = OffloadContext.newBuilder();
+        contextBuilder.addOffloadSegment(
+                MLDataFormats.OffloadSegment.newBuilder()
+                        .setUidLsb(uuid.getLeastSignificantBits())
+                        .setUidMsb(uuid.getMostSignificantBits())
+                        .setComplete(true).setEndEntryId(9).build());
+
+        @Cleanup
+        final ReadHandle readHandle = offloader.readOffloaded(0, contextBuilder.build(), driverMeta).get();
+        try {
+            readHandle.read(-1, -1);
+            Assert.fail("Shouldn't be able to read anything");
+        } catch (Exception e) {
+        }
+
+        try {
+            readHandle.read(0, 20);
+            Assert.fail("Shouldn't be able to read anything");
+        } catch (Exception e) {
+        }
+    }
+
+    @Test
+    public void testReadNotExistLedger() throws Exception {
+        @Cleanup
+        LedgerOffloader offloader = getOffloader(new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_SIZE_IN_BYTES, "1000");
+            put(config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE).get(0), "5242880");
+            put(TieredStorageConfiguration.MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC, "600");
+        }});
+        ManagedLedger ml = createMockManagedLedger();
+        UUID uuid = UUID.randomUUID();
+        long beginLedger = 0;
+        long beginEntry = 0;
+
+        Map<String, String> driverMeta = new HashMap<String, String>() {{
+            put(TieredStorageConfiguration.METADATA_FIELD_BUCKET, BUCKET);
+        }};
+        @Cleanup
+        OffloadHandle offloadHandle = offloader
+                .streamingOffload(ml, uuid, beginLedger, beginEntry, driverMeta).get();
+
+        // Segment should closed because size in bytes full
+        final LinkedList<Entry> entries = new LinkedList<>();
+        for (int i = 0; i < 10; i++) {
+            final byte[] data = new byte[100];
+            random.nextBytes(data);
+            final EntryImpl entry = EntryImpl.create(0, i, data);
+            offloadHandle.offerEntry(entry);
+            entries.add(entry);
+        }
+
+        final LedgerOffloader.OffloadResult offloadResult = offloadHandle.getOffloadResultAsync().get();
+        assertEquals(offloadResult.endLedger, 0);
+        assertEquals(offloadResult.endEntry, 9);
+        final OffloadContext.Builder contextBuilder = OffloadContext.newBuilder();
+        contextBuilder.addOffloadSegment(
+                MLDataFormats.OffloadSegment.newBuilder()
+                        .setUidLsb(uuid.getLeastSignificantBits())
+                        .setUidMsb(uuid.getMostSignificantBits())
+                        .setComplete(true).setEndEntryId(9).build());
+
+        @Cleanup
+        final ReadHandle readHandle = offloader.readOffloaded(0, contextBuilder.build(), driverMeta).get();
+
+        // delete blob(ledger)
+        blobStore.removeBlob(BUCKET, uuid.toString());
+
+        try {
+            readHandle.read(0, 9);
+            fail("Should be read fail");
+        } catch (BKException e) {
+            assertEquals(e.getCode(), NoSuchLedgerExistsException);
+        }
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderTest.java
index cd97954b97..0e514f962d 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderTest.java
@@ -1,697 +1,697 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static org.apache.bookkeeper.client.api.BKException.Code.NoSuchLedgerExistsException;
-import static org.mockito.AdditionalAnswers.delegatesTo;
-import static org.mockito.ArgumentMatchers.any;
-import static org.mockito.ArgumentMatchers.anyInt;
-import static org.mockito.ArgumentMatchers.anyLong;
-import static org.mockito.Mockito.mock;
-import static org.testng.Assert.assertEquals;
-import static org.testng.Assert.assertNotNull;
-import static org.testng.Assert.assertTrue;
-import static org.testng.Assert.fail;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.UUID;
-import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.Executors;
-import java.util.concurrent.ScheduledExecutorService;
-import lombok.Cleanup;
-import org.apache.bookkeeper.client.BKException;
-import org.apache.bookkeeper.client.api.LedgerEntries;
-import org.apache.bookkeeper.client.api.LedgerEntry;
-import org.apache.bookkeeper.client.api.ReadHandle;
-import org.apache.bookkeeper.mledger.LedgerOffloader;
-import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
-import org.apache.bookkeeper.mledger.ManagedLedgerException;
-import org.apache.bookkeeper.mledger.OffloadedLedgerMetadata;
-import org.apache.bookkeeper.mledger.impl.LedgerOffloaderStatsImpl;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.JCloudBlobStoreProvider;
-import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration;
-import org.apache.pulsar.common.naming.TopicName;
-import org.jclouds.blobstore.BlobStore;
-import org.jclouds.blobstore.options.CopyOptions;
-import org.mockito.Mockito;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.testng.Assert;
-import org.testng.annotations.AfterClass;
-import org.testng.annotations.Test;
-import org.testng.collections.Maps;
-
-public class BlobStoreManagedLedgerOffloaderTest extends BlobStoreManagedLedgerOffloaderBase {
-
-    private static final Logger log = LoggerFactory.getLogger(BlobStoreManagedLedgerOffloaderTest.class);
-    private final ScheduledExecutorService scheduledExecutorService;
-    private TieredStorageConfiguration mockedConfig;
-    private final LedgerOffloaderStats offloaderStats;
-
-    BlobStoreManagedLedgerOffloaderTest() throws Exception {
-        super();
-        config = getConfiguration(BUCKET);
-        JCloudBlobStoreProvider provider = getBlobStoreProvider();
-        assertNotNull(provider);
-        provider.validate(config);
-        blobStore = provider.getBlobStore(config);
-        scheduledExecutorService = Executors.newScheduledThreadPool(1);
-        this.offloaderStats = LedgerOffloaderStats.create(true, true, scheduledExecutorService, 60);
-    }
-
-    @AfterClass(alwaysRun = true)
-    protected void cleanupInstance() throws Exception {
-        offloaderStats.close();
-        scheduledExecutorService.shutdownNow();
-    }
-
-    private BlobStoreManagedLedgerOffloader getOffloader() throws IOException {
-        return getOffloader(BUCKET);
-    }
-
-    private BlobStoreManagedLedgerOffloader getOffloader(BlobStore mockedBlobStore) throws IOException {
-        return getOffloader(BUCKET, mockedBlobStore);
-    }
-
-    private BlobStoreManagedLedgerOffloader getOffloader(String bucket) throws IOException {
-        mockedConfig = mock(TieredStorageConfiguration.class, delegatesTo(getConfiguration(bucket)));
-        Mockito.doReturn(blobStore).when(mockedConfig).getBlobStore(); // Use the REAL blobStore
-        BlobStoreManagedLedgerOffloader offloader = BlobStoreManagedLedgerOffloader.create(mockedConfig,
-                new HashMap<String, String>(), scheduler, scheduler, this.offloaderStats,
-                entryOffsetsCache);
-        return offloader;
-    }
-
-    private BlobStoreManagedLedgerOffloader getOffloader(String bucket, BlobStore mockedBlobStore) throws IOException {
-        mockedConfig = mock(TieredStorageConfiguration.class, delegatesTo(getConfiguration(bucket)));
-        Mockito.doReturn(mockedBlobStore).when(mockedConfig).getBlobStore();
-        BlobStoreManagedLedgerOffloader offloader = BlobStoreManagedLedgerOffloader.create(mockedConfig,
-                new HashMap<String, String>(), scheduler, scheduler, this.offloaderStats,
-                entryOffsetsCache);
-        return offloader;
-    }
-
-    @Test(timeOut = 600000)  // 10 minutes.
-    public void testHappyCase() throws Exception {
-        @Cleanup
-        LedgerOffloader offloader = getOffloader();
-        offloader.offload(buildReadHandle(), UUID.randomUUID(), new HashMap<>()).get();
-    }
-
-    @Test(timeOut = 600000)  // 10 minutes.
-    public void testBucketDoesNotExist() throws Exception {
-
-        if (provider == JCloudBlobStoreProvider.TRANSIENT) {
-            // Skip this test, since it isn't applicable.
-            return;
-        }
-
-        @Cleanup
-        LedgerOffloader offloader = getOffloader("some-non-existant-bucket-name");
-        try {
-            offloader.offload(buildReadHandle(), UUID.randomUUID(), new HashMap<>()).get();
-            Assert.fail("Shouldn't be able to add to bucket");
-        } catch (ExecutionException e) {
-            log.error("Exception: ", e);
-            Assert.assertTrue(e.getMessage().toLowerCase().contains("not found"));
-        }
-    }
-
-    @Test(timeOut = 600000)  // 10 minutes.
-    public void testOffloadAndRead() throws Exception {
-        @Cleanup
-        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 3);
-        @Cleanup
-        LedgerOffloader offloader = getOffloader();
-
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, new HashMap<>()).get();
-
-        @Cleanup
-        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
-        assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
-
-        try (LedgerEntries toWriteEntries = toWrite.read(0, toWrite.getLastAddConfirmed());
-             LedgerEntries toTestEntries = toTest.read(0, toTest.getLastAddConfirmed())) {
-            Iterator<LedgerEntry> toWriteIter = toWriteEntries.iterator();
-            Iterator<LedgerEntry> toTestIter = toTestEntries.iterator();
-
-            while (toWriteIter.hasNext() && toTestIter.hasNext()) {
-                LedgerEntry toWriteEntry = toWriteIter.next();
-                LedgerEntry toTestEntry = toTestIter.next();
-
-                assertEquals(toWriteEntry.getLedgerId(), toTestEntry.getLedgerId());
-                assertEquals(toWriteEntry.getEntryId(), toTestEntry.getEntryId());
-                assertEquals(toWriteEntry.getLength(), toTestEntry.getLength());
-                assertEquals(toWriteEntry.getEntryBuffer(), toTestEntry.getEntryBuffer());
-            }
-            Assert.assertFalse(toWriteIter.hasNext());
-            Assert.assertFalse(toTestIter.hasNext());
-        }
-    }
-
-    @Test(timeOut = 60000)
-    public void testReadHandlerState() throws Exception {
-        @Cleanup
-        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 3);
-        @Cleanup
-        LedgerOffloader offloader = getOffloader();
-
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, new HashMap<>()).get();
-
-        BlobStoreBackedReadHandleImpl toTest = (BlobStoreBackedReadHandleImpl) offloader
-                .readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
-        Assert.assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
-        Assert.assertEquals(toTest.getState(), BlobStoreBackedReadHandleImpl.State.Opened);
-        @Cleanup
-        LedgerEntries ledgerEntries = toTest.read(0, 1);
-        toTest.close();
-        Assert.assertEquals(toTest.getState(), BlobStoreBackedReadHandleImpl.State.Closed);
-    }
-
-    @Test(timeOut = 600000)  // 10 minutes.
-    public void testOffloadAndReadMetrics() throws Exception {
-        @Cleanup
-        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 3);
-        @Cleanup
-        LedgerOffloader offloader = getOffloader();
-
-        UUID uuid = UUID.randomUUID();
-        String managedLegerName = "public/default/persistent/testOffload";
-        String topic = TopicName.fromPersistenceNamingEncoding(managedLegerName);
-        Map<String, String> extraMap = new HashMap<>();
-        extraMap.put("ManagedLedgerName", managedLegerName);
-        offloader.offload(toWrite, uuid, extraMap).get();
-
-        @Cleanup
-        LedgerOffloaderStatsImpl offloaderStats = (LedgerOffloaderStatsImpl) this.offloaderStats;
-
-        assertEquals(offloaderStats.getOffloadError(topic), 0);
-        assertTrue(offloaderStats.getOffloadBytes(topic) > 0);
-        assertTrue(offloaderStats.getReadLedgerLatency(topic).count > 0);
-        assertEquals(offloaderStats.getWriteStorageError(topic), 0);
-
-        Map<String, String> map = new HashMap<>();
-        map.putAll(offloader.getOffloadDriverMetadata());
-        map.put("ManagedLedgerName", managedLegerName);
-        @Cleanup
-        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, map).get();
-        @Cleanup
-        LedgerEntries toTestEntries = toTest.read(0, toTest.getLastAddConfirmed());
-        Iterator<LedgerEntry> toTestIter = toTestEntries.iterator();
-        while (toTestIter.hasNext()) {
-            LedgerEntry toTestEntry = toTestIter.next();
-        }
-
-        assertEquals(offloaderStats.getReadOffloadError(topic), 0);
-        assertTrue(offloaderStats.getReadOffloadBytes(topic) > 0);
-        assertTrue(offloaderStats.getReadOffloadDataLatency(topic).count > 0);
-        assertTrue(offloaderStats.getReadOffloadIndexLatency(topic).count > 0);
-    }
-
-    @Test
-    public void testOffloadFailInitDataBlockUpload() throws Exception {
-        @Cleanup
-        ReadHandle readHandle = buildReadHandle();
-        UUID uuid = UUID.randomUUID();
-        String failureString = "fail InitDataBlockUpload";
-
-        // mock throw exception when initiateMultipartUpload
-        try {
-            BlobStore spiedBlobStore = mock(BlobStore.class, delegatesTo(blobStore));
-
-            Mockito
-                .doThrow(new RuntimeException(failureString))
-                .when(spiedBlobStore).initiateMultipartUpload(any(), any(), any());
-
-            @Cleanup
-            BlobStoreManagedLedgerOffloader offloader = getOffloader(spiedBlobStore);
-            offloader.offload(readHandle, uuid, new HashMap<>()).get();
-            Assert.fail("Should throw exception when initiateMultipartUpload");
-        } catch (Exception e) {
-            Assert.assertTrue(e.getCause() instanceof RuntimeException);
-            Assert.assertTrue(e.getCause().getMessage().contains(failureString));
-            Assert.assertFalse(blobStore.blobExists(BUCKET,
-                    DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
-            Assert.assertFalse(blobStore.blobExists(BUCKET,
-                    DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
-        }
-    }
-
-    @Test
-    public void testOffloadFailDataBlockPartUpload() throws Exception {
-        @Cleanup
-        ReadHandle readHandle = buildReadHandle();
-        UUID uuid = UUID.randomUUID();
-        String failureString = "fail DataBlockPartUpload";
-
-        // mock throw exception when uploadPart
-        try {
-
-            BlobStore spiedBlobStore = mock(BlobStore.class, delegatesTo(blobStore));
-            Mockito
-                .doThrow(new RuntimeException(failureString))
-                .when(spiedBlobStore).uploadMultipartPart(any(), anyInt(), any());
-
-            @Cleanup
-            BlobStoreManagedLedgerOffloader offloader = getOffloader(spiedBlobStore);
-            offloader.offload(readHandle, uuid, new HashMap<>()).get();
-            Assert.fail("Should throw exception for when uploadPart");
-        } catch (Exception e) {
-            Assert.assertTrue(e.getCause() instanceof RuntimeException);
-            Assert.assertTrue(e.getCause().getMessage().contains(failureString));
-            Assert.assertFalse(blobStore.blobExists(BUCKET,
-                    DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
-            Assert.assertFalse(blobStore.blobExists(BUCKET,
-                    DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
-        }
-    }
-
-    @Test
-    public void testOffloadFailDataBlockUploadComplete() throws Exception {
-        @Cleanup
-        ReadHandle readHandle = buildReadHandle();
-        UUID uuid = UUID.randomUUID();
-        String failureString = "fail DataBlockUploadComplete";
-
-        // mock throw exception when completeMultipartUpload
-        try {
-            BlobStore spiedBlobStore = mock(BlobStore.class, delegatesTo(blobStore));
-            Mockito
-                .doThrow(new RuntimeException(failureString))
-                .when(spiedBlobStore).completeMultipartUpload(any(), any());
-            Mockito
-                .doNothing()
-                .when(spiedBlobStore).abortMultipartUpload(any());
-
-            @Cleanup
-            BlobStoreManagedLedgerOffloader offloader = getOffloader(spiedBlobStore);
-            offloader.offload(readHandle, uuid, new HashMap<>()).get();
-
-            Assert.fail("Should throw exception for when completeMultipartUpload");
-        } catch (Exception e) {
-            // excepted
-            Assert.assertTrue(e.getCause() instanceof RuntimeException);
-            Assert.assertTrue(e.getCause().getMessage().contains(failureString));
-            Assert.assertFalse(blobStore.blobExists(BUCKET,
-                    DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
-            Assert.assertFalse(blobStore.blobExists(BUCKET,
-                    DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
-        }
-    }
-
-    @Test
-    public void testOffloadFailPutIndexBlock() throws Exception {
-        @Cleanup
-        ReadHandle readHandle = buildReadHandle();
-        UUID uuid = UUID.randomUUID();
-        String failureString = "fail putObject";
-
-        // mock throw exception when putObject
-        try {
-            BlobStore spiedBlobStore = mock(BlobStore.class, delegatesTo(blobStore));
-            Mockito
-                .doThrow(new RuntimeException(failureString))
-                .when(spiedBlobStore).putBlob(any(), any());
-
-            @Cleanup
-            BlobStoreManagedLedgerOffloader offloader = getOffloader(spiedBlobStore);
-            offloader.offload(readHandle, uuid, new HashMap<>()).get();
-
-            Assert.fail("Should throw exception for when putObject for index block");
-         } catch (Exception e) {
-            // excepted
-            Assert.assertTrue(e.getCause() instanceof RuntimeException);
-            Assert.assertTrue(e.getCause().getMessage().contains(failureString));
-            Assert.assertFalse(blobStore.blobExists(BUCKET,
-                    DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
-            Assert.assertFalse(blobStore.blobExists(BUCKET,
-                    DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
-        }
-    }
-
-    @Test(timeOut = 600000)
-    public void testOffloadReadRandomAccess() throws Exception {
-        @Cleanup
-        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 3);
-        long[][] randomAccesses = new long[10][2];
-        Random r = new Random(0);
-        for (int i = 0; i < 10; i++) {
-            long first = r.nextInt((int) toWrite.getLastAddConfirmed());
-            long second = r.nextInt((int) toWrite.getLastAddConfirmed());
-            if (second < first) {
-                long tmp = first;
-                first = second;
-                second = tmp;
-            }
-            randomAccesses[i][0] = first;
-            randomAccesses[i][1] = second;
-        }
-
-        @Cleanup
-        LedgerOffloader offloader = getOffloader();
-
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, new HashMap<>()).get();
-
-        @Cleanup
-        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
-        assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
-
-        for (long[] access : randomAccesses) {
-            try (LedgerEntries toWriteEntries = toWrite.read(access[0], access[1]);
-                 LedgerEntries toTestEntries = toTest.read(access[0], access[1])) {
-                Iterator<LedgerEntry> toWriteIter = toWriteEntries.iterator();
-                Iterator<LedgerEntry> toTestIter = toTestEntries.iterator();
-
-                while (toWriteIter.hasNext() && toTestIter.hasNext()) {
-                    LedgerEntry toWriteEntry = toWriteIter.next();
-                    LedgerEntry toTestEntry = toTestIter.next();
-
-                    assertEquals(toWriteEntry.getLedgerId(), toTestEntry.getLedgerId());
-                    assertEquals(toWriteEntry.getEntryId(), toTestEntry.getEntryId());
-                    assertEquals(toWriteEntry.getLength(), toTestEntry.getLength());
-                    assertEquals(toWriteEntry.getEntryBuffer(), toTestEntry.getEntryBuffer());
-                }
-                Assert.assertFalse(toWriteIter.hasNext());
-                Assert.assertFalse(toTestIter.hasNext());
-            }
-        }
-    }
-
-    @Test
-    public void testOffloadReadInvalidEntryIds() throws Exception {
-        @Cleanup
-        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
-        @Cleanup
-        LedgerOffloader offloader = getOffloader();
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, new HashMap<>()).get();
-
-        @Cleanup
-        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
-        assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
-
-        try {
-            toTest.read(-1, -1);
-            Assert.fail("Shouldn't be able to read anything");
-        } catch (BKException.BKIncorrectParameterException e) {
-        }
-
-        try {
-            toTest.read(0, toTest.getLastAddConfirmed() + 1);
-            Assert.fail("Shouldn't be able to read anything");
-        } catch (BKException.BKIncorrectParameterException e) {
-        }
-    }
-
-    @Test
-    public void testDeleteOffloaded() throws Exception {
-        @Cleanup
-        ReadHandle readHandle = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
-        UUID uuid = UUID.randomUUID();
-
-        @Cleanup
-        BlobStoreManagedLedgerOffloader offloader = getOffloader();
-
-        // verify object exist after offload
-        offloader.offload(readHandle, uuid, new HashMap<>()).get();
-        Assert.assertTrue(blobStore.blobExists(BUCKET,
-                DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
-        Assert.assertTrue(blobStore.blobExists(BUCKET,
-                DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
-
-        // verify object deleted after delete
-        offloader.deleteOffloaded(readHandle.getId(), uuid, config.getOffloadDriverMetadata()).get();
-        Assert.assertFalse(blobStore.blobExists(BUCKET,
-                DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
-        Assert.assertFalse(blobStore.blobExists(BUCKET,
-                DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
-    }
-
-    @Test
-    public void testDeleteOffloadedFail() throws Exception {
-        String failureString = "fail deleteOffloaded";
-        @Cleanup
-        ReadHandle readHandle = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
-        UUID uuid = UUID.randomUUID();
-
-        BlobStore spiedBlobStore = mock(BlobStore.class, delegatesTo(blobStore));
-
-        Mockito
-            .doThrow(new RuntimeException(failureString))
-            .when(spiedBlobStore).removeBlobs(any(), any());
-
-        @Cleanup
-        BlobStoreManagedLedgerOffloader offloader = getOffloader(spiedBlobStore);
-
-        try {
-            // verify object exist after offload
-            offloader.offload(readHandle, uuid, new HashMap<>()).get();
-            Assert.assertTrue(blobStore.blobExists(BUCKET,
-                    DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
-            Assert.assertTrue(blobStore.blobExists(BUCKET,
-                    DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
-
-            offloader.deleteOffloaded(readHandle.getId(), uuid, config.getOffloadDriverMetadata()).get();
-        } catch (Exception e) {
-            // expected
-            Assert.assertTrue(e.getCause().getMessage().contains(failureString));
-            // verify object still there.
-            Assert.assertTrue(blobStore.blobExists(BUCKET,
-                    DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
-            Assert.assertTrue(blobStore.blobExists(BUCKET,
-                    DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
-        }
-    }
-
-    @Test
-    public void testOffloadEmpty() throws Exception {
-        CompletableFuture<LedgerEntries> noEntries = new CompletableFuture<>();
-        noEntries.completeExceptionally(new BKException.BKReadException());
-
-        ReadHandle readHandle = Mockito.mock(ReadHandle.class);
-        Mockito.doReturn(-1L).when(readHandle).getLastAddConfirmed();
-        Mockito.doReturn(noEntries).when(readHandle).readAsync(anyLong(), anyLong());
-        Mockito.doReturn(0L).when(readHandle).getLength();
-        Mockito.doReturn(true).when(readHandle).isClosed();
-        Mockito.doReturn(1234L).when(readHandle).getId();
-
-        UUID uuid = UUID.randomUUID();
-        @Cleanup
-        LedgerOffloader offloader = getOffloader();
-
-        try {
-            offloader.offload(readHandle, uuid, new HashMap<>()).get();
-            Assert.fail("Shouldn't have been able to offload");
-        } catch (ExecutionException e) {
-            assertEquals(e.getCause().getClass(), IllegalArgumentException.class);
-        }
-    }
-
-    @Test
-    public void testReadUnknownDataVersion() throws Exception {
-        @Cleanup
-        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
-        BlobStoreManagedLedgerOffloader offloader = getOffloader();
-
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, new HashMap<>()).get();
-
-        String dataKey = DataBlockUtils.dataBlockOffloadKey(toWrite.getId(), uuid);
-
-        // Here it will return a Immutable map.
-        Assert.assertTrue(blobStore.blobExists(BUCKET, dataKey));
-        Map<String, String> immutableMap = blobStore.blobMetadata(BUCKET, dataKey).getUserMetadata();
-        Map<String, String> userMeta = Maps.newHashMap();
-        userMeta.putAll(immutableMap);
-        userMeta.put(DataBlockUtils.METADATA_FORMAT_VERSION_KEY.toLowerCase(), String.valueOf(-12345));
-        blobStore.copyBlob(BUCKET, dataKey, BUCKET, dataKey, CopyOptions.builder().userMetadata(userMeta).build());
-
-        try (ReadHandle toRead = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get()) {
-            toRead.readAsync(0, 0).get();
-            Assert.fail("Shouldn't have been able to read");
-        } catch (ExecutionException e) {
-            log.error("Exception: ", e);
-            assertEquals(e.getCause().getClass(), IOException.class);
-            Assert.assertTrue(e.getCause().getMessage().contains("Error reading from BlobStore"));
-        }
-
-        userMeta.put(DataBlockUtils.METADATA_FORMAT_VERSION_KEY.toLowerCase(), String.valueOf(12345));
-        blobStore.copyBlob(BUCKET, dataKey, BUCKET, dataKey, CopyOptions.builder().userMetadata(userMeta).build());
-
-        try (ReadHandle toRead = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get()) {
-            toRead.readAsync(0, 0).get();
-            Assert.fail("Shouldn't have been able to read");
-        } catch (ExecutionException e) {
-            assertEquals(e.getCause().getClass(), IOException.class);
-            Assert.assertTrue(e.getCause().getMessage().contains("Error reading from BlobStore"));
-        }
-    }
-
-    @Test
-    public void testReadUnknownIndexVersion() throws Exception {
-        @Cleanup
-        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
-        BlobStoreManagedLedgerOffloader offloader = getOffloader();
-
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, new HashMap<>()).get();
-
-        String indexKey = DataBlockUtils.indexBlockOffloadKey(toWrite.getId(), uuid);
-
-        // Here it will return a Immutable map.
-        Map<String, String> immutableMap = blobStore.blobMetadata(BUCKET, indexKey).getUserMetadata();
-        Map<String, String> userMeta = Maps.newHashMap();
-        userMeta.putAll(immutableMap);
-        userMeta.put(DataBlockUtils.METADATA_FORMAT_VERSION_KEY.toLowerCase(), String.valueOf(-12345));
-        blobStore.copyBlob(BUCKET, indexKey, BUCKET, indexKey, CopyOptions.builder().userMetadata(userMeta).build());
-
-        try {
-            offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
-            Assert.fail("Shouldn't have been able to open");
-        } catch (ExecutionException e) {
-            assertEquals(e.getCause().getClass(), IOException.class);
-            Assert.assertTrue(e.getCause().getMessage().contains("Invalid object version"));
-        }
-
-        userMeta.put(DataBlockUtils.METADATA_FORMAT_VERSION_KEY.toLowerCase(), String.valueOf(12345));
-        blobStore.copyBlob(BUCKET, indexKey, BUCKET, indexKey, CopyOptions.builder().userMetadata(userMeta).build());
-
-        try {
-            offloader.readOffloaded(toWrite.getId(), uuid, config.getOffloadDriverMetadata()).get();
-            Assert.fail("Shouldn't have been able to open");
-        } catch (ExecutionException e) {
-            assertEquals(e.getCause().getClass(), IOException.class);
-            Assert.assertTrue(e.getCause().getMessage().contains("Invalid object version"));
-        }
-    }
-
-    @Test
-    public void testReadEOFException() throws Throwable {
-        @Cleanup
-        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
-        @Cleanup
-        LedgerOffloader offloader = getOffloader();
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, new HashMap<>()).get();
-
-        @Cleanup
-        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
-        Assert.assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
-        @Cleanup
-        LedgerEntries ledgerEntries = toTest.readAsync(0, toTest.getLastAddConfirmed()).get();
-
-        try {
-            @Cleanup
-            LedgerEntries ledgerEntries2 = toTest.readAsync(0, 0).get();
-        } catch (Exception e) {
-            Assert.fail("Get unexpected exception when reading entries", e);
-        }
-    }
-
-    @Test(timeOut = 600000)  // 10 minutes.
-    public void testScanLedgers() throws Exception {
-        @Cleanup
-        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 3);
-        @Cleanup
-        LedgerOffloader offloader = getOffloader();
-
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, new HashMap<>()).get();
-
-        List<OffloadedLedgerMetadata> result = new ArrayList<>();
-        offloader.scanLedgers(
-                (m) -> {
-                    log.info("found {}", m);
-                    if (m.getLedgerId() == toWrite.getId()) {
-                        result.add(m);
-                    }
-                    return true;
-                }, offloader.getOffloadDriverMetadata());
-        assertEquals(2, result.size());
-
-        // data and index
-
-        OffloadedLedgerMetadata offloadedLedgerMetadata = result.get(0);
-        assertEquals(toWrite.getId(), offloadedLedgerMetadata.getLedgerId());
-
-        OffloadedLedgerMetadata offloadedLedgerMetadata2 = result.get(1);
-        assertEquals(toWrite.getId(), offloadedLedgerMetadata2.getLedgerId());
-    }
-
-    @Test
-    public void testReadWithAClosedLedgerHandler() throws Exception {
-        @Cleanup
-        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
-        @Cleanup
-        LedgerOffloader offloader = getOffloader();
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, new HashMap<>()).get();
-
-        @Cleanup
-        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
-        Assert.assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
-        long lac = toTest.getLastAddConfirmed();
-        @Cleanup
-        LedgerEntries ledgerEntries = toTest.readAsync(0, lac).get();
-        toTest.closeAsync().get();
-        try {
-            toTest.readAsync(0, lac).get();
-        } catch (Exception e) {
-            if (e.getCause() instanceof ManagedLedgerException.OffloadReadHandleClosedException) {
-                // expected exception
-                return;
-            }
-            throw e;
-        }
-    }
-
-    @Test
-    public void testReadNotExistLedger() throws Exception {
-        @Cleanup
-        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 3);
-        @Cleanup
-        LedgerOffloader offloader = getOffloader();
-
-        UUID uuid = UUID.randomUUID();
-        offloader.offload(toWrite, uuid, new HashMap<>()).get();
-        @Cleanup
-        ReadHandle offloadRead = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
-        assertEquals(offloadRead.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
-
-        // delete blob(ledger)
-        blobStore.removeBlob(BUCKET, DataBlockUtils.dataBlockOffloadKey(toWrite.getId(), uuid));
-
-        try {
-            offloadRead.read(0, offloadRead.getLastAddConfirmed());
-            fail("Should be read fail");
-        } catch (BKException e) {
-            assertEquals(e.getCode(), NoSuchLedgerExistsException);
-        }
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static org.apache.bookkeeper.client.api.BKException.Code.NoSuchLedgerExistsException;
+import static org.mockito.AdditionalAnswers.delegatesTo;
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyInt;
+import static org.mockito.ArgumentMatchers.anyLong;
+import static org.mockito.Mockito.mock;
+import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertNotNull;
+import static org.testng.Assert.assertTrue;
+import static org.testng.Assert.fail;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Executors;
+import java.util.concurrent.ScheduledExecutorService;
+import lombok.Cleanup;
+import org.apache.bookkeeper.client.BKException;
+import org.apache.bookkeeper.client.api.LedgerEntries;
+import org.apache.bookkeeper.client.api.LedgerEntry;
+import org.apache.bookkeeper.client.api.ReadHandle;
+import org.apache.bookkeeper.mledger.LedgerOffloader;
+import org.apache.bookkeeper.mledger.LedgerOffloaderStats;
+import org.apache.bookkeeper.mledger.ManagedLedgerException;
+import org.apache.bookkeeper.mledger.OffloadedLedgerMetadata;
+import org.apache.bookkeeper.mledger.impl.LedgerOffloaderStatsImpl;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.JCloudBlobStoreProvider;
+import org.apache.bookkeeper.mledger.offload.jcloud.provider.TieredStorageConfiguration;
+import org.apache.pulsar.common.naming.TopicName;
+import org.jclouds.blobstore.BlobStore;
+import org.jclouds.blobstore.options.CopyOptions;
+import org.mockito.Mockito;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.testng.Assert;
+import org.testng.annotations.AfterClass;
+import org.testng.annotations.Test;
+import org.testng.collections.Maps;
+
+public class BlobStoreManagedLedgerOffloaderTest extends BlobStoreManagedLedgerOffloaderBase {
+
+    private static final Logger log = LoggerFactory.getLogger(BlobStoreManagedLedgerOffloaderTest.class);
+    private final ScheduledExecutorService scheduledExecutorService;
+    private TieredStorageConfiguration mockedConfig;
+    private final LedgerOffloaderStats offloaderStats;
+
+    BlobStoreManagedLedgerOffloaderTest() throws Exception {
+        super();
+        config = getConfiguration(BUCKET);
+        JCloudBlobStoreProvider provider = getBlobStoreProvider();
+        assertNotNull(provider);
+        provider.validate(config);
+        blobStore = provider.getBlobStore(config);
+        scheduledExecutorService = Executors.newScheduledThreadPool(1);
+        this.offloaderStats = LedgerOffloaderStats.create(true, true, scheduledExecutorService, 60);
+    }
+
+    @AfterClass(alwaysRun = true)
+    protected void cleanupInstance() throws Exception {
+        offloaderStats.close();
+        scheduledExecutorService.shutdownNow();
+    }
+
+    private BlobStoreManagedLedgerOffloader getOffloader() throws IOException {
+        return getOffloader(BUCKET);
+    }
+
+    private BlobStoreManagedLedgerOffloader getOffloader(BlobStore mockedBlobStore) throws IOException {
+        return getOffloader(BUCKET, mockedBlobStore);
+    }
+
+    private BlobStoreManagedLedgerOffloader getOffloader(String bucket) throws IOException {
+        mockedConfig = mock(TieredStorageConfiguration.class, delegatesTo(getConfiguration(bucket)));
+        Mockito.doReturn(blobStore).when(mockedConfig).getBlobStore(); // Use the REAL blobStore
+        BlobStoreManagedLedgerOffloader offloader = BlobStoreManagedLedgerOffloader.create(mockedConfig,
+                new HashMap<String, String>(), scheduler, scheduler, this.offloaderStats,
+                entryOffsetsCache);
+        return offloader;
+    }
+
+    private BlobStoreManagedLedgerOffloader getOffloader(String bucket, BlobStore mockedBlobStore) throws IOException {
+        mockedConfig = mock(TieredStorageConfiguration.class, delegatesTo(getConfiguration(bucket)));
+        Mockito.doReturn(mockedBlobStore).when(mockedConfig).getBlobStore();
+        BlobStoreManagedLedgerOffloader offloader = BlobStoreManagedLedgerOffloader.create(mockedConfig,
+                new HashMap<String, String>(), scheduler, scheduler, this.offloaderStats,
+                entryOffsetsCache);
+        return offloader;
+    }
+
+    @Test(timeOut = 600000)  // 10 minutes.
+    public void testHappyCase() throws Exception {
+        @Cleanup
+        LedgerOffloader offloader = getOffloader();
+        offloader.offload(buildReadHandle(), UUID.randomUUID(), new HashMap<>()).get();
+    }
+
+    @Test(timeOut = 600000)  // 10 minutes.
+    public void testBucketDoesNotExist() throws Exception {
+
+        if (provider == JCloudBlobStoreProvider.TRANSIENT) {
+            // Skip this test, since it isn't applicable.
+            return;
+        }
+
+        @Cleanup
+        LedgerOffloader offloader = getOffloader("some-non-existant-bucket-name");
+        try {
+            offloader.offload(buildReadHandle(), UUID.randomUUID(), new HashMap<>()).get();
+            Assert.fail("Shouldn't be able to add to bucket");
+        } catch (ExecutionException e) {
+            log.error("Exception: ", e);
+            Assert.assertTrue(e.getMessage().toLowerCase().contains("not found"));
+        }
+    }
+
+    @Test(timeOut = 600000)  // 10 minutes.
+    public void testOffloadAndRead() throws Exception {
+        @Cleanup
+        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 3);
+        @Cleanup
+        LedgerOffloader offloader = getOffloader();
+
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, new HashMap<>()).get();
+
+        @Cleanup
+        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
+        assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
+
+        try (LedgerEntries toWriteEntries = toWrite.read(0, toWrite.getLastAddConfirmed());
+             LedgerEntries toTestEntries = toTest.read(0, toTest.getLastAddConfirmed())) {
+            Iterator<LedgerEntry> toWriteIter = toWriteEntries.iterator();
+            Iterator<LedgerEntry> toTestIter = toTestEntries.iterator();
+
+            while (toWriteIter.hasNext() && toTestIter.hasNext()) {
+                LedgerEntry toWriteEntry = toWriteIter.next();
+                LedgerEntry toTestEntry = toTestIter.next();
+
+                assertEquals(toWriteEntry.getLedgerId(), toTestEntry.getLedgerId());
+                assertEquals(toWriteEntry.getEntryId(), toTestEntry.getEntryId());
+                assertEquals(toWriteEntry.getLength(), toTestEntry.getLength());
+                assertEquals(toWriteEntry.getEntryBuffer(), toTestEntry.getEntryBuffer());
+            }
+            Assert.assertFalse(toWriteIter.hasNext());
+            Assert.assertFalse(toTestIter.hasNext());
+        }
+    }
+
+    @Test(timeOut = 60000)
+    public void testReadHandlerState() throws Exception {
+        @Cleanup
+        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 3);
+        @Cleanup
+        LedgerOffloader offloader = getOffloader();
+
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, new HashMap<>()).get();
+
+        BlobStoreBackedReadHandleImpl toTest = (BlobStoreBackedReadHandleImpl) offloader
+                .readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
+        Assert.assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
+        Assert.assertEquals(toTest.getState(), BlobStoreBackedReadHandleImpl.State.Opened);
+        @Cleanup
+        LedgerEntries ledgerEntries = toTest.read(0, 1);
+        toTest.close();
+        Assert.assertEquals(toTest.getState(), BlobStoreBackedReadHandleImpl.State.Closed);
+    }
+
+    @Test(timeOut = 600000)  // 10 minutes.
+    public void testOffloadAndReadMetrics() throws Exception {
+        @Cleanup
+        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 3);
+        @Cleanup
+        LedgerOffloader offloader = getOffloader();
+
+        UUID uuid = UUID.randomUUID();
+        String managedLegerName = "public/default/persistent/testOffload";
+        String topic = TopicName.fromPersistenceNamingEncoding(managedLegerName);
+        Map<String, String> extraMap = new HashMap<>();
+        extraMap.put("ManagedLedgerName", managedLegerName);
+        offloader.offload(toWrite, uuid, extraMap).get();
+
+        @Cleanup
+        LedgerOffloaderStatsImpl offloaderStats = (LedgerOffloaderStatsImpl) this.offloaderStats;
+
+        assertEquals(offloaderStats.getOffloadError(topic), 0);
+        assertTrue(offloaderStats.getOffloadBytes(topic) > 0);
+        assertTrue(offloaderStats.getReadLedgerLatency(topic).count > 0);
+        assertEquals(offloaderStats.getWriteStorageError(topic), 0);
+
+        Map<String, String> map = new HashMap<>();
+        map.putAll(offloader.getOffloadDriverMetadata());
+        map.put("ManagedLedgerName", managedLegerName);
+        @Cleanup
+        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, map).get();
+        @Cleanup
+        LedgerEntries toTestEntries = toTest.read(0, toTest.getLastAddConfirmed());
+        Iterator<LedgerEntry> toTestIter = toTestEntries.iterator();
+        while (toTestIter.hasNext()) {
+            LedgerEntry toTestEntry = toTestIter.next();
+        }
+
+        assertEquals(offloaderStats.getReadOffloadError(topic), 0);
+        assertTrue(offloaderStats.getReadOffloadBytes(topic) > 0);
+        assertTrue(offloaderStats.getReadOffloadDataLatency(topic).count > 0);
+        assertTrue(offloaderStats.getReadOffloadIndexLatency(topic).count > 0);
+    }
+
+    @Test
+    public void testOffloadFailInitDataBlockUpload() throws Exception {
+        @Cleanup
+        ReadHandle readHandle = buildReadHandle();
+        UUID uuid = UUID.randomUUID();
+        String failureString = "fail InitDataBlockUpload";
+
+        // mock throw exception when initiateMultipartUpload
+        try {
+            BlobStore spiedBlobStore = mock(BlobStore.class, delegatesTo(blobStore));
+
+            Mockito
+                .doThrow(new RuntimeException(failureString))
+                .when(spiedBlobStore).initiateMultipartUpload(any(), any(), any());
+
+            @Cleanup
+            BlobStoreManagedLedgerOffloader offloader = getOffloader(spiedBlobStore);
+            offloader.offload(readHandle, uuid, new HashMap<>()).get();
+            Assert.fail("Should throw exception when initiateMultipartUpload");
+        } catch (Exception e) {
+            Assert.assertTrue(e.getCause() instanceof RuntimeException);
+            Assert.assertTrue(e.getCause().getMessage().contains(failureString));
+            Assert.assertFalse(blobStore.blobExists(BUCKET,
+                    DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
+            Assert.assertFalse(blobStore.blobExists(BUCKET,
+                    DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
+        }
+    }
+
+    @Test
+    public void testOffloadFailDataBlockPartUpload() throws Exception {
+        @Cleanup
+        ReadHandle readHandle = buildReadHandle();
+        UUID uuid = UUID.randomUUID();
+        String failureString = "fail DataBlockPartUpload";
+
+        // mock throw exception when uploadPart
+        try {
+
+            BlobStore spiedBlobStore = mock(BlobStore.class, delegatesTo(blobStore));
+            Mockito
+                .doThrow(new RuntimeException(failureString))
+                .when(spiedBlobStore).uploadMultipartPart(any(), anyInt(), any());
+
+            @Cleanup
+            BlobStoreManagedLedgerOffloader offloader = getOffloader(spiedBlobStore);
+            offloader.offload(readHandle, uuid, new HashMap<>()).get();
+            Assert.fail("Should throw exception for when uploadPart");
+        } catch (Exception e) {
+            Assert.assertTrue(e.getCause() instanceof RuntimeException);
+            Assert.assertTrue(e.getCause().getMessage().contains(failureString));
+            Assert.assertFalse(blobStore.blobExists(BUCKET,
+                    DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
+            Assert.assertFalse(blobStore.blobExists(BUCKET,
+                    DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
+        }
+    }
+
+    @Test
+    public void testOffloadFailDataBlockUploadComplete() throws Exception {
+        @Cleanup
+        ReadHandle readHandle = buildReadHandle();
+        UUID uuid = UUID.randomUUID();
+        String failureString = "fail DataBlockUploadComplete";
+
+        // mock throw exception when completeMultipartUpload
+        try {
+            BlobStore spiedBlobStore = mock(BlobStore.class, delegatesTo(blobStore));
+            Mockito
+                .doThrow(new RuntimeException(failureString))
+                .when(spiedBlobStore).completeMultipartUpload(any(), any());
+            Mockito
+                .doNothing()
+                .when(spiedBlobStore).abortMultipartUpload(any());
+
+            @Cleanup
+            BlobStoreManagedLedgerOffloader offloader = getOffloader(spiedBlobStore);
+            offloader.offload(readHandle, uuid, new HashMap<>()).get();
+
+            Assert.fail("Should throw exception for when completeMultipartUpload");
+        } catch (Exception e) {
+            // excepted
+            Assert.assertTrue(e.getCause() instanceof RuntimeException);
+            Assert.assertTrue(e.getCause().getMessage().contains(failureString));
+            Assert.assertFalse(blobStore.blobExists(BUCKET,
+                    DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
+            Assert.assertFalse(blobStore.blobExists(BUCKET,
+                    DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
+        }
+    }
+
+    @Test
+    public void testOffloadFailPutIndexBlock() throws Exception {
+        @Cleanup
+        ReadHandle readHandle = buildReadHandle();
+        UUID uuid = UUID.randomUUID();
+        String failureString = "fail putObject";
+
+        // mock throw exception when putObject
+        try {
+            BlobStore spiedBlobStore = mock(BlobStore.class, delegatesTo(blobStore));
+            Mockito
+                .doThrow(new RuntimeException(failureString))
+                .when(spiedBlobStore).putBlob(any(), any());
+
+            @Cleanup
+            BlobStoreManagedLedgerOffloader offloader = getOffloader(spiedBlobStore);
+            offloader.offload(readHandle, uuid, new HashMap<>()).get();
+
+            Assert.fail("Should throw exception for when putObject for index block");
+         } catch (Exception e) {
+            // excepted
+            Assert.assertTrue(e.getCause() instanceof RuntimeException);
+            Assert.assertTrue(e.getCause().getMessage().contains(failureString));
+            Assert.assertFalse(blobStore.blobExists(BUCKET,
+                    DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
+            Assert.assertFalse(blobStore.blobExists(BUCKET,
+                    DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
+        }
+    }
+
+    @Test(timeOut = 600000)
+    public void testOffloadReadRandomAccess() throws Exception {
+        @Cleanup
+        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 3);
+        long[][] randomAccesses = new long[10][2];
+        Random r = new Random(0);
+        for (int i = 0; i < 10; i++) {
+            long first = r.nextInt((int) toWrite.getLastAddConfirmed());
+            long second = r.nextInt((int) toWrite.getLastAddConfirmed());
+            if (second < first) {
+                long tmp = first;
+                first = second;
+                second = tmp;
+            }
+            randomAccesses[i][0] = first;
+            randomAccesses[i][1] = second;
+        }
+
+        @Cleanup
+        LedgerOffloader offloader = getOffloader();
+
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, new HashMap<>()).get();
+
+        @Cleanup
+        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
+        assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
+
+        for (long[] access : randomAccesses) {
+            try (LedgerEntries toWriteEntries = toWrite.read(access[0], access[1]);
+                 LedgerEntries toTestEntries = toTest.read(access[0], access[1])) {
+                Iterator<LedgerEntry> toWriteIter = toWriteEntries.iterator();
+                Iterator<LedgerEntry> toTestIter = toTestEntries.iterator();
+
+                while (toWriteIter.hasNext() && toTestIter.hasNext()) {
+                    LedgerEntry toWriteEntry = toWriteIter.next();
+                    LedgerEntry toTestEntry = toTestIter.next();
+
+                    assertEquals(toWriteEntry.getLedgerId(), toTestEntry.getLedgerId());
+                    assertEquals(toWriteEntry.getEntryId(), toTestEntry.getEntryId());
+                    assertEquals(toWriteEntry.getLength(), toTestEntry.getLength());
+                    assertEquals(toWriteEntry.getEntryBuffer(), toTestEntry.getEntryBuffer());
+                }
+                Assert.assertFalse(toWriteIter.hasNext());
+                Assert.assertFalse(toTestIter.hasNext());
+            }
+        }
+    }
+
+    @Test
+    public void testOffloadReadInvalidEntryIds() throws Exception {
+        @Cleanup
+        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
+        @Cleanup
+        LedgerOffloader offloader = getOffloader();
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, new HashMap<>()).get();
+
+        @Cleanup
+        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
+        assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
+
+        try {
+            toTest.read(-1, -1);
+            Assert.fail("Shouldn't be able to read anything");
+        } catch (BKException.BKIncorrectParameterException e) {
+        }
+
+        try {
+            toTest.read(0, toTest.getLastAddConfirmed() + 1);
+            Assert.fail("Shouldn't be able to read anything");
+        } catch (BKException.BKIncorrectParameterException e) {
+        }
+    }
+
+    @Test
+    public void testDeleteOffloaded() throws Exception {
+        @Cleanup
+        ReadHandle readHandle = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
+        UUID uuid = UUID.randomUUID();
+
+        @Cleanup
+        BlobStoreManagedLedgerOffloader offloader = getOffloader();
+
+        // verify object exist after offload
+        offloader.offload(readHandle, uuid, new HashMap<>()).get();
+        Assert.assertTrue(blobStore.blobExists(BUCKET,
+                DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
+        Assert.assertTrue(blobStore.blobExists(BUCKET,
+                DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
+
+        // verify object deleted after delete
+        offloader.deleteOffloaded(readHandle.getId(), uuid, config.getOffloadDriverMetadata()).get();
+        Assert.assertFalse(blobStore.blobExists(BUCKET,
+                DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
+        Assert.assertFalse(blobStore.blobExists(BUCKET,
+                DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
+    }
+
+    @Test
+    public void testDeleteOffloadedFail() throws Exception {
+        String failureString = "fail deleteOffloaded";
+        @Cleanup
+        ReadHandle readHandle = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
+        UUID uuid = UUID.randomUUID();
+
+        BlobStore spiedBlobStore = mock(BlobStore.class, delegatesTo(blobStore));
+
+        Mockito
+            .doThrow(new RuntimeException(failureString))
+            .when(spiedBlobStore).removeBlobs(any(), any());
+
+        @Cleanup
+        BlobStoreManagedLedgerOffloader offloader = getOffloader(spiedBlobStore);
+
+        try {
+            // verify object exist after offload
+            offloader.offload(readHandle, uuid, new HashMap<>()).get();
+            Assert.assertTrue(blobStore.blobExists(BUCKET,
+                    DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
+            Assert.assertTrue(blobStore.blobExists(BUCKET,
+                    DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
+
+            offloader.deleteOffloaded(readHandle.getId(), uuid, config.getOffloadDriverMetadata()).get();
+        } catch (Exception e) {
+            // expected
+            Assert.assertTrue(e.getCause().getMessage().contains(failureString));
+            // verify object still there.
+            Assert.assertTrue(blobStore.blobExists(BUCKET,
+                    DataBlockUtils.dataBlockOffloadKey(readHandle.getId(), uuid)));
+            Assert.assertTrue(blobStore.blobExists(BUCKET,
+                    DataBlockUtils.indexBlockOffloadKey(readHandle.getId(), uuid)));
+        }
+    }
+
+    @Test
+    public void testOffloadEmpty() throws Exception {
+        CompletableFuture<LedgerEntries> noEntries = new CompletableFuture<>();
+        noEntries.completeExceptionally(new BKException.BKReadException());
+
+        ReadHandle readHandle = Mockito.mock(ReadHandle.class);
+        Mockito.doReturn(-1L).when(readHandle).getLastAddConfirmed();
+        Mockito.doReturn(noEntries).when(readHandle).readAsync(anyLong(), anyLong());
+        Mockito.doReturn(0L).when(readHandle).getLength();
+        Mockito.doReturn(true).when(readHandle).isClosed();
+        Mockito.doReturn(1234L).when(readHandle).getId();
+
+        UUID uuid = UUID.randomUUID();
+        @Cleanup
+        LedgerOffloader offloader = getOffloader();
+
+        try {
+            offloader.offload(readHandle, uuid, new HashMap<>()).get();
+            Assert.fail("Shouldn't have been able to offload");
+        } catch (ExecutionException e) {
+            assertEquals(e.getCause().getClass(), IllegalArgumentException.class);
+        }
+    }
+
+    @Test
+    public void testReadUnknownDataVersion() throws Exception {
+        @Cleanup
+        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
+        BlobStoreManagedLedgerOffloader offloader = getOffloader();
+
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, new HashMap<>()).get();
+
+        String dataKey = DataBlockUtils.dataBlockOffloadKey(toWrite.getId(), uuid);
+
+        // Here it will return a Immutable map.
+        Assert.assertTrue(blobStore.blobExists(BUCKET, dataKey));
+        Map<String, String> immutableMap = blobStore.blobMetadata(BUCKET, dataKey).getUserMetadata();
+        Map<String, String> userMeta = Maps.newHashMap();
+        userMeta.putAll(immutableMap);
+        userMeta.put(DataBlockUtils.METADATA_FORMAT_VERSION_KEY.toLowerCase(), String.valueOf(-12345));
+        blobStore.copyBlob(BUCKET, dataKey, BUCKET, dataKey, CopyOptions.builder().userMetadata(userMeta).build());
+
+        try (ReadHandle toRead = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get()) {
+            toRead.readAsync(0, 0).get();
+            Assert.fail("Shouldn't have been able to read");
+        } catch (ExecutionException e) {
+            log.error("Exception: ", e);
+            assertEquals(e.getCause().getClass(), IOException.class);
+            Assert.assertTrue(e.getCause().getMessage().contains("Error reading from BlobStore"));
+        }
+
+        userMeta.put(DataBlockUtils.METADATA_FORMAT_VERSION_KEY.toLowerCase(), String.valueOf(12345));
+        blobStore.copyBlob(BUCKET, dataKey, BUCKET, dataKey, CopyOptions.builder().userMetadata(userMeta).build());
+
+        try (ReadHandle toRead = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get()) {
+            toRead.readAsync(0, 0).get();
+            Assert.fail("Shouldn't have been able to read");
+        } catch (ExecutionException e) {
+            assertEquals(e.getCause().getClass(), IOException.class);
+            Assert.assertTrue(e.getCause().getMessage().contains("Error reading from BlobStore"));
+        }
+    }
+
+    @Test
+    public void testReadUnknownIndexVersion() throws Exception {
+        @Cleanup
+        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
+        BlobStoreManagedLedgerOffloader offloader = getOffloader();
+
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, new HashMap<>()).get();
+
+        String indexKey = DataBlockUtils.indexBlockOffloadKey(toWrite.getId(), uuid);
+
+        // Here it will return a Immutable map.
+        Map<String, String> immutableMap = blobStore.blobMetadata(BUCKET, indexKey).getUserMetadata();
+        Map<String, String> userMeta = Maps.newHashMap();
+        userMeta.putAll(immutableMap);
+        userMeta.put(DataBlockUtils.METADATA_FORMAT_VERSION_KEY.toLowerCase(), String.valueOf(-12345));
+        blobStore.copyBlob(BUCKET, indexKey, BUCKET, indexKey, CopyOptions.builder().userMetadata(userMeta).build());
+
+        try {
+            offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
+            Assert.fail("Shouldn't have been able to open");
+        } catch (ExecutionException e) {
+            assertEquals(e.getCause().getClass(), IOException.class);
+            Assert.assertTrue(e.getCause().getMessage().contains("Invalid object version"));
+        }
+
+        userMeta.put(DataBlockUtils.METADATA_FORMAT_VERSION_KEY.toLowerCase(), String.valueOf(12345));
+        blobStore.copyBlob(BUCKET, indexKey, BUCKET, indexKey, CopyOptions.builder().userMetadata(userMeta).build());
+
+        try {
+            offloader.readOffloaded(toWrite.getId(), uuid, config.getOffloadDriverMetadata()).get();
+            Assert.fail("Shouldn't have been able to open");
+        } catch (ExecutionException e) {
+            assertEquals(e.getCause().getClass(), IOException.class);
+            Assert.assertTrue(e.getCause().getMessage().contains("Invalid object version"));
+        }
+    }
+
+    @Test
+    public void testReadEOFException() throws Throwable {
+        @Cleanup
+        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
+        @Cleanup
+        LedgerOffloader offloader = getOffloader();
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, new HashMap<>()).get();
+
+        @Cleanup
+        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
+        Assert.assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
+        @Cleanup
+        LedgerEntries ledgerEntries = toTest.readAsync(0, toTest.getLastAddConfirmed()).get();
+
+        try {
+            @Cleanup
+            LedgerEntries ledgerEntries2 = toTest.readAsync(0, 0).get();
+        } catch (Exception e) {
+            Assert.fail("Get unexpected exception when reading entries", e);
+        }
+    }
+
+    @Test(timeOut = 600000)  // 10 minutes.
+    public void testScanLedgers() throws Exception {
+        @Cleanup
+        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 3);
+        @Cleanup
+        LedgerOffloader offloader = getOffloader();
+
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, new HashMap<>()).get();
+
+        List<OffloadedLedgerMetadata> result = new ArrayList<>();
+        offloader.scanLedgers(
+                (m) -> {
+                    log.info("found {}", m);
+                    if (m.getLedgerId() == toWrite.getId()) {
+                        result.add(m);
+                    }
+                    return true;
+                }, offloader.getOffloadDriverMetadata());
+        assertEquals(2, result.size());
+
+        // data and index
+
+        OffloadedLedgerMetadata offloadedLedgerMetadata = result.get(0);
+        assertEquals(toWrite.getId(), offloadedLedgerMetadata.getLedgerId());
+
+        OffloadedLedgerMetadata offloadedLedgerMetadata2 = result.get(1);
+        assertEquals(toWrite.getId(), offloadedLedgerMetadata2.getLedgerId());
+    }
+
+    @Test
+    public void testReadWithAClosedLedgerHandler() throws Exception {
+        @Cleanup
+        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 1);
+        @Cleanup
+        LedgerOffloader offloader = getOffloader();
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, new HashMap<>()).get();
+
+        @Cleanup
+        ReadHandle toTest = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
+        Assert.assertEquals(toTest.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
+        long lac = toTest.getLastAddConfirmed();
+        @Cleanup
+        LedgerEntries ledgerEntries = toTest.readAsync(0, lac).get();
+        toTest.closeAsync().get();
+        try {
+            toTest.readAsync(0, lac).get();
+        } catch (Exception e) {
+            if (e.getCause() instanceof ManagedLedgerException.OffloadReadHandleClosedException) {
+                // expected exception
+                return;
+            }
+            throw e;
+        }
+    }
+
+    @Test
+    public void testReadNotExistLedger() throws Exception {
+        @Cleanup
+        ReadHandle toWrite = buildReadHandle(DEFAULT_BLOCK_SIZE, 3);
+        @Cleanup
+        LedgerOffloader offloader = getOffloader();
+
+        UUID uuid = UUID.randomUUID();
+        offloader.offload(toWrite, uuid, new HashMap<>()).get();
+        @Cleanup
+        ReadHandle offloadRead = offloader.readOffloaded(toWrite.getId(), uuid, Collections.emptyMap()).get();
+        assertEquals(offloadRead.getLastAddConfirmed(), toWrite.getLastAddConfirmed());
+
+        // delete blob(ledger)
+        blobStore.removeBlob(BUCKET, DataBlockUtils.dataBlockOffloadKey(toWrite.getId(), uuid));
+
+        try {
+            offloadRead.read(0, offloadRead.getLastAddConfirmed());
+            fail("Should be read fail");
+        } catch (BKException e) {
+            assertEquals(e.getCode(), NoSuchLedgerExistsException);
+        }
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlockAwareSegmentInputStreamTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlockAwareSegmentInputStreamTest.java
index 1d1fc91fd0..6b8c95c6b1 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlockAwareSegmentInputStreamTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlockAwareSegmentInputStreamTest.java
@@ -1,820 +1,820 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static org.testng.Assert.assertEquals;
-import static org.testng.Assert.assertNotEquals;
-import static org.testng.Assert.fail;
-import static org.testng.internal.junit.ArrayAsserts.assertArrayEquals;
-import com.google.common.io.ByteStreams;
-import com.google.common.primitives.Ints;
-import com.google.common.primitives.Longs;
-import io.netty.buffer.ByteBuf;
-import io.netty.buffer.Unpooled;
-import java.io.ByteArrayInputStream;
-import java.io.IOException;
-import java.lang.reflect.Field;
-import java.nio.ByteBuffer;
-import java.util.Arrays;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Random;
-import java.util.concurrent.CompletableFuture;
-import java.util.function.Supplier;
-import java.util.stream.IntStream;
-import lombok.Cleanup;
-import lombok.Data;
-import lombok.extern.slf4j.Slf4j;
-import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;
-import org.apache.bookkeeper.client.api.LedgerEntries;
-import org.apache.bookkeeper.client.api.LedgerEntry;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.client.api.ReadHandle;
-import org.apache.bookkeeper.mledger.offload.jcloud.DataBlockHeader;
-import org.testng.annotations.DataProvider;
-import org.testng.annotations.Test;
-import org.testng.collections.Lists;
-
-@Slf4j
-public class BlockAwareSegmentInputStreamTest {
-    private static final byte DEFAULT_ENTRY_BYTE = 0xB;
-
-    @Data
-    class MockLedgerEntry implements LedgerEntry {
-        long ledgerId;
-        long entryId;
-        long length;
-        byte entryBytes[];
-        ByteBuf entryBuffer;
-
-        MockLedgerEntry(long ledgerId, long entryId, long length,
-                        Supplier<Byte> dataSupplier) {
-            this.ledgerId = ledgerId;
-            this.entryId = entryId;
-            this.length = length;
-            this.entryBytes = new byte[(int) length];
-            entryBuffer = Unpooled.wrappedBuffer(entryBytes);
-            entryBuffer.writerIndex(0);
-            IntStream.range(0, (int) length).forEach(i -> entryBuffer.writeByte(dataSupplier.get()));
-        }
-
-        @Override
-        public ByteBuffer getEntryNioBuffer() {
-            return null;
-        }
-
-        @Override
-        public LedgerEntry duplicate() {
-            return null;
-        }
-
-        @Override
-        public void close() {
-            entryBuffer.release();
-        }
-    }
-
-    @Data
-    class MockLedgerEntries implements LedgerEntries {
-        int ledgerId;
-        int startEntryId;
-        int count;
-        int entrySize;
-        List<LedgerEntry> entries;
-
-        MockLedgerEntries(int ledgerId, int startEntryId, int count, int entrySize, Supplier<Byte> dataSupplier) {
-            this.ledgerId = ledgerId;
-            this.startEntryId = startEntryId;
-            this.count = count;
-            this.entrySize = entrySize;
-            this.entries = Lists.newArrayList(count);
-
-            IntStream.range(startEntryId, startEntryId + count).forEach(i ->
-                    entries.add(new MockLedgerEntry(ledgerId, i, entrySize, dataSupplier)));
-        }
-
-        @Override
-        public void close() {
-            entries.clear();
-        }
-
-        @Override
-        public LedgerEntry getEntry(long entryId) {
-            if (entryId < startEntryId || entryId >= startEntryId + count) {
-                return null;
-            }
-
-            return entries.get(((int) entryId - startEntryId));
-        }
-
-        @Override
-        public Iterator<LedgerEntry> iterator() {
-            return entries.iterator();
-        }
-    }
-
-    class MockReadHandle implements ReadHandle {
-        int ledgerId;
-        int entrySize;
-        int lac;
-        Supplier<Byte> dataSupplier;
-
-        MockReadHandle(int ledgerId, int entrySize, int lac, Supplier<Byte> dataSupplier) {
-            this.ledgerId = ledgerId;
-            this.entrySize = entrySize;
-            this.lac = lac;
-            this.dataSupplier = dataSupplier;
-        }
-
-        MockReadHandle(int ledgerId, int entrySize, int lac) {
-            this(ledgerId, entrySize, lac, () -> DEFAULT_ENTRY_BYTE);
-        }
-
-        @Override
-        public CompletableFuture<LedgerEntries> readAsync(long firstEntry, long lastEntry) {
-            CompletableFuture<LedgerEntries> future = new CompletableFuture<>();
-            LedgerEntries entries = new MockLedgerEntries(ledgerId,
-                (int) firstEntry,
-                (int) (lastEntry - firstEntry + 1),
-                    entrySize, dataSupplier);
-
-            future.complete(entries);
-            return future;
-        }
-
-        @Override
-        public CompletableFuture<LedgerEntries> readUnconfirmedAsync(long firstEntry, long lastEntry) {
-            return readAsync(firstEntry, lastEntry);
-        }
-
-        @Override
-        public CompletableFuture<Long> readLastAddConfirmedAsync() {
-            return null;
-        }
-
-        @Override
-        public CompletableFuture<Long> tryReadLastAddConfirmedAsync() {
-            return null;
-        }
-
-        @Override
-        public long getLastAddConfirmed() {
-            return lac;
-        }
-
-        @Override
-        public long getLength() {
-            return (lac + 1) * entrySize;
-        }
-
-        @Override
-        public boolean isClosed() {
-            return true;
-        }
-
-        @Override
-        public CompletableFuture<LastConfirmedAndEntry>
-        readLastAddConfirmedAndEntryAsync(long entryId, long timeOutInMillis, boolean parallel) {
-            return null;
-        }
-
-        @Override
-        public LedgerMetadata getLedgerMetadata() {
-            return null;
-        }
-
-        @Override
-        public long getId() {
-            return ledgerId;
-        }
-
-        @Override
-        public CompletableFuture<Void> closeAsync() {
-            return null;
-        }
-    }
-
-    @DataProvider(name = "useBufferRead")
-    public static Object[][] useBufferRead() {
-        return new Object[][]{
-            {Boolean.TRUE},
-            {Boolean.FALSE}
-        };
-    }
-
-    @Test(dataProvider = "useBufferRead")
-    public void testHaveEndPadding(boolean useBufferRead) throws Exception {
-        int ledgerId = 1;
-        int entrySize = 8;
-        int lac = 160;
-        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac);
-
-        // set block size bigger than to (header + entry) size.
-        int blockSize = 3148 + 5;
-        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
-        int expectedEntryCount = (blockSize - DataBlockHeaderImpl.getDataStartOffset()) / (entrySize + 4 + 8);
-
-        // verify get methods
-        assertEquals(inputStream.getLedger(), readHandle);
-        assertEquals(inputStream.getStartEntryId(), 0);
-        assertEquals(inputStream.getBlockSize(), blockSize);
-
-        // verify read inputStream
-        // 1. read header. 128
-        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
-        if (useBufferRead) {
-            int ret = inputStream.read(headerB, 0, DataBlockHeaderImpl.getDataStartOffset());
-            assertEquals(DataBlockHeaderImpl.getDataStartOffset(), ret);
-        } else {
-            ByteStreams.readFully(inputStream, headerB);
-        }
-        DataBlockHeader headerRead = DataBlockHeaderImpl.fromStream(new ByteArrayInputStream(headerB));
-        assertEquals(headerRead.getBlockLength(), blockSize);
-        assertEquals(headerRead.getFirstEntryId(), 0);
-
-        byte[] entryData = new byte[entrySize];
-        Arrays.fill(entryData, (byte) 0xB); // 0xB is MockLedgerEntry.blockPadding
-
-        // 2. read Ledger entries. 201 * 20
-        IntStream.range(0, expectedEntryCount).forEach(i -> {
-            try {
-                byte lengthBuf[] = new byte[4];
-                byte entryIdBuf[] = new byte[8];
-                byte content[] = new byte[entrySize];
-                if (useBufferRead) {
-                    int read = inputStream.read(lengthBuf, 0, 4);
-                    assertEquals(read, 4);
-                    read = inputStream.read(entryIdBuf, 0, 8);
-                    assertEquals(read, 8);
-                    read = inputStream.read(content, 0, entrySize);
-                    assertEquals(read, entrySize);
-                } else {
-                    inputStream.read(lengthBuf);
-                    inputStream.read(entryIdBuf);
-                    inputStream.read(content);
-                }
-
-                assertEquals(entrySize, Ints.fromByteArray(lengthBuf));
-                assertEquals(i, Longs.fromByteArray(entryIdBuf));
-                assertArrayEquals(entryData, content);
-            } catch (Exception e) {
-                fail("meet exception", e);
-            }
-        });
-
-        // 3. read padding
-        int left = blockSize - DataBlockHeaderImpl.getDataStartOffset() -  expectedEntryCount * (entrySize + 4 + 8);
-        assertEquals(left, 5);
-        byte padding[] = new byte[left];
-        if (useBufferRead) {
-            int ret = 0;
-            int offset = 0;
-            while ((ret = inputStream.read(padding, offset, padding.length - offset)) > 0) {
-                offset += ret;
-            }
-            assertEquals(inputStream.read(padding, 0, padding.length), -1);
-        } else {
-            int len = left;
-            int offset = 0;
-            byte[] buf = new byte[4];
-            while (len > 0) {
-                int ret = inputStream.read(buf);
-                for (int i = 0; i < ret; i++) {
-                    padding[offset++] = buf[i];
-                }
-                len -= ret;
-            }
-        }
-        ByteBuf paddingBuf = Unpooled.wrappedBuffer(padding);
-        IntStream.range(0, paddingBuf.capacity() / 4).forEach(i ->
-            assertEquals(Integer.toHexString(paddingBuf.readInt()),
-                         Integer.toHexString(0xFEDCDEAD)));
-
-        // 4. reach end.
-        if (useBufferRead) {
-            byte[] b = new byte[4];
-            int ret = inputStream.read(b, 0, 4);
-            assertEquals(ret, -1);
-        }
-        assertEquals(inputStream.read(), -1);
-
-        assertEquals(inputStream.getBlockEntryCount(), expectedEntryCount);
-        assertEquals(inputStream.getBlockEntryBytesCount(), entrySize * expectedEntryCount);
-        assertEquals(inputStream.getEndEntryId(), expectedEntryCount - 1);
-
-        inputStream.close();
-    }
-
-    @Test(dataProvider = "useBufferRead")
-    public void testNoEndPadding(boolean useBufferRead) throws Exception {
-        int ledgerId = 1;
-        int entrySize = 8;
-        int lac = 120;
-        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac);
-
-        // set block size equals to (header + entry) size.
-        int blockSize = 2148;
-        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
-        int expectedEntryCount = (blockSize - DataBlockHeaderImpl.getDataStartOffset())
-            / (entrySize + BlockAwareSegmentInputStreamImpl.ENTRY_HEADER_SIZE);
-
-        // verify get methods
-        assertEquals(inputStream.getLedger(), readHandle);
-        assertEquals(inputStream.getStartEntryId(), 0);
-        assertEquals(inputStream.getBlockSize(), blockSize);
-
-        // verify read inputStream
-        // 1. read header. 128
-        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
-        if (useBufferRead) {
-            int ret = inputStream.read(headerB, 0, DataBlockHeaderImpl.getDataStartOffset());
-            assertEquals(DataBlockHeaderImpl.getDataStartOffset(), ret);
-        } else {
-            ByteStreams.readFully(inputStream, headerB);
-        }
-        DataBlockHeader headerRead = DataBlockHeaderImpl.fromStream(new ByteArrayInputStream(headerB));
-        assertEquals(headerRead.getBlockLength(), blockSize);
-        assertEquals(headerRead.getFirstEntryId(), 0);
-
-        byte[] entryData = new byte[entrySize];
-        Arrays.fill(entryData, (byte) 0xB); // 0xB is MockLedgerEntry.blockPadding
-
-        // 2. read Ledger entries. 201 * 20
-        IntStream.range(0, expectedEntryCount).forEach(i -> {
-            try {
-                byte lengthBuf[] = new byte[4];
-                byte entryIdBuf[] = new byte[8];
-                byte content[] = new byte[entrySize];
-                if (useBufferRead) {
-                    int read = inputStream.read(lengthBuf, 0, 4);
-                    assertEquals(read, 4);
-                    read = inputStream.read(entryIdBuf, 0, 8);
-                    assertEquals(read, 8);
-                    read = inputStream.read(content, 0, entrySize);
-                    assertEquals(read, entrySize);
-                } else {
-                    inputStream.read(lengthBuf);
-                    inputStream.read(entryIdBuf);
-                    inputStream.read(content);
-                }
-
-                assertEquals(entrySize, Ints.fromByteArray(lengthBuf));
-                assertEquals(i, Longs.fromByteArray(entryIdBuf));
-                assertArrayEquals(entryData, content);
-            } catch (Exception e) {
-                fail("meet exception", e);
-            }
-        });
-
-        // 3. should be no padding
-        int left = blockSize - DataBlockHeaderImpl.getDataStartOffset() -  expectedEntryCount * (entrySize + 4 + 8);
-        assertEquals(left, 0);
-
-        // 4. reach end.
-        if (useBufferRead) {
-            byte[] b = new byte[4];
-            int ret = inputStream.read(b, 0, 4);
-            assertEquals(ret, -1);
-        }
-        assertEquals(inputStream.read(), -1);
-
-        assertEquals(inputStream.getBlockEntryCount(), expectedEntryCount);
-        assertEquals(inputStream.getBlockEntryBytesCount(), entrySize * expectedEntryCount);
-        assertEquals(inputStream.getEndEntryId(), expectedEntryCount - 1);
-
-        inputStream.close();
-    }
-
-    @Test(dataProvider = "useBufferRead")
-    public void testReadTillLac(boolean useBufferRead) throws Exception {
-        // simulate last data block read.
-        int ledgerId = 1;
-        int entrySize = 8;
-        int lac = 89;
-        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac);
-
-        // set block size equals to (header + lac_entry) size.
-        int blockSize = DataBlockHeaderImpl.getDataStartOffset() + (1 + lac) * (entrySize + 4 + 8);
-        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
-        int expectedEntryCount = (blockSize - DataBlockHeaderImpl.getDataStartOffset()) / (entrySize + 4 + 8);
-
-        // verify get methods
-        assertEquals(inputStream.getLedger(), readHandle);
-        assertEquals(inputStream.getStartEntryId(), 0);
-        assertEquals(inputStream.getBlockSize(), blockSize);
-
-        // verify read inputStream
-        // 1. read header. 128
-        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
-        if (useBufferRead) {
-            int ret = inputStream.read(headerB, 0, DataBlockHeaderImpl.getDataStartOffset());
-            assertEquals(DataBlockHeaderImpl.getDataStartOffset(), ret);
-        } else {
-            ByteStreams.readFully(inputStream, headerB);
-        }
-        DataBlockHeader headerRead = DataBlockHeaderImpl.fromStream(new ByteArrayInputStream(headerB));
-        assertEquals(headerRead.getBlockLength(), blockSize);
-        assertEquals(headerRead.getFirstEntryId(), 0);
-
-        byte[] entryData = new byte[entrySize];
-        Arrays.fill(entryData, (byte) 0xB); // 0xB is MockLedgerEntry.blockPadding
-
-        // 2. read Ledger entries. 96 * 20
-        IntStream.range(0, expectedEntryCount).forEach(i -> {
-            try {
-                byte lengthBuf[] = new byte[4];
-                byte entryIdBuf[] = new byte[8];
-                byte content[] = new byte[entrySize];
-                if (useBufferRead) {
-                    int read = inputStream.read(lengthBuf, 0, 4);
-                    assertEquals(read, 4);
-                    read = inputStream.read(entryIdBuf, 0, 8);
-                    assertEquals(read, 8);
-                    read = inputStream.read(content, 0, entrySize);
-                    assertEquals(read, entrySize);
-                } else {
-                    inputStream.read(lengthBuf);
-                    inputStream.read(entryIdBuf);
-                    inputStream.read(content);
-                }
-
-                assertEquals(entrySize, Ints.fromByteArray(lengthBuf));
-                assertEquals(i, Longs.fromByteArray(entryIdBuf));
-                assertArrayEquals(entryData, content);
-            } catch (Exception e) {
-                fail("meet exception", e);
-            }
-        });
-
-        // 3. should have no padding
-        int left = blockSize - DataBlockHeaderImpl.getDataStartOffset() -  expectedEntryCount * (entrySize + 4 + 8);
-        assertEquals(left, 0);
-
-        // 4. reach end.
-        if (useBufferRead) {
-            byte[] b = new byte[4];
-            int ret = inputStream.read(b, 0, 4);
-            assertEquals(ret, -1);
-        }
-        assertEquals(inputStream.read(), -1);
-
-        assertEquals(inputStream.getBlockEntryCount(), expectedEntryCount);
-        assertEquals(inputStream.getBlockEntryBytesCount(), entrySize * expectedEntryCount);
-        assertEquals(inputStream.getEndEntryId(), expectedEntryCount - 1);
-
-        inputStream.close();
-    }
-
-    @Test(dataProvider = "useBufferRead")
-    public void testNoEntryPutIn(boolean useBufferRead) throws Exception {
-        // simulate first entry size over the block size budget, it shouldn't be added.
-        // 2 entries, each with bigger size than block size, so there should no entry added into block.
-        int ledgerId = 1;
-        int entrySize = 1000;
-        int lac = 1;
-        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac);
-
-        // set block size not able to hold one entry
-        int blockSize = DataBlockHeaderImpl.getDataStartOffset() + entrySize;
-        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
-        int expectedEntryCount = 0;
-
-        // verify get methods
-        assertEquals(inputStream.getLedger(), readHandle);
-        assertEquals(inputStream.getStartEntryId(), 0);
-        assertEquals(inputStream.getBlockSize(), blockSize);
-
-        // verify read inputStream
-        // 1. read header. 128
-        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
-        if (useBufferRead) {
-            int ret = inputStream.read(headerB, 0, DataBlockHeaderImpl.getDataStartOffset());
-            assertEquals(DataBlockHeaderImpl.getDataStartOffset(), ret);
-        } else {
-            ByteStreams.readFully(inputStream, headerB);
-        }
-        DataBlockHeader headerRead = DataBlockHeaderImpl.fromStream(new ByteArrayInputStream(headerB));
-        assertEquals(headerRead.getBlockLength(), blockSize);
-        assertEquals(headerRead.getFirstEntryId(), 0);
-
-
-        // 2. since no entry put in, it should only get padding after header.
-        byte padding[] = new byte[blockSize - DataBlockHeaderImpl.getDataStartOffset()];
-        if (useBufferRead) {
-            int ret = 0;
-            int offset = 0;
-            while ((ret = inputStream.read(padding, offset, padding.length - offset)) > 0) {
-                offset += ret;
-            }
-            assertEquals(inputStream.read(padding, 0, padding.length), -1);
-        } else {
-            int len = padding.length;
-            int offset = 0;
-            byte[] buf = new byte[4];
-            while (len > 0) {
-                int ret = inputStream.read(buf);
-                for (int i = 0; i < ret; i++) {
-                    padding[offset++] = buf[i];
-                }
-                len -= ret;
-            }
-        }
-        ByteBuf paddingBuf = Unpooled.wrappedBuffer(padding);
-        IntStream.range(0, paddingBuf.capacity() / 4).forEach(i ->
-            assertEquals(Integer.toHexString(paddingBuf.readInt()),
-                         Integer.toHexString(0xFEDCDEAD)));
-
-        // 3. reach end.
-        if (useBufferRead) {
-            byte[] b = new byte[4];
-            int ret = inputStream.read(b, 0, 4);
-            assertEquals(ret, -1);
-        }
-        assertEquals(inputStream.read(), -1);
-
-        assertEquals(inputStream.getBlockEntryCount(), 0);
-        assertEquals(inputStream.getBlockEntryBytesCount(), 0);
-        assertEquals(inputStream.getEndEntryId(), -1);
-
-        inputStream.close();
-    }
-
-    @Test(dataProvider = "useBufferRead")
-    public void testPaddingOnLastBlock(boolean useBufferRead) throws Exception {
-        int ledgerId = 1;
-        int entrySize = 1000;
-        int lac = 0;
-        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac);
-
-        // set block size not able to hold one entry
-        int blockSize = DataBlockHeaderImpl.getDataStartOffset() + entrySize * 2;
-        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
-        int expectedEntryCount = 1;
-
-        // verify get methods
-        assertEquals(inputStream.getLedger(), readHandle);
-        assertEquals(inputStream.getStartEntryId(), 0);
-        assertEquals(inputStream.getBlockSize(), blockSize);
-
-        // verify read inputStream
-        // 1. read header. 128
-        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
-        if (useBufferRead) {
-            int ret = inputStream.read(headerB, 0, DataBlockHeaderImpl.getDataStartOffset());
-            assertEquals(DataBlockHeaderImpl.getDataStartOffset(), ret);
-        } else {
-            ByteStreams.readFully(inputStream, headerB);
-        }
-        DataBlockHeader headerRead = DataBlockHeaderImpl.fromStream(new ByteArrayInputStream(headerB));
-        assertEquals(headerRead.getBlockLength(), blockSize);
-        assertEquals(headerRead.getFirstEntryId(), 0);
-
-        // 2. There should be a single entry
-        byte[] entryData = new byte[entrySize];
-        Arrays.fill(entryData, (byte) 0xB); // 0xB is MockLedgerEntry.blockPadding
-
-        IntStream.range(0, expectedEntryCount).forEach(i -> {
-            try {
-                byte lengthBuf[] = new byte[4];
-                byte entryIdBuf[] = new byte[8];
-                byte content[] = new byte[entrySize];
-                if (useBufferRead) {
-                    int read = inputStream.read(lengthBuf, 0, 4);
-                    assertEquals(read, 4);
-                    read = inputStream.read(entryIdBuf, 0, 8);
-                    assertEquals(read, 8);
-                    read = inputStream.read(content, 0, entrySize);
-                    assertEquals(read, entrySize);
-                } else {
-                    inputStream.read(lengthBuf);
-                    inputStream.read(entryIdBuf);
-                    inputStream.read(content);
-                }
-
-                assertEquals(entrySize, Ints.fromByteArray(lengthBuf));
-                assertEquals(i, Longs.fromByteArray(entryIdBuf));
-                assertArrayEquals(entryData, content);
-            } catch (Exception e) {
-                fail("meet exception", e);
-            }
-        });
-
-        // 3. Then padding
-        int consumedBytes = DataBlockHeaderImpl.getDataStartOffset()
-            + expectedEntryCount * (entrySize + BlockAwareSegmentInputStreamImpl.ENTRY_HEADER_SIZE);
-        byte padding[] = new byte[blockSize - consumedBytes];
-        if (useBufferRead) {
-            int ret = 0;
-            int offset = 0;
-            while ((ret = inputStream.read(padding, offset, padding.length - offset)) > 0) {
-                offset += ret;
-            }
-            assertEquals(inputStream.read(padding, 0, padding.length), -1);
-        } else {
-            int len = blockSize - consumedBytes;
-            int offset = 0;
-            byte[] buf = new byte[4];
-            while (len > 0) {
-                int ret = inputStream.read(buf);
-                for (int i = 0; i < ret; i++) {
-                    padding[offset++] = buf[i];
-                }
-                len -= ret;
-            }
-        }
-        ByteBuf paddingBuf = Unpooled.wrappedBuffer(padding);
-        IntStream.range(0, paddingBuf.capacity() / 4).forEach(i ->
-                assertEquals(Integer.toHexString(paddingBuf.readInt()),
-                             Integer.toHexString(0xFEDCDEAD)));
-
-        // 3. reach end.
-        if (useBufferRead) {
-            byte[] b = new byte[4];
-            int ret = inputStream.read(b, 0, 4);
-            assertEquals(ret, -1);
-        }
-        assertEquals(inputStream.read(), -1);
-
-        assertEquals(inputStream.getBlockEntryCount(), 1);
-        assertEquals(inputStream.getBlockEntryBytesCount(), entrySize);
-        assertEquals(inputStream.getEndEntryId(), 0);
-
-        inputStream.close();
-    }
-
-    @Test
-    public void testOnlyNegativeOnEOF() throws Exception {
-        int ledgerId = 1;
-        int entrySize = 10000;
-        int lac = 0;
-
-        Random r = new Random(0);
-        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac, () -> (byte) r.nextInt());
-
-        int blockSize = DataBlockHeaderImpl.getDataStartOffset() + entrySize * 2;
-        @Cleanup
-        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
-
-        int bytesRead = 0;
-        for (int i = 0; i < blockSize * 2; i++) {
-            int ret = inputStream.read();
-            if (ret < 0) { // should only be EOF
-                assertEquals(bytesRead, blockSize);
-                break;
-            } else {
-                bytesRead++;
-            }
-        }
-    }
-
-    @Test
-    public void testOnlyNegativeOnEOFWithBufferedRead() throws IOException {
-        int ledgerId = 1;
-        int entrySize = 10000;
-        int lac = 0;
-
-        Random r = new Random(0);
-        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac, () -> (byte) r.nextInt());
-
-        int blockSize = DataBlockHeaderImpl.getDataStartOffset() + entrySize * 2;
-        @Cleanup
-        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
-
-        int bytesRead = 0;
-        int ret;
-        int offset = 0;
-        int resetOffsetCount = 0;
-        byte[] buf = new byte[1024];
-        while ((ret = inputStream.read(buf, offset, buf.length - offset)) > 0) {
-            bytesRead += ret;
-            int currentOffset = offset;
-            offset = (offset + ret) % buf.length;
-            if (offset < currentOffset) {
-                resetOffsetCount++;
-            }
-        }
-        assertEquals(bytesRead, blockSize);
-        assertNotEquals(resetOffsetCount, 0);
-    }
-
-    // This test is for testing the read(byte[] buf, int off, int len) method can work properly
-    // on the offset not 0.
-    @Test
-    public void testReadTillLacWithSmallBuffer() throws Exception {
-        // simulate last data block read.
-        int ledgerId = 1;
-        int entrySize = 8;
-        int lac = 89;
-        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac);
-
-        // set block size equals to (header + lac_entry) size.
-        int blockSize = DataBlockHeaderImpl.getDataStartOffset() + (1 + lac) * (entrySize + 4 + 8);
-        @Cleanup
-        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
-        int expectedEntryCount = (blockSize - DataBlockHeaderImpl.getDataStartOffset()) / (entrySize + 4 + 8);
-
-        // verify get methods
-        assertEquals(inputStream.getLedger(), readHandle);
-        assertEquals(inputStream.getStartEntryId(), 0);
-        assertEquals(inputStream.getBlockSize(), blockSize);
-
-        // verify read inputStream
-        // 1. read header. 128
-        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
-        // read twice to test the offset not 0 case
-        int ret = inputStream.read(headerB, 0, 66);
-        assertEquals(ret, 66);
-        ret = inputStream.read(headerB, 66, headerB.length - 66);
-        assertEquals(headerB.length - 66, ret);
-        DataBlockHeader headerRead = DataBlockHeaderImpl.fromStream(new ByteArrayInputStream(headerB));
-        assertEquals(headerRead.getBlockLength(), blockSize);
-        assertEquals(headerRead.getFirstEntryId(), 0);
-
-        byte[] entryData = new byte[entrySize];
-        Arrays.fill(entryData, (byte) 0xB); // 0xB is MockLedgerEntry.blockPadding
-
-        // 2. read Ledger entries. 96 * 20
-        IntStream.range(0, expectedEntryCount).forEach(i -> {
-            try {
-                byte lengthBuf[] = new byte[4];
-                byte entryIdBuf[] = new byte[8];
-                byte content[] = new byte[entrySize];
-
-                int read = inputStream.read(lengthBuf, 0, 4);
-                assertEquals(read, 4);
-                read = inputStream.read(entryIdBuf, 0, 8);
-                assertEquals(read, 8);
-
-                Random random = new Random(System.currentTimeMillis());
-                int o = 0;
-                int totalRead = 0;
-                int maxReadTime = 10;
-                while (o != content.length) {
-                    int r;
-                    if (maxReadTime-- == 0) {
-                        r = entrySize - o;
-                    } else {
-                        r = random.nextInt(entrySize - o);
-                    }
-                    read = inputStream.read(content, o, r);
-                    totalRead += read;
-                    o += r;
-                }
-                assertEquals(totalRead, entrySize);
-
-                assertEquals(entrySize, Ints.fromByteArray(lengthBuf));
-                assertEquals(i, Longs.fromByteArray(entryIdBuf));
-                assertArrayEquals(entryData, content);
-            } catch (Exception e) {
-                fail("meet exception", e);
-            }
-        });
-
-        // 3. should have no padding
-        int left = blockSize - DataBlockHeaderImpl.getDataStartOffset() -  expectedEntryCount * (entrySize + 4 + 8);
-        assertEquals(left, 0);
-        assertEquals(inputStream.getBlockSize(), inputStream.getDataBlockFullOffset());
-
-        // 4. reach end.
-        byte[] b = new byte[4];
-        ret = inputStream.read(b, 0, 4);
-        assertEquals(ret, -1);
-
-        assertEquals(inputStream.getBlockEntryCount(), expectedEntryCount);
-        assertEquals(inputStream.getBlockEntryBytesCount(), entrySize * expectedEntryCount);
-        assertEquals(inputStream.getEndEntryId(), expectedEntryCount - 1);
-
-        inputStream.close();
-    }
-
-    @Test
-    public void testCloseReleaseResources() throws Exception {
-        ReadHandle readHandle = new MockReadHandle(1, 10, 10);
-
-        @Cleanup
-        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, 1024);
-        inputStream.read();
-        Field field = BlockAwareSegmentInputStreamImpl.class.getDeclaredField("paddingBuf");
-        field.setAccessible(true);
-        ByteBuf paddingBuf = (ByteBuf) field.get(inputStream);
-        assertEquals(1, paddingBuf.refCnt());
-        inputStream.close();
-        assertEquals(0, paddingBuf.refCnt());
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertNotEquals;
+import static org.testng.Assert.fail;
+import static org.testng.internal.junit.ArrayAsserts.assertArrayEquals;
+import com.google.common.io.ByteStreams;
+import com.google.common.primitives.Ints;
+import com.google.common.primitives.Longs;
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.Unpooled;
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+import java.lang.reflect.Field;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Random;
+import java.util.concurrent.CompletableFuture;
+import java.util.function.Supplier;
+import java.util.stream.IntStream;
+import lombok.Cleanup;
+import lombok.Data;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;
+import org.apache.bookkeeper.client.api.LedgerEntries;
+import org.apache.bookkeeper.client.api.LedgerEntry;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.client.api.ReadHandle;
+import org.apache.bookkeeper.mledger.offload.jcloud.DataBlockHeader;
+import org.testng.annotations.DataProvider;
+import org.testng.annotations.Test;
+import org.testng.collections.Lists;
+
+@Slf4j
+public class BlockAwareSegmentInputStreamTest {
+    private static final byte DEFAULT_ENTRY_BYTE = 0xB;
+
+    @Data
+    class MockLedgerEntry implements LedgerEntry {
+        long ledgerId;
+        long entryId;
+        long length;
+        byte entryBytes[];
+        ByteBuf entryBuffer;
+
+        MockLedgerEntry(long ledgerId, long entryId, long length,
+                        Supplier<Byte> dataSupplier) {
+            this.ledgerId = ledgerId;
+            this.entryId = entryId;
+            this.length = length;
+            this.entryBytes = new byte[(int) length];
+            entryBuffer = Unpooled.wrappedBuffer(entryBytes);
+            entryBuffer.writerIndex(0);
+            IntStream.range(0, (int) length).forEach(i -> entryBuffer.writeByte(dataSupplier.get()));
+        }
+
+        @Override
+        public ByteBuffer getEntryNioBuffer() {
+            return null;
+        }
+
+        @Override
+        public LedgerEntry duplicate() {
+            return null;
+        }
+
+        @Override
+        public void close() {
+            entryBuffer.release();
+        }
+    }
+
+    @Data
+    class MockLedgerEntries implements LedgerEntries {
+        int ledgerId;
+        int startEntryId;
+        int count;
+        int entrySize;
+        List<LedgerEntry> entries;
+
+        MockLedgerEntries(int ledgerId, int startEntryId, int count, int entrySize, Supplier<Byte> dataSupplier) {
+            this.ledgerId = ledgerId;
+            this.startEntryId = startEntryId;
+            this.count = count;
+            this.entrySize = entrySize;
+            this.entries = Lists.newArrayList(count);
+
+            IntStream.range(startEntryId, startEntryId + count).forEach(i ->
+                    entries.add(new MockLedgerEntry(ledgerId, i, entrySize, dataSupplier)));
+        }
+
+        @Override
+        public void close() {
+            entries.clear();
+        }
+
+        @Override
+        public LedgerEntry getEntry(long entryId) {
+            if (entryId < startEntryId || entryId >= startEntryId + count) {
+                return null;
+            }
+
+            return entries.get(((int) entryId - startEntryId));
+        }
+
+        @Override
+        public Iterator<LedgerEntry> iterator() {
+            return entries.iterator();
+        }
+    }
+
+    class MockReadHandle implements ReadHandle {
+        int ledgerId;
+        int entrySize;
+        int lac;
+        Supplier<Byte> dataSupplier;
+
+        MockReadHandle(int ledgerId, int entrySize, int lac, Supplier<Byte> dataSupplier) {
+            this.ledgerId = ledgerId;
+            this.entrySize = entrySize;
+            this.lac = lac;
+            this.dataSupplier = dataSupplier;
+        }
+
+        MockReadHandle(int ledgerId, int entrySize, int lac) {
+            this(ledgerId, entrySize, lac, () -> DEFAULT_ENTRY_BYTE);
+        }
+
+        @Override
+        public CompletableFuture<LedgerEntries> readAsync(long firstEntry, long lastEntry) {
+            CompletableFuture<LedgerEntries> future = new CompletableFuture<>();
+            LedgerEntries entries = new MockLedgerEntries(ledgerId,
+                (int) firstEntry,
+                (int) (lastEntry - firstEntry + 1),
+                    entrySize, dataSupplier);
+
+            future.complete(entries);
+            return future;
+        }
+
+        @Override
+        public CompletableFuture<LedgerEntries> readUnconfirmedAsync(long firstEntry, long lastEntry) {
+            return readAsync(firstEntry, lastEntry);
+        }
+
+        @Override
+        public CompletableFuture<Long> readLastAddConfirmedAsync() {
+            return null;
+        }
+
+        @Override
+        public CompletableFuture<Long> tryReadLastAddConfirmedAsync() {
+            return null;
+        }
+
+        @Override
+        public long getLastAddConfirmed() {
+            return lac;
+        }
+
+        @Override
+        public long getLength() {
+            return (lac + 1) * entrySize;
+        }
+
+        @Override
+        public boolean isClosed() {
+            return true;
+        }
+
+        @Override
+        public CompletableFuture<LastConfirmedAndEntry>
+        readLastAddConfirmedAndEntryAsync(long entryId, long timeOutInMillis, boolean parallel) {
+            return null;
+        }
+
+        @Override
+        public LedgerMetadata getLedgerMetadata() {
+            return null;
+        }
+
+        @Override
+        public long getId() {
+            return ledgerId;
+        }
+
+        @Override
+        public CompletableFuture<Void> closeAsync() {
+            return null;
+        }
+    }
+
+    @DataProvider(name = "useBufferRead")
+    public static Object[][] useBufferRead() {
+        return new Object[][]{
+            {Boolean.TRUE},
+            {Boolean.FALSE}
+        };
+    }
+
+    @Test(dataProvider = "useBufferRead")
+    public void testHaveEndPadding(boolean useBufferRead) throws Exception {
+        int ledgerId = 1;
+        int entrySize = 8;
+        int lac = 160;
+        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac);
+
+        // set block size bigger than to (header + entry) size.
+        int blockSize = 3148 + 5;
+        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
+        int expectedEntryCount = (blockSize - DataBlockHeaderImpl.getDataStartOffset()) / (entrySize + 4 + 8);
+
+        // verify get methods
+        assertEquals(inputStream.getLedger(), readHandle);
+        assertEquals(inputStream.getStartEntryId(), 0);
+        assertEquals(inputStream.getBlockSize(), blockSize);
+
+        // verify read inputStream
+        // 1. read header. 128
+        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
+        if (useBufferRead) {
+            int ret = inputStream.read(headerB, 0, DataBlockHeaderImpl.getDataStartOffset());
+            assertEquals(DataBlockHeaderImpl.getDataStartOffset(), ret);
+        } else {
+            ByteStreams.readFully(inputStream, headerB);
+        }
+        DataBlockHeader headerRead = DataBlockHeaderImpl.fromStream(new ByteArrayInputStream(headerB));
+        assertEquals(headerRead.getBlockLength(), blockSize);
+        assertEquals(headerRead.getFirstEntryId(), 0);
+
+        byte[] entryData = new byte[entrySize];
+        Arrays.fill(entryData, (byte) 0xB); // 0xB is MockLedgerEntry.blockPadding
+
+        // 2. read Ledger entries. 201 * 20
+        IntStream.range(0, expectedEntryCount).forEach(i -> {
+            try {
+                byte lengthBuf[] = new byte[4];
+                byte entryIdBuf[] = new byte[8];
+                byte content[] = new byte[entrySize];
+                if (useBufferRead) {
+                    int read = inputStream.read(lengthBuf, 0, 4);
+                    assertEquals(read, 4);
+                    read = inputStream.read(entryIdBuf, 0, 8);
+                    assertEquals(read, 8);
+                    read = inputStream.read(content, 0, entrySize);
+                    assertEquals(read, entrySize);
+                } else {
+                    inputStream.read(lengthBuf);
+                    inputStream.read(entryIdBuf);
+                    inputStream.read(content);
+                }
+
+                assertEquals(entrySize, Ints.fromByteArray(lengthBuf));
+                assertEquals(i, Longs.fromByteArray(entryIdBuf));
+                assertArrayEquals(entryData, content);
+            } catch (Exception e) {
+                fail("meet exception", e);
+            }
+        });
+
+        // 3. read padding
+        int left = blockSize - DataBlockHeaderImpl.getDataStartOffset() -  expectedEntryCount * (entrySize + 4 + 8);
+        assertEquals(left, 5);
+        byte padding[] = new byte[left];
+        if (useBufferRead) {
+            int ret = 0;
+            int offset = 0;
+            while ((ret = inputStream.read(padding, offset, padding.length - offset)) > 0) {
+                offset += ret;
+            }
+            assertEquals(inputStream.read(padding, 0, padding.length), -1);
+        } else {
+            int len = left;
+            int offset = 0;
+            byte[] buf = new byte[4];
+            while (len > 0) {
+                int ret = inputStream.read(buf);
+                for (int i = 0; i < ret; i++) {
+                    padding[offset++] = buf[i];
+                }
+                len -= ret;
+            }
+        }
+        ByteBuf paddingBuf = Unpooled.wrappedBuffer(padding);
+        IntStream.range(0, paddingBuf.capacity() / 4).forEach(i ->
+            assertEquals(Integer.toHexString(paddingBuf.readInt()),
+                         Integer.toHexString(0xFEDCDEAD)));
+
+        // 4. reach end.
+        if (useBufferRead) {
+            byte[] b = new byte[4];
+            int ret = inputStream.read(b, 0, 4);
+            assertEquals(ret, -1);
+        }
+        assertEquals(inputStream.read(), -1);
+
+        assertEquals(inputStream.getBlockEntryCount(), expectedEntryCount);
+        assertEquals(inputStream.getBlockEntryBytesCount(), entrySize * expectedEntryCount);
+        assertEquals(inputStream.getEndEntryId(), expectedEntryCount - 1);
+
+        inputStream.close();
+    }
+
+    @Test(dataProvider = "useBufferRead")
+    public void testNoEndPadding(boolean useBufferRead) throws Exception {
+        int ledgerId = 1;
+        int entrySize = 8;
+        int lac = 120;
+        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac);
+
+        // set block size equals to (header + entry) size.
+        int blockSize = 2148;
+        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
+        int expectedEntryCount = (blockSize - DataBlockHeaderImpl.getDataStartOffset())
+            / (entrySize + BlockAwareSegmentInputStreamImpl.ENTRY_HEADER_SIZE);
+
+        // verify get methods
+        assertEquals(inputStream.getLedger(), readHandle);
+        assertEquals(inputStream.getStartEntryId(), 0);
+        assertEquals(inputStream.getBlockSize(), blockSize);
+
+        // verify read inputStream
+        // 1. read header. 128
+        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
+        if (useBufferRead) {
+            int ret = inputStream.read(headerB, 0, DataBlockHeaderImpl.getDataStartOffset());
+            assertEquals(DataBlockHeaderImpl.getDataStartOffset(), ret);
+        } else {
+            ByteStreams.readFully(inputStream, headerB);
+        }
+        DataBlockHeader headerRead = DataBlockHeaderImpl.fromStream(new ByteArrayInputStream(headerB));
+        assertEquals(headerRead.getBlockLength(), blockSize);
+        assertEquals(headerRead.getFirstEntryId(), 0);
+
+        byte[] entryData = new byte[entrySize];
+        Arrays.fill(entryData, (byte) 0xB); // 0xB is MockLedgerEntry.blockPadding
+
+        // 2. read Ledger entries. 201 * 20
+        IntStream.range(0, expectedEntryCount).forEach(i -> {
+            try {
+                byte lengthBuf[] = new byte[4];
+                byte entryIdBuf[] = new byte[8];
+                byte content[] = new byte[entrySize];
+                if (useBufferRead) {
+                    int read = inputStream.read(lengthBuf, 0, 4);
+                    assertEquals(read, 4);
+                    read = inputStream.read(entryIdBuf, 0, 8);
+                    assertEquals(read, 8);
+                    read = inputStream.read(content, 0, entrySize);
+                    assertEquals(read, entrySize);
+                } else {
+                    inputStream.read(lengthBuf);
+                    inputStream.read(entryIdBuf);
+                    inputStream.read(content);
+                }
+
+                assertEquals(entrySize, Ints.fromByteArray(lengthBuf));
+                assertEquals(i, Longs.fromByteArray(entryIdBuf));
+                assertArrayEquals(entryData, content);
+            } catch (Exception e) {
+                fail("meet exception", e);
+            }
+        });
+
+        // 3. should be no padding
+        int left = blockSize - DataBlockHeaderImpl.getDataStartOffset() -  expectedEntryCount * (entrySize + 4 + 8);
+        assertEquals(left, 0);
+
+        // 4. reach end.
+        if (useBufferRead) {
+            byte[] b = new byte[4];
+            int ret = inputStream.read(b, 0, 4);
+            assertEquals(ret, -1);
+        }
+        assertEquals(inputStream.read(), -1);
+
+        assertEquals(inputStream.getBlockEntryCount(), expectedEntryCount);
+        assertEquals(inputStream.getBlockEntryBytesCount(), entrySize * expectedEntryCount);
+        assertEquals(inputStream.getEndEntryId(), expectedEntryCount - 1);
+
+        inputStream.close();
+    }
+
+    @Test(dataProvider = "useBufferRead")
+    public void testReadTillLac(boolean useBufferRead) throws Exception {
+        // simulate last data block read.
+        int ledgerId = 1;
+        int entrySize = 8;
+        int lac = 89;
+        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac);
+
+        // set block size equals to (header + lac_entry) size.
+        int blockSize = DataBlockHeaderImpl.getDataStartOffset() + (1 + lac) * (entrySize + 4 + 8);
+        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
+        int expectedEntryCount = (blockSize - DataBlockHeaderImpl.getDataStartOffset()) / (entrySize + 4 + 8);
+
+        // verify get methods
+        assertEquals(inputStream.getLedger(), readHandle);
+        assertEquals(inputStream.getStartEntryId(), 0);
+        assertEquals(inputStream.getBlockSize(), blockSize);
+
+        // verify read inputStream
+        // 1. read header. 128
+        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
+        if (useBufferRead) {
+            int ret = inputStream.read(headerB, 0, DataBlockHeaderImpl.getDataStartOffset());
+            assertEquals(DataBlockHeaderImpl.getDataStartOffset(), ret);
+        } else {
+            ByteStreams.readFully(inputStream, headerB);
+        }
+        DataBlockHeader headerRead = DataBlockHeaderImpl.fromStream(new ByteArrayInputStream(headerB));
+        assertEquals(headerRead.getBlockLength(), blockSize);
+        assertEquals(headerRead.getFirstEntryId(), 0);
+
+        byte[] entryData = new byte[entrySize];
+        Arrays.fill(entryData, (byte) 0xB); // 0xB is MockLedgerEntry.blockPadding
+
+        // 2. read Ledger entries. 96 * 20
+        IntStream.range(0, expectedEntryCount).forEach(i -> {
+            try {
+                byte lengthBuf[] = new byte[4];
+                byte entryIdBuf[] = new byte[8];
+                byte content[] = new byte[entrySize];
+                if (useBufferRead) {
+                    int read = inputStream.read(lengthBuf, 0, 4);
+                    assertEquals(read, 4);
+                    read = inputStream.read(entryIdBuf, 0, 8);
+                    assertEquals(read, 8);
+                    read = inputStream.read(content, 0, entrySize);
+                    assertEquals(read, entrySize);
+                } else {
+                    inputStream.read(lengthBuf);
+                    inputStream.read(entryIdBuf);
+                    inputStream.read(content);
+                }
+
+                assertEquals(entrySize, Ints.fromByteArray(lengthBuf));
+                assertEquals(i, Longs.fromByteArray(entryIdBuf));
+                assertArrayEquals(entryData, content);
+            } catch (Exception e) {
+                fail("meet exception", e);
+            }
+        });
+
+        // 3. should have no padding
+        int left = blockSize - DataBlockHeaderImpl.getDataStartOffset() -  expectedEntryCount * (entrySize + 4 + 8);
+        assertEquals(left, 0);
+
+        // 4. reach end.
+        if (useBufferRead) {
+            byte[] b = new byte[4];
+            int ret = inputStream.read(b, 0, 4);
+            assertEquals(ret, -1);
+        }
+        assertEquals(inputStream.read(), -1);
+
+        assertEquals(inputStream.getBlockEntryCount(), expectedEntryCount);
+        assertEquals(inputStream.getBlockEntryBytesCount(), entrySize * expectedEntryCount);
+        assertEquals(inputStream.getEndEntryId(), expectedEntryCount - 1);
+
+        inputStream.close();
+    }
+
+    @Test(dataProvider = "useBufferRead")
+    public void testNoEntryPutIn(boolean useBufferRead) throws Exception {
+        // simulate first entry size over the block size budget, it shouldn't be added.
+        // 2 entries, each with bigger size than block size, so there should no entry added into block.
+        int ledgerId = 1;
+        int entrySize = 1000;
+        int lac = 1;
+        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac);
+
+        // set block size not able to hold one entry
+        int blockSize = DataBlockHeaderImpl.getDataStartOffset() + entrySize;
+        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
+        int expectedEntryCount = 0;
+
+        // verify get methods
+        assertEquals(inputStream.getLedger(), readHandle);
+        assertEquals(inputStream.getStartEntryId(), 0);
+        assertEquals(inputStream.getBlockSize(), blockSize);
+
+        // verify read inputStream
+        // 1. read header. 128
+        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
+        if (useBufferRead) {
+            int ret = inputStream.read(headerB, 0, DataBlockHeaderImpl.getDataStartOffset());
+            assertEquals(DataBlockHeaderImpl.getDataStartOffset(), ret);
+        } else {
+            ByteStreams.readFully(inputStream, headerB);
+        }
+        DataBlockHeader headerRead = DataBlockHeaderImpl.fromStream(new ByteArrayInputStream(headerB));
+        assertEquals(headerRead.getBlockLength(), blockSize);
+        assertEquals(headerRead.getFirstEntryId(), 0);
+
+
+        // 2. since no entry put in, it should only get padding after header.
+        byte padding[] = new byte[blockSize - DataBlockHeaderImpl.getDataStartOffset()];
+        if (useBufferRead) {
+            int ret = 0;
+            int offset = 0;
+            while ((ret = inputStream.read(padding, offset, padding.length - offset)) > 0) {
+                offset += ret;
+            }
+            assertEquals(inputStream.read(padding, 0, padding.length), -1);
+        } else {
+            int len = padding.length;
+            int offset = 0;
+            byte[] buf = new byte[4];
+            while (len > 0) {
+                int ret = inputStream.read(buf);
+                for (int i = 0; i < ret; i++) {
+                    padding[offset++] = buf[i];
+                }
+                len -= ret;
+            }
+        }
+        ByteBuf paddingBuf = Unpooled.wrappedBuffer(padding);
+        IntStream.range(0, paddingBuf.capacity() / 4).forEach(i ->
+            assertEquals(Integer.toHexString(paddingBuf.readInt()),
+                         Integer.toHexString(0xFEDCDEAD)));
+
+        // 3. reach end.
+        if (useBufferRead) {
+            byte[] b = new byte[4];
+            int ret = inputStream.read(b, 0, 4);
+            assertEquals(ret, -1);
+        }
+        assertEquals(inputStream.read(), -1);
+
+        assertEquals(inputStream.getBlockEntryCount(), 0);
+        assertEquals(inputStream.getBlockEntryBytesCount(), 0);
+        assertEquals(inputStream.getEndEntryId(), -1);
+
+        inputStream.close();
+    }
+
+    @Test(dataProvider = "useBufferRead")
+    public void testPaddingOnLastBlock(boolean useBufferRead) throws Exception {
+        int ledgerId = 1;
+        int entrySize = 1000;
+        int lac = 0;
+        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac);
+
+        // set block size not able to hold one entry
+        int blockSize = DataBlockHeaderImpl.getDataStartOffset() + entrySize * 2;
+        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
+        int expectedEntryCount = 1;
+
+        // verify get methods
+        assertEquals(inputStream.getLedger(), readHandle);
+        assertEquals(inputStream.getStartEntryId(), 0);
+        assertEquals(inputStream.getBlockSize(), blockSize);
+
+        // verify read inputStream
+        // 1. read header. 128
+        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
+        if (useBufferRead) {
+            int ret = inputStream.read(headerB, 0, DataBlockHeaderImpl.getDataStartOffset());
+            assertEquals(DataBlockHeaderImpl.getDataStartOffset(), ret);
+        } else {
+            ByteStreams.readFully(inputStream, headerB);
+        }
+        DataBlockHeader headerRead = DataBlockHeaderImpl.fromStream(new ByteArrayInputStream(headerB));
+        assertEquals(headerRead.getBlockLength(), blockSize);
+        assertEquals(headerRead.getFirstEntryId(), 0);
+
+        // 2. There should be a single entry
+        byte[] entryData = new byte[entrySize];
+        Arrays.fill(entryData, (byte) 0xB); // 0xB is MockLedgerEntry.blockPadding
+
+        IntStream.range(0, expectedEntryCount).forEach(i -> {
+            try {
+                byte lengthBuf[] = new byte[4];
+                byte entryIdBuf[] = new byte[8];
+                byte content[] = new byte[entrySize];
+                if (useBufferRead) {
+                    int read = inputStream.read(lengthBuf, 0, 4);
+                    assertEquals(read, 4);
+                    read = inputStream.read(entryIdBuf, 0, 8);
+                    assertEquals(read, 8);
+                    read = inputStream.read(content, 0, entrySize);
+                    assertEquals(read, entrySize);
+                } else {
+                    inputStream.read(lengthBuf);
+                    inputStream.read(entryIdBuf);
+                    inputStream.read(content);
+                }
+
+                assertEquals(entrySize, Ints.fromByteArray(lengthBuf));
+                assertEquals(i, Longs.fromByteArray(entryIdBuf));
+                assertArrayEquals(entryData, content);
+            } catch (Exception e) {
+                fail("meet exception", e);
+            }
+        });
+
+        // 3. Then padding
+        int consumedBytes = DataBlockHeaderImpl.getDataStartOffset()
+            + expectedEntryCount * (entrySize + BlockAwareSegmentInputStreamImpl.ENTRY_HEADER_SIZE);
+        byte padding[] = new byte[blockSize - consumedBytes];
+        if (useBufferRead) {
+            int ret = 0;
+            int offset = 0;
+            while ((ret = inputStream.read(padding, offset, padding.length - offset)) > 0) {
+                offset += ret;
+            }
+            assertEquals(inputStream.read(padding, 0, padding.length), -1);
+        } else {
+            int len = blockSize - consumedBytes;
+            int offset = 0;
+            byte[] buf = new byte[4];
+            while (len > 0) {
+                int ret = inputStream.read(buf);
+                for (int i = 0; i < ret; i++) {
+                    padding[offset++] = buf[i];
+                }
+                len -= ret;
+            }
+        }
+        ByteBuf paddingBuf = Unpooled.wrappedBuffer(padding);
+        IntStream.range(0, paddingBuf.capacity() / 4).forEach(i ->
+                assertEquals(Integer.toHexString(paddingBuf.readInt()),
+                             Integer.toHexString(0xFEDCDEAD)));
+
+        // 3. reach end.
+        if (useBufferRead) {
+            byte[] b = new byte[4];
+            int ret = inputStream.read(b, 0, 4);
+            assertEquals(ret, -1);
+        }
+        assertEquals(inputStream.read(), -1);
+
+        assertEquals(inputStream.getBlockEntryCount(), 1);
+        assertEquals(inputStream.getBlockEntryBytesCount(), entrySize);
+        assertEquals(inputStream.getEndEntryId(), 0);
+
+        inputStream.close();
+    }
+
+    @Test
+    public void testOnlyNegativeOnEOF() throws Exception {
+        int ledgerId = 1;
+        int entrySize = 10000;
+        int lac = 0;
+
+        Random r = new Random(0);
+        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac, () -> (byte) r.nextInt());
+
+        int blockSize = DataBlockHeaderImpl.getDataStartOffset() + entrySize * 2;
+        @Cleanup
+        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
+
+        int bytesRead = 0;
+        for (int i = 0; i < blockSize * 2; i++) {
+            int ret = inputStream.read();
+            if (ret < 0) { // should only be EOF
+                assertEquals(bytesRead, blockSize);
+                break;
+            } else {
+                bytesRead++;
+            }
+        }
+    }
+
+    @Test
+    public void testOnlyNegativeOnEOFWithBufferedRead() throws IOException {
+        int ledgerId = 1;
+        int entrySize = 10000;
+        int lac = 0;
+
+        Random r = new Random(0);
+        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac, () -> (byte) r.nextInt());
+
+        int blockSize = DataBlockHeaderImpl.getDataStartOffset() + entrySize * 2;
+        @Cleanup
+        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
+
+        int bytesRead = 0;
+        int ret;
+        int offset = 0;
+        int resetOffsetCount = 0;
+        byte[] buf = new byte[1024];
+        while ((ret = inputStream.read(buf, offset, buf.length - offset)) > 0) {
+            bytesRead += ret;
+            int currentOffset = offset;
+            offset = (offset + ret) % buf.length;
+            if (offset < currentOffset) {
+                resetOffsetCount++;
+            }
+        }
+        assertEquals(bytesRead, blockSize);
+        assertNotEquals(resetOffsetCount, 0);
+    }
+
+    // This test is for testing the read(byte[] buf, int off, int len) method can work properly
+    // on the offset not 0.
+    @Test
+    public void testReadTillLacWithSmallBuffer() throws Exception {
+        // simulate last data block read.
+        int ledgerId = 1;
+        int entrySize = 8;
+        int lac = 89;
+        ReadHandle readHandle = new MockReadHandle(ledgerId, entrySize, lac);
+
+        // set block size equals to (header + lac_entry) size.
+        int blockSize = DataBlockHeaderImpl.getDataStartOffset() + (1 + lac) * (entrySize + 4 + 8);
+        @Cleanup
+        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, blockSize);
+        int expectedEntryCount = (blockSize - DataBlockHeaderImpl.getDataStartOffset()) / (entrySize + 4 + 8);
+
+        // verify get methods
+        assertEquals(inputStream.getLedger(), readHandle);
+        assertEquals(inputStream.getStartEntryId(), 0);
+        assertEquals(inputStream.getBlockSize(), blockSize);
+
+        // verify read inputStream
+        // 1. read header. 128
+        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
+        // read twice to test the offset not 0 case
+        int ret = inputStream.read(headerB, 0, 66);
+        assertEquals(ret, 66);
+        ret = inputStream.read(headerB, 66, headerB.length - 66);
+        assertEquals(headerB.length - 66, ret);
+        DataBlockHeader headerRead = DataBlockHeaderImpl.fromStream(new ByteArrayInputStream(headerB));
+        assertEquals(headerRead.getBlockLength(), blockSize);
+        assertEquals(headerRead.getFirstEntryId(), 0);
+
+        byte[] entryData = new byte[entrySize];
+        Arrays.fill(entryData, (byte) 0xB); // 0xB is MockLedgerEntry.blockPadding
+
+        // 2. read Ledger entries. 96 * 20
+        IntStream.range(0, expectedEntryCount).forEach(i -> {
+            try {
+                byte lengthBuf[] = new byte[4];
+                byte entryIdBuf[] = new byte[8];
+                byte content[] = new byte[entrySize];
+
+                int read = inputStream.read(lengthBuf, 0, 4);
+                assertEquals(read, 4);
+                read = inputStream.read(entryIdBuf, 0, 8);
+                assertEquals(read, 8);
+
+                Random random = new Random(System.currentTimeMillis());
+                int o = 0;
+                int totalRead = 0;
+                int maxReadTime = 10;
+                while (o != content.length) {
+                    int r;
+                    if (maxReadTime-- == 0) {
+                        r = entrySize - o;
+                    } else {
+                        r = random.nextInt(entrySize - o);
+                    }
+                    read = inputStream.read(content, o, r);
+                    totalRead += read;
+                    o += r;
+                }
+                assertEquals(totalRead, entrySize);
+
+                assertEquals(entrySize, Ints.fromByteArray(lengthBuf));
+                assertEquals(i, Longs.fromByteArray(entryIdBuf));
+                assertArrayEquals(entryData, content);
+            } catch (Exception e) {
+                fail("meet exception", e);
+            }
+        });
+
+        // 3. should have no padding
+        int left = blockSize - DataBlockHeaderImpl.getDataStartOffset() -  expectedEntryCount * (entrySize + 4 + 8);
+        assertEquals(left, 0);
+        assertEquals(inputStream.getBlockSize(), inputStream.getDataBlockFullOffset());
+
+        // 4. reach end.
+        byte[] b = new byte[4];
+        ret = inputStream.read(b, 0, 4);
+        assertEquals(ret, -1);
+
+        assertEquals(inputStream.getBlockEntryCount(), expectedEntryCount);
+        assertEquals(inputStream.getBlockEntryBytesCount(), entrySize * expectedEntryCount);
+        assertEquals(inputStream.getEndEntryId(), expectedEntryCount - 1);
+
+        inputStream.close();
+    }
+
+    @Test
+    public void testCloseReleaseResources() throws Exception {
+        ReadHandle readHandle = new MockReadHandle(1, 10, 10);
+
+        @Cleanup
+        BlockAwareSegmentInputStreamImpl inputStream = new BlockAwareSegmentInputStreamImpl(readHandle, 0, 1024);
+        inputStream.read();
+        Field field = BlockAwareSegmentInputStreamImpl.class.getDeclaredField("paddingBuf");
+        field.setAccessible(true);
+        ByteBuf paddingBuf = (ByteBuf) field.get(inputStream);
+        assertEquals(1, paddingBuf.refCnt());
+        inputStream.close();
+        assertEquals(0, paddingBuf.refCnt());
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BufferedOffloadStreamTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BufferedOffloadStreamTest.java
index 87c7ccf5df..c617a6f9dd 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BufferedOffloadStreamTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BufferedOffloadStreamTest.java
@@ -1,193 +1,193 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static org.testng.Assert.assertEquals;
-import com.google.common.io.ByteStreams;
-import com.google.common.primitives.Ints;
-import com.google.common.primitives.Longs;
-import io.netty.buffer.ByteBuf;
-import io.netty.buffer.Unpooled;
-import java.io.ByteArrayInputStream;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Random;
-import java.util.UUID;
-import java.util.concurrent.atomic.AtomicLong;
-import org.apache.bookkeeper.mledger.Entry;
-import org.apache.bookkeeper.mledger.impl.EntryImpl;
-import org.apache.bookkeeper.mledger.impl.OffloadSegmentInfoImpl;
-import org.testng.Assert;
-import org.testng.annotations.Test;
-
-public class BufferedOffloadStreamTest {
-    final Random random = new Random();
-
-    private void testWithPadding(int paddingLen) throws Exception {
-        int blockSize = StreamingDataBlockHeaderImpl.getDataStartOffset();
-        List<Entry> entryBuffer = new LinkedList<>();
-        final UUID uuid = UUID.randomUUID();
-        OffloadSegmentInfoImpl segmentInfo = new OffloadSegmentInfoImpl(uuid, 0, 0, "",
-                new HashMap<>());
-        final int entryCount = 10;
-        List<Entry> entries = new ArrayList<>();
-        for (int i = 0; i < entryCount; i++) {
-            final byte[] bytes = new byte[random.nextInt(10)];
-            final EntryImpl entry = EntryImpl.create(0, i, bytes);
-            entries.add(entry);
-            entry.retain();
-            entryBuffer.add(entry);
-            blockSize += BufferedOffloadStream.ENTRY_HEADER_SIZE + entry.getLength();
-        }
-        segmentInfo.closeSegment(0, 9);
-        blockSize += paddingLen;
-
-        final BufferedOffloadStream inputStream = new BufferedOffloadStream(blockSize, entryBuffer,
-                segmentInfo.beginLedgerId,
-                segmentInfo.beginEntryId);
-        Assert.assertEquals(inputStream.getLedgerId(), 0);
-        Assert.assertEquals(inputStream.getBeginEntryId(), 0);
-        Assert.assertEquals(inputStream.getBlockSize(), blockSize);
-
-        byte[] headerB = new byte[DataBlockHeaderImpl.getDataStartOffset()];
-        ByteStreams.readFully(inputStream, headerB);
-        StreamingDataBlockHeaderImpl headerRead = StreamingDataBlockHeaderImpl
-                .fromStream(new ByteArrayInputStream(headerB));
-        assertEquals(headerRead.getBlockLength(), blockSize);
-        assertEquals(headerRead.getFirstEntryId(), 0);
-
-        int left = blockSize - DataBlockHeaderImpl.getDataStartOffset();
-        for (int i = 0; i < entryCount; i++) {
-            byte[] lengthBuf = new byte[4];
-            byte[] entryIdBuf = new byte[8];
-            byte[] content = new byte[entries.get(i).getLength()];
-
-            left -= lengthBuf.length + entryIdBuf.length + content.length;
-            inputStream.read(lengthBuf);
-            inputStream.read(entryIdBuf);
-            inputStream.read(content);
-            assertEquals(entries.get(i).getLength(), Ints.fromByteArray(lengthBuf));
-            assertEquals(i, Longs.fromByteArray(entryIdBuf));
-            assertEquals(entries.get(i).getData(), content);
-        }
-        Assert.assertEquals(left, paddingLen);
-        byte[] padding = new byte[left];
-        inputStream.read(padding);
-
-        ByteBuf paddingBuf = Unpooled.wrappedBuffer(padding);
-        for (int i = 0; i < paddingBuf.capacity() / 4; i++) {
-            assertEquals(Integer.toHexString(paddingBuf.readInt()),
-                    Integer.toHexString(0xFEDCDEAD));
-        }
-
-        // 4. reach end.
-        assertEquals(inputStream.read(), -1);
-        assertEquals(inputStream.read(), -1);
-        inputStream.close();
-    }
-
-    @Test
-    public void testHavePadding() throws Exception {
-        testWithPadding(10);
-    }
-
-    @Test
-    public void testNoPadding() throws Exception {
-        testWithPadding(0);
-    }
-
-    @Test(enabled = false, description = "Disable because let offloader to ensure there is no another ledger id")
-    public void shouldEndWhenSegmentChanged() throws IOException {
-        int blockSize = StreamingDataBlockHeaderImpl.getDataStartOffset();
-        int paddingLen = 10;
-        List<Entry> entryBuffer = new LinkedList<>();
-        final UUID uuid = UUID.randomUUID();
-        OffloadSegmentInfoImpl segmentInfo = new OffloadSegmentInfoImpl(uuid, 0, 0, "",
-                new HashMap<>());
-        AtomicLong bufferLength = new AtomicLong();
-        final int entryCount = 10;
-        List<Entry> entries = new ArrayList<>();
-        for (int i = 0; i < entryCount; i++) {
-            final byte[] bytes = new byte[random.nextInt(10)];
-            final EntryImpl entry = EntryImpl.create(0, i, bytes);
-            entries.add(entry);
-            entry.retain();
-            entryBuffer.add(entry);
-            blockSize += BufferedOffloadStream.ENTRY_HEADER_SIZE + entry.getLength();
-        }
-        //create new ledger
-        {
-            final byte[] bytes = new byte[random.nextInt(10)];
-            final EntryImpl entry = EntryImpl.create(1, 0, bytes);
-            entries.add(entry);
-            entry.retain();
-            entryBuffer.add(entry);
-        }
-        segmentInfo.closeSegment(1, 0);
-        blockSize += paddingLen;
-
-        final BufferedOffloadStream inputStream = new BufferedOffloadStream(blockSize, entryBuffer,
-                segmentInfo.beginLedgerId,
-                segmentInfo.beginEntryId);
-        Assert.assertEquals(inputStream.getLedgerId(), 0);
-        Assert.assertEquals(inputStream.getBeginEntryId(), 0);
-        Assert.assertEquals(inputStream.getBlockSize(), blockSize);
-
-        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
-        ByteStreams.readFully(inputStream, headerB);
-        StreamingDataBlockHeaderImpl headerRead = StreamingDataBlockHeaderImpl
-                .fromStream(new ByteArrayInputStream(headerB));
-        assertEquals(headerRead.getBlockLength(), blockSize);
-        assertEquals(headerRead.getFirstEntryId(), 0);
-
-        int left = blockSize - DataBlockHeaderImpl.getDataStartOffset();
-        for (int i = 0; i < entryCount; i++) {
-            byte[] lengthBuf = new byte[4];
-            byte[] entryIdBuf = new byte[8];
-            byte[] content = new byte[entries.get(i).getLength()];
-
-            left -= lengthBuf.length + entryIdBuf.length + content.length;
-            inputStream.read(lengthBuf);
-            inputStream.read(entryIdBuf);
-            inputStream.read(content);
-            assertEquals(entries.get(i).getLength(), Ints.fromByteArray(lengthBuf));
-            assertEquals(i, Longs.fromByteArray(entryIdBuf));
-            assertEquals(entries.get(i).getData(), content);
-        }
-        Assert.assertEquals(left, paddingLen);
-        byte[] padding = new byte[left];
-        inputStream.read(padding);
-
-        ByteBuf paddingBuf = Unpooled.wrappedBuffer(padding);
-        for (int i = 0; i < paddingBuf.capacity() / 4; i++) {
-            assertEquals(Integer.toHexString(paddingBuf.readInt()),
-                    Integer.toHexString(0xFEDCDEAD));
-        }
-
-        // 4. reach end.
-        assertEquals(inputStream.read(), -1);
-        assertEquals(inputStream.read(), -1);
-        inputStream.close();
-
-        Assert.assertEquals(entryBuffer.size(), 1);
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static org.testng.Assert.assertEquals;
+import com.google.common.io.ByteStreams;
+import com.google.common.primitives.Ints;
+import com.google.common.primitives.Longs;
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.Unpooled;
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Random;
+import java.util.UUID;
+import java.util.concurrent.atomic.AtomicLong;
+import org.apache.bookkeeper.mledger.Entry;
+import org.apache.bookkeeper.mledger.impl.EntryImpl;
+import org.apache.bookkeeper.mledger.impl.OffloadSegmentInfoImpl;
+import org.testng.Assert;
+import org.testng.annotations.Test;
+
+public class BufferedOffloadStreamTest {
+    final Random random = new Random();
+
+    private void testWithPadding(int paddingLen) throws Exception {
+        int blockSize = StreamingDataBlockHeaderImpl.getDataStartOffset();
+        List<Entry> entryBuffer = new LinkedList<>();
+        final UUID uuid = UUID.randomUUID();
+        OffloadSegmentInfoImpl segmentInfo = new OffloadSegmentInfoImpl(uuid, 0, 0, "",
+                new HashMap<>());
+        final int entryCount = 10;
+        List<Entry> entries = new ArrayList<>();
+        for (int i = 0; i < entryCount; i++) {
+            final byte[] bytes = new byte[random.nextInt(10)];
+            final EntryImpl entry = EntryImpl.create(0, i, bytes);
+            entries.add(entry);
+            entry.retain();
+            entryBuffer.add(entry);
+            blockSize += BufferedOffloadStream.ENTRY_HEADER_SIZE + entry.getLength();
+        }
+        segmentInfo.closeSegment(0, 9);
+        blockSize += paddingLen;
+
+        final BufferedOffloadStream inputStream = new BufferedOffloadStream(blockSize, entryBuffer,
+                segmentInfo.beginLedgerId,
+                segmentInfo.beginEntryId);
+        Assert.assertEquals(inputStream.getLedgerId(), 0);
+        Assert.assertEquals(inputStream.getBeginEntryId(), 0);
+        Assert.assertEquals(inputStream.getBlockSize(), blockSize);
+
+        byte[] headerB = new byte[DataBlockHeaderImpl.getDataStartOffset()];
+        ByteStreams.readFully(inputStream, headerB);
+        StreamingDataBlockHeaderImpl headerRead = StreamingDataBlockHeaderImpl
+                .fromStream(new ByteArrayInputStream(headerB));
+        assertEquals(headerRead.getBlockLength(), blockSize);
+        assertEquals(headerRead.getFirstEntryId(), 0);
+
+        int left = blockSize - DataBlockHeaderImpl.getDataStartOffset();
+        for (int i = 0; i < entryCount; i++) {
+            byte[] lengthBuf = new byte[4];
+            byte[] entryIdBuf = new byte[8];
+            byte[] content = new byte[entries.get(i).getLength()];
+
+            left -= lengthBuf.length + entryIdBuf.length + content.length;
+            inputStream.read(lengthBuf);
+            inputStream.read(entryIdBuf);
+            inputStream.read(content);
+            assertEquals(entries.get(i).getLength(), Ints.fromByteArray(lengthBuf));
+            assertEquals(i, Longs.fromByteArray(entryIdBuf));
+            assertEquals(entries.get(i).getData(), content);
+        }
+        Assert.assertEquals(left, paddingLen);
+        byte[] padding = new byte[left];
+        inputStream.read(padding);
+
+        ByteBuf paddingBuf = Unpooled.wrappedBuffer(padding);
+        for (int i = 0; i < paddingBuf.capacity() / 4; i++) {
+            assertEquals(Integer.toHexString(paddingBuf.readInt()),
+                    Integer.toHexString(0xFEDCDEAD));
+        }
+
+        // 4. reach end.
+        assertEquals(inputStream.read(), -1);
+        assertEquals(inputStream.read(), -1);
+        inputStream.close();
+    }
+
+    @Test
+    public void testHavePadding() throws Exception {
+        testWithPadding(10);
+    }
+
+    @Test
+    public void testNoPadding() throws Exception {
+        testWithPadding(0);
+    }
+
+    @Test(enabled = false, description = "Disable because let offloader to ensure there is no another ledger id")
+    public void shouldEndWhenSegmentChanged() throws IOException {
+        int blockSize = StreamingDataBlockHeaderImpl.getDataStartOffset();
+        int paddingLen = 10;
+        List<Entry> entryBuffer = new LinkedList<>();
+        final UUID uuid = UUID.randomUUID();
+        OffloadSegmentInfoImpl segmentInfo = new OffloadSegmentInfoImpl(uuid, 0, 0, "",
+                new HashMap<>());
+        AtomicLong bufferLength = new AtomicLong();
+        final int entryCount = 10;
+        List<Entry> entries = new ArrayList<>();
+        for (int i = 0; i < entryCount; i++) {
+            final byte[] bytes = new byte[random.nextInt(10)];
+            final EntryImpl entry = EntryImpl.create(0, i, bytes);
+            entries.add(entry);
+            entry.retain();
+            entryBuffer.add(entry);
+            blockSize += BufferedOffloadStream.ENTRY_HEADER_SIZE + entry.getLength();
+        }
+        //create new ledger
+        {
+            final byte[] bytes = new byte[random.nextInt(10)];
+            final EntryImpl entry = EntryImpl.create(1, 0, bytes);
+            entries.add(entry);
+            entry.retain();
+            entryBuffer.add(entry);
+        }
+        segmentInfo.closeSegment(1, 0);
+        blockSize += paddingLen;
+
+        final BufferedOffloadStream inputStream = new BufferedOffloadStream(blockSize, entryBuffer,
+                segmentInfo.beginLedgerId,
+                segmentInfo.beginEntryId);
+        Assert.assertEquals(inputStream.getLedgerId(), 0);
+        Assert.assertEquals(inputStream.getBeginEntryId(), 0);
+        Assert.assertEquals(inputStream.getBlockSize(), blockSize);
+
+        byte headerB[] = new byte[DataBlockHeaderImpl.getDataStartOffset()];
+        ByteStreams.readFully(inputStream, headerB);
+        StreamingDataBlockHeaderImpl headerRead = StreamingDataBlockHeaderImpl
+                .fromStream(new ByteArrayInputStream(headerB));
+        assertEquals(headerRead.getBlockLength(), blockSize);
+        assertEquals(headerRead.getFirstEntryId(), 0);
+
+        int left = blockSize - DataBlockHeaderImpl.getDataStartOffset();
+        for (int i = 0; i < entryCount; i++) {
+            byte[] lengthBuf = new byte[4];
+            byte[] entryIdBuf = new byte[8];
+            byte[] content = new byte[entries.get(i).getLength()];
+
+            left -= lengthBuf.length + entryIdBuf.length + content.length;
+            inputStream.read(lengthBuf);
+            inputStream.read(entryIdBuf);
+            inputStream.read(content);
+            assertEquals(entries.get(i).getLength(), Ints.fromByteArray(lengthBuf));
+            assertEquals(i, Longs.fromByteArray(entryIdBuf));
+            assertEquals(entries.get(i).getData(), content);
+        }
+        Assert.assertEquals(left, paddingLen);
+        byte[] padding = new byte[left];
+        inputStream.read(padding);
+
+        ByteBuf paddingBuf = Unpooled.wrappedBuffer(padding);
+        for (int i = 0; i < paddingBuf.capacity() / 4; i++) {
+            assertEquals(Integer.toHexString(paddingBuf.readInt()),
+                    Integer.toHexString(0xFEDCDEAD));
+        }
+
+        // 4. reach end.
+        assertEquals(inputStream.read(), -1);
+        assertEquals(inputStream.read(), -1);
+        inputStream.close();
+
+        Assert.assertEquals(entryBuffer.size(), 1);
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockHeaderTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockHeaderTest.java
index 0a9b7de09d..1c07093933 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockHeaderTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockHeaderTest.java
@@ -1,83 +1,83 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static org.testng.Assert.assertEquals;
-import static org.testng.Assert.assertTrue;
-import static org.testng.Assert.fail;
-import java.io.ByteArrayInputStream;
-import java.io.EOFException;
-import java.io.IOException;
-import java.io.InputStream;
-import lombok.extern.slf4j.Slf4j;
-import org.apache.bookkeeper.mledger.offload.jcloud.DataBlockHeader;
-import org.testng.annotations.Test;
-
-@Slf4j
-public class DataBlockHeaderTest {
-
-    @Test
-    public void dataBlockHeaderImplTest() throws Exception {
-        int blockLength = 1024 * 1024;
-        long firstEntryId = 3333L;
-
-        DataBlockHeaderImpl dataBlockHeader = DataBlockHeaderImpl.of(blockLength,
-            firstEntryId);
-
-        // verify get methods
-        assertEquals(dataBlockHeader.getBlockMagicWord(), DataBlockHeaderImpl.MAGIC_WORD);
-        assertEquals(dataBlockHeader.getBlockLength(), blockLength);
-        assertEquals(dataBlockHeader.getFirstEntryId(), firstEntryId);
-
-        // verify toStream and fromStream
-        InputStream stream = dataBlockHeader.toStream();
-        stream.mark(0);
-        DataBlockHeader rebuild = DataBlockHeaderImpl.fromStream(stream);
-        assertEquals(rebuild.getBlockLength(), blockLength);
-        assertEquals(rebuild.getFirstEntryId(), firstEntryId);
-        // verify InputStream reach end
-        assertEquals(stream.read(), -1);
-
-        stream.reset();
-        byte[] streamContent = new byte[DataBlockHeaderImpl.getDataStartOffset()];
-
-        // stream with all 0, simulate junk data, should throw exception for header magic not match.
-        try (InputStream stream2 =
-                    new ByteArrayInputStream(streamContent, 0, DataBlockHeaderImpl.getDataStartOffset())) {
-            DataBlockHeader rebuild2 = DataBlockHeaderImpl.fromStream(stream2);
-            fail("Should throw IOException");
-        } catch (Exception e) {
-            assertTrue(e instanceof IOException);
-            assertTrue(e.getMessage().contains("Data block header magic word not match"));
-        }
-
-        // simulate read header too small, throw EOFException.
-        stream.read(streamContent);
-        try (InputStream stream3 =
-                new ByteArrayInputStream(streamContent, 0, DataBlockHeaderImpl.getDataStartOffset() - 1)) {
-            DataBlockHeader rebuild3 = DataBlockHeaderImpl.fromStream(stream3);
-            fail("Should throw EOFException");
-        } catch (EOFException e) {
-            // expected
-        }
-
-        stream.close();
-    }
-
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertTrue;
+import static org.testng.Assert.fail;
+import java.io.ByteArrayInputStream;
+import java.io.EOFException;
+import java.io.IOException;
+import java.io.InputStream;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.bookkeeper.mledger.offload.jcloud.DataBlockHeader;
+import org.testng.annotations.Test;
+
+@Slf4j
+public class DataBlockHeaderTest {
+
+    @Test
+    public void dataBlockHeaderImplTest() throws Exception {
+        int blockLength = 1024 * 1024;
+        long firstEntryId = 3333L;
+
+        DataBlockHeaderImpl dataBlockHeader = DataBlockHeaderImpl.of(blockLength,
+            firstEntryId);
+
+        // verify get methods
+        assertEquals(dataBlockHeader.getBlockMagicWord(), DataBlockHeaderImpl.MAGIC_WORD);
+        assertEquals(dataBlockHeader.getBlockLength(), blockLength);
+        assertEquals(dataBlockHeader.getFirstEntryId(), firstEntryId);
+
+        // verify toStream and fromStream
+        InputStream stream = dataBlockHeader.toStream();
+        stream.mark(0);
+        DataBlockHeader rebuild = DataBlockHeaderImpl.fromStream(stream);
+        assertEquals(rebuild.getBlockLength(), blockLength);
+        assertEquals(rebuild.getFirstEntryId(), firstEntryId);
+        // verify InputStream reach end
+        assertEquals(stream.read(), -1);
+
+        stream.reset();
+        byte[] streamContent = new byte[DataBlockHeaderImpl.getDataStartOffset()];
+
+        // stream with all 0, simulate junk data, should throw exception for header magic not match.
+        try (InputStream stream2 =
+                    new ByteArrayInputStream(streamContent, 0, DataBlockHeaderImpl.getDataStartOffset())) {
+            DataBlockHeader rebuild2 = DataBlockHeaderImpl.fromStream(stream2);
+            fail("Should throw IOException");
+        } catch (Exception e) {
+            assertTrue(e instanceof IOException);
+            assertTrue(e.getMessage().contains("Data block header magic word not match"));
+        }
+
+        // simulate read header too small, throw EOFException.
+        stream.read(streamContent);
+        try (InputStream stream3 =
+                new ByteArrayInputStream(streamContent, 0, DataBlockHeaderImpl.getDataStartOffset() - 1)) {
+            DataBlockHeader rebuild3 = DataBlockHeaderImpl.fromStream(stream3);
+            fail("Should throw EOFException");
+        } catch (EOFException e) {
+            // expected
+        }
+
+        stream.close();
+    }
+
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockHeaderV2Test.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockHeaderV2Test.java
index 468f3cdfd0..10109767c4 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockHeaderV2Test.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockHeaderV2Test.java
@@ -1,87 +1,87 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static org.testng.Assert.assertEquals;
-import static org.testng.Assert.assertTrue;
-import static org.testng.Assert.fail;
-import java.io.ByteArrayInputStream;
-import java.io.EOFException;
-import java.io.IOException;
-import java.io.InputStream;
-import lombok.extern.slf4j.Slf4j;
-import org.apache.bookkeeper.mledger.offload.jcloud.DataBlockHeader;
-import org.testng.annotations.Test;
-
-@Slf4j
-public class DataBlockHeaderV2Test {
-
-    @Test
-    public void dataBlockHeaderImplTest() throws Exception {
-        int blockLength = 1024 * 1024;
-        long firstEntryId = 3333L;
-        long ledgerId = 3L;
-
-        StreamingDataBlockHeaderImpl dataBlockHeader = StreamingDataBlockHeaderImpl.of(blockLength,
-                ledgerId, firstEntryId);
-
-        // verify get methods
-        assertEquals(StreamingDataBlockHeaderImpl.getBlockMagicWord(), StreamingDataBlockHeaderImpl.MAGIC_WORD);
-        assertEquals(dataBlockHeader.getBlockLength(), blockLength);
-        assertEquals(dataBlockHeader.getFirstEntryId(), firstEntryId);
-        assertEquals(dataBlockHeader.getLedgerId(), ledgerId);
-
-        // verify toStream and fromStream
-        InputStream stream = dataBlockHeader.toStream();
-        stream.mark(0);
-        StreamingDataBlockHeaderImpl rebuild = StreamingDataBlockHeaderImpl.fromStream(stream);
-        assertEquals(rebuild.getBlockLength(), blockLength);
-        assertEquals(rebuild.getFirstEntryId(), firstEntryId);
-        assertEquals(rebuild.getLedgerId(), ledgerId);
-        // verify InputStream reach end
-        assertEquals(stream.read(), -1);
-
-        stream.reset();
-        byte[] streamContent = new byte[StreamingDataBlockHeaderImpl.getDataStartOffset()];
-
-        // stream with all 0, simulate junk data, should throw exception for header magic not match.
-        try (InputStream stream2 = new ByteArrayInputStream(streamContent, 0,
-                StreamingDataBlockHeaderImpl.getDataStartOffset())) {
-            DataBlockHeader rebuild2 = StreamingDataBlockHeaderImpl.fromStream(stream2);
-            fail("Should throw IOException");
-        } catch (Exception e) {
-            assertTrue(e instanceof IOException);
-            assertTrue(e.getMessage().contains("Data block header magic word not match"));
-        }
-
-        // simulate read header too small, throw EOFException.
-        stream.read(streamContent);
-        try (InputStream stream3 =
-                     new ByteArrayInputStream(streamContent, 0,
-                             StreamingDataBlockHeaderImpl.getDataStartOffset() - 1)) {
-            DataBlockHeader rebuild3 = StreamingDataBlockHeaderImpl.fromStream(stream3);
-            fail("Should throw EOFException");
-        } catch (EOFException e) {
-            // expected
-        }
-
-        stream.close();
-    }
-
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertTrue;
+import static org.testng.Assert.fail;
+import java.io.ByteArrayInputStream;
+import java.io.EOFException;
+import java.io.IOException;
+import java.io.InputStream;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.bookkeeper.mledger.offload.jcloud.DataBlockHeader;
+import org.testng.annotations.Test;
+
+@Slf4j
+public class DataBlockHeaderV2Test {
+
+    @Test
+    public void dataBlockHeaderImplTest() throws Exception {
+        int blockLength = 1024 * 1024;
+        long firstEntryId = 3333L;
+        long ledgerId = 3L;
+
+        StreamingDataBlockHeaderImpl dataBlockHeader = StreamingDataBlockHeaderImpl.of(blockLength,
+                ledgerId, firstEntryId);
+
+        // verify get methods
+        assertEquals(StreamingDataBlockHeaderImpl.getBlockMagicWord(), StreamingDataBlockHeaderImpl.MAGIC_WORD);
+        assertEquals(dataBlockHeader.getBlockLength(), blockLength);
+        assertEquals(dataBlockHeader.getFirstEntryId(), firstEntryId);
+        assertEquals(dataBlockHeader.getLedgerId(), ledgerId);
+
+        // verify toStream and fromStream
+        InputStream stream = dataBlockHeader.toStream();
+        stream.mark(0);
+        StreamingDataBlockHeaderImpl rebuild = StreamingDataBlockHeaderImpl.fromStream(stream);
+        assertEquals(rebuild.getBlockLength(), blockLength);
+        assertEquals(rebuild.getFirstEntryId(), firstEntryId);
+        assertEquals(rebuild.getLedgerId(), ledgerId);
+        // verify InputStream reach end
+        assertEquals(stream.read(), -1);
+
+        stream.reset();
+        byte[] streamContent = new byte[StreamingDataBlockHeaderImpl.getDataStartOffset()];
+
+        // stream with all 0, simulate junk data, should throw exception for header magic not match.
+        try (InputStream stream2 = new ByteArrayInputStream(streamContent, 0,
+                StreamingDataBlockHeaderImpl.getDataStartOffset())) {
+            DataBlockHeader rebuild2 = StreamingDataBlockHeaderImpl.fromStream(stream2);
+            fail("Should throw IOException");
+        } catch (Exception e) {
+            assertTrue(e instanceof IOException);
+            assertTrue(e.getMessage().contains("Data block header magic word not match"));
+        }
+
+        // simulate read header too small, throw EOFException.
+        stream.read(streamContent);
+        try (InputStream stream3 =
+                     new ByteArrayInputStream(streamContent, 0,
+                             StreamingDataBlockHeaderImpl.getDataStartOffset() - 1)) {
+            DataBlockHeader rebuild3 = StreamingDataBlockHeaderImpl.fromStream(stream3);
+            fail("Should throw EOFException");
+        } catch (EOFException e) {
+            // expected
+        }
+
+        stream.close();
+    }
+
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockUtilsTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockUtilsTest.java
index 2956f998bf..f1792b20fd 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockUtilsTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/DataBlockUtilsTest.java
@@ -1,64 +1,64 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static org.testng.Assert.assertEquals;
-import static org.testng.Assert.assertNull;
-import java.util.UUID;
-import org.testng.annotations.Test;
-
-public class DataBlockUtilsTest {
-
-    @Test
-    public void parseLedgerIdTest() throws Exception {
-       UUID id = UUID.randomUUID();
-       long ledgerId = 123124;
-        String key = DataBlockUtils.dataBlockOffloadKey(ledgerId, id);
-        String keyIndex = DataBlockUtils.indexBlockOffloadKey(ledgerId, id);
-
-        assertEquals(ledgerId, DataBlockUtils.parseLedgerId(key).longValue());
-        assertEquals(ledgerId, DataBlockUtils.parseLedgerId(keyIndex).longValue());
-
-        assertNull(DataBlockUtils.parseLedgerId(null));
-        assertNull(DataBlockUtils.parseLedgerId(""));
-        assertNull(DataBlockUtils.parseLedgerId("-ledger-"));
-        assertNull(DataBlockUtils.parseLedgerId("something"));
-        assertNull(DataBlockUtils.parseLedgerId("-ledger-index"));
-    }
-
-    @Test
-    public void parseContextUuidTest() throws Exception {
-        UUID id = UUID.randomUUID();
-        long ledgerId = 123124;
-        String key = DataBlockUtils.dataBlockOffloadKey(ledgerId, id);
-        String keyIndex = DataBlockUtils.indexBlockOffloadKey(ledgerId, id);
-
-        assertEquals(ledgerId, DataBlockUtils.parseLedgerId(key).longValue());
-        assertEquals(ledgerId, DataBlockUtils.parseLedgerId(keyIndex).longValue());
-        assertEquals(id.toString(), DataBlockUtils.parseContextUuid(key, ledgerId));
-        assertEquals(id.toString(), DataBlockUtils.parseContextUuid(keyIndex, ledgerId));
-
-        assertNull(DataBlockUtils.parseContextUuid(null, null));
-        assertNull(DataBlockUtils.parseContextUuid(null, ledgerId));
-        assertNull(DataBlockUtils.parseContextUuid("foo", null));
-        assertNull(DataBlockUtils.parseContextUuid("-ledger-" + ledgerId, ledgerId));
-        assertNull(DataBlockUtils.parseContextUuid("something" + ledgerId, ledgerId));
-    }
-
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertNull;
+import java.util.UUID;
+import org.testng.annotations.Test;
+
+public class DataBlockUtilsTest {
+
+    @Test
+    public void parseLedgerIdTest() throws Exception {
+       UUID id = UUID.randomUUID();
+       long ledgerId = 123124;
+        String key = DataBlockUtils.dataBlockOffloadKey(ledgerId, id);
+        String keyIndex = DataBlockUtils.indexBlockOffloadKey(ledgerId, id);
+
+        assertEquals(ledgerId, DataBlockUtils.parseLedgerId(key).longValue());
+        assertEquals(ledgerId, DataBlockUtils.parseLedgerId(keyIndex).longValue());
+
+        assertNull(DataBlockUtils.parseLedgerId(null));
+        assertNull(DataBlockUtils.parseLedgerId(""));
+        assertNull(DataBlockUtils.parseLedgerId("-ledger-"));
+        assertNull(DataBlockUtils.parseLedgerId("something"));
+        assertNull(DataBlockUtils.parseLedgerId("-ledger-index"));
+    }
+
+    @Test
+    public void parseContextUuidTest() throws Exception {
+        UUID id = UUID.randomUUID();
+        long ledgerId = 123124;
+        String key = DataBlockUtils.dataBlockOffloadKey(ledgerId, id);
+        String keyIndex = DataBlockUtils.indexBlockOffloadKey(ledgerId, id);
+
+        assertEquals(ledgerId, DataBlockUtils.parseLedgerId(key).longValue());
+        assertEquals(ledgerId, DataBlockUtils.parseLedgerId(keyIndex).longValue());
+        assertEquals(id.toString(), DataBlockUtils.parseContextUuid(key, ledgerId));
+        assertEquals(id.toString(), DataBlockUtils.parseContextUuid(keyIndex, ledgerId));
+
+        assertNull(DataBlockUtils.parseContextUuid(null, null));
+        assertNull(DataBlockUtils.parseContextUuid(null, ledgerId));
+        assertNull(DataBlockUtils.parseContextUuid("foo", null));
+        assertNull(DataBlockUtils.parseContextUuid("-ledger-" + ledgerId, ledgerId));
+        assertNull(DataBlockUtils.parseContextUuid("something" + ledgerId, ledgerId));
+    }
+
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/MockManagedLedger.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/MockManagedLedger.java
index d4f07ed9f9..162bf3947a 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/MockManagedLedger.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/MockManagedLedger.java
@@ -1,463 +1,463 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import com.google.common.collect.Range;
-import io.netty.buffer.ByteBuf;
-import java.util.Map;
-import java.util.NavigableMap;
-import java.util.Optional;
-import java.util.concurrent.CompletableFuture;
-import java.util.function.Predicate;
-import lombok.extern.slf4j.Slf4j;
-import org.apache.bookkeeper.mledger.AsyncCallbacks;
-import org.apache.bookkeeper.mledger.Entry;
-import org.apache.bookkeeper.mledger.ManagedCursor;
-import org.apache.bookkeeper.mledger.ManagedLedger;
-import org.apache.bookkeeper.mledger.ManagedLedgerConfig;
-import org.apache.bookkeeper.mledger.ManagedLedgerException;
-import org.apache.bookkeeper.mledger.ManagedLedgerMXBean;
-import org.apache.bookkeeper.mledger.Position;
-import org.apache.bookkeeper.mledger.PositionBound;
-import org.apache.bookkeeper.mledger.intercept.ManagedLedgerInterceptor;
-import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;
-import org.apache.pulsar.common.api.proto.CommandSubscribe;
-import org.apache.pulsar.common.policies.data.ManagedLedgerInternalStats;
-
-@Slf4j
-public class MockManagedLedger implements ManagedLedger {
-    @Override
-    public String getName() {
-        return null;
-    }
-
-    @Override
-    public Position addEntry(byte[] data) {
-        return null;
-    }
-
-    @Override
-    public Position addEntry(byte[] data, int numberOfMessages) {
-        return null;
-    }
-
-    @Override
-    public void asyncAddEntry(byte[] data, AsyncCallbacks.AddEntryCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public Position addEntry(byte[] data, int offset, int length) {
-        return null;
-    }
-
-    @Override
-    public Position addEntry(byte[] data, int numberOfMessages, int offset, int length) throws InterruptedException,
-            ManagedLedgerException {
-        return null;
-    }
-
-    @Override
-    public void asyncAddEntry(byte[] data, int offset, int length, AsyncCallbacks.AddEntryCallback callback,
-                              Object ctx) {
-
-    }
-
-    @Override
-    public void asyncAddEntry(byte[] data, int numberOfMessages, int offset, int length,
-                              AsyncCallbacks.AddEntryCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public void asyncAddEntry(ByteBuf buffer, AsyncCallbacks.AddEntryCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public void asyncAddEntry(ByteBuf buffer, int numberOfMessages, AsyncCallbacks.AddEntryCallback callback,
-                              Object ctx) {
-
-    }
-
-    @Override
-    public ManagedCursor openCursor(String name) throws InterruptedException, ManagedLedgerException {
-        return null;
-    }
-
-    @Override
-    public ManagedCursor openCursor(String name, CommandSubscribe.InitialPosition initialPosition) throws
-            InterruptedException, ManagedLedgerException {
-        return null;
-    }
-
-    @Override
-    public ManagedCursor openCursor(String name, CommandSubscribe.InitialPosition initialPosition,
-                                    Map<String, Long> properties, Map<String, String> cursorProperties)
-            throws InterruptedException, ManagedLedgerException {
-        return null;
-    }
-
-    @Override
-    public ManagedCursor newNonDurableCursor(Position startCursorPosition) throws ManagedLedgerException {
-        return null;
-    }
-
-    @Override
-    public ManagedCursor newNonDurableCursor(Position startPosition, String subscriptionName) throws
-            ManagedLedgerException {
-        return null;
-    }
-
-    @Override
-    public ManagedCursor newNonDurableCursor(Position startPosition, String subscriptionName,
-                                             CommandSubscribe.InitialPosition initialPosition,
-                                             boolean isReadCompacted) throws ManagedLedgerException {
-        return null;
-    }
-
-    @Override
-    public void asyncDeleteCursor(String name, AsyncCallbacks.DeleteCursorCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public void deleteCursor(String name) throws InterruptedException, ManagedLedgerException {
-
-    }
-
-    @Override
-    public void removeWaitingCursor(ManagedCursor cursor) {
-
-    }
-
-    @Override
-    public void asyncOpenCursor(String name, AsyncCallbacks.OpenCursorCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public void asyncOpenCursor(String name, CommandSubscribe.InitialPosition initialPosition,
-                                AsyncCallbacks.OpenCursorCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public void asyncOpenCursor(String name, CommandSubscribe.InitialPosition initialPosition,
-                                Map<String, Long> properties, Map<String, String> cursorProperties,
-                                AsyncCallbacks.OpenCursorCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public Iterable<ManagedCursor> getCursors() {
-        return null;
-    }
-
-    @Override
-    public Iterable<ManagedCursor> getActiveCursors() {
-        return null;
-    }
-
-    @Override
-    public long getNumberOfEntries() {
-        return 0;
-    }
-
-    @Override
-    public long getNumberOfEntries(Range<Position> range) {
-        return 0;
-    }
-
-    @Override
-    public long getNumberOfActiveEntries() {
-        return 0;
-    }
-
-    @Override
-    public long getTotalSize() {
-        return 0;
-    }
-
-    @Override
-    public long getEstimatedBacklogSize() {
-        return 0;
-    }
-
-    @Override
-    public CompletableFuture<Long> getEarliestMessagePublishTimeInBacklog() {
-        return CompletableFuture.completedFuture(0L);
-    }
-
-    @Override
-    public long getOffloadedSize() {
-        return 0;
-    }
-
-    @Override
-    public long getLastOffloadedLedgerId() {
-        return 0;
-    }
-
-    @Override
-    public long getLastOffloadedSuccessTimestamp() {
-        return 0;
-    }
-
-    @Override
-    public long getLastOffloadedFailureTimestamp() {
-        return 0;
-    }
-
-    @Override
-    public void asyncTerminate(AsyncCallbacks.TerminateCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public Position terminate() throws InterruptedException, ManagedLedgerException {
-        return null;
-    }
-
-    @Override
-    public void close() throws InterruptedException, ManagedLedgerException {
-
-    }
-
-    @Override
-    public void asyncClose(AsyncCallbacks.CloseCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public ManagedLedgerMXBean getStats() {
-        return null;
-    }
-
-    @Override
-    public void delete() throws InterruptedException, ManagedLedgerException {
-
-    }
-
-    @Override
-    public void asyncDelete(AsyncCallbacks.DeleteLedgerCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public Position offloadPrefix(Position pos) throws InterruptedException, ManagedLedgerException {
-        return null;
-    }
-
-    @Override
-    public void asyncOffloadPrefix(Position pos, AsyncCallbacks.OffloadCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public ManagedCursor getSlowestConsumer() {
-        return null;
-    }
-
-    @Override
-    public boolean isTerminated() {
-        return false;
-    }
-
-    @Override
-    public ManagedLedgerConfig getConfig() {
-        return new ManagedLedgerConfig();
-    }
-
-    @Override
-    public void setConfig(ManagedLedgerConfig config) {
-
-    }
-
-    @Override
-    public Position getLastConfirmedEntry() {
-        return null;
-    }
-
-    @Override
-    public void readyToCreateNewLedger() {
-
-    }
-
-    @Override
-    public Map<String, String> getProperties() {
-        return null;
-    }
-
-    @Override
-    public void setProperty(String key, String value) throws InterruptedException, ManagedLedgerException {
-
-    }
-
-    @Override
-    public void asyncSetProperty(String key, String value, AsyncCallbacks.UpdatePropertiesCallback callback,
-                                 Object ctx) {
-
-    }
-
-    @Override
-    public void deleteProperty(String key) throws InterruptedException, ManagedLedgerException {
-
-    }
-
-    @Override
-    public void asyncDeleteProperty(String key, AsyncCallbacks.UpdatePropertiesCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public void setProperties(Map<String, String> properties) throws InterruptedException, ManagedLedgerException {
-
-    }
-
-    @Override
-    public void asyncSetProperties(Map<String, String> properties,
-                                   AsyncCallbacks.UpdatePropertiesCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public void trimConsumedLedgersInBackground(CompletableFuture<?> promise) {
-
-    }
-
-    @Override
-    public void rollCurrentLedgerIfFull() {
-
-    }
-
-    @Override
-    public CompletableFuture<Position> asyncFindPosition(Predicate<Entry> predicate) {
-        return CompletableFuture.completedFuture(null);
-    }
-
-    @Override
-    public ManagedLedgerInterceptor getManagedLedgerInterceptor() {
-        return null;
-    }
-
-    @Override
-    public CompletableFuture<LedgerInfo> getLedgerInfo(long ledgerId) {
-        final LedgerInfo build = LedgerInfo.newBuilder().setLedgerId(ledgerId).setSize(100).setEntries(20).build();
-        return CompletableFuture.completedFuture(build);
-    }
-
-    @Override
-    public Optional<LedgerInfo> getOptionalLedgerInfo(long ledgerId) {
-        final LedgerInfo build = LedgerInfo.newBuilder().setLedgerId(ledgerId).setSize(100).setEntries(20).build();
-        return Optional.of(build);
-    }
-
-    @Override
-    public CompletableFuture<Void> asyncTruncate() {
-        return CompletableFuture.completedFuture(null);
-    }
-
-    @Override
-    public CompletableFuture<ManagedLedgerInternalStats> getManagedLedgerInternalStats(boolean includeLedgerMetadata) {
-        return CompletableFuture.completedFuture(null);
-    }
-
-    @Override
-    public boolean checkInactiveLedgerAndRollOver() {
-        return false;
-    }
-
-    @Override
-    public void checkCursorsToCacheEntries() {
-        // no-op
-    }
-
-    @Override
-    public void asyncReadEntry(Position position, AsyncCallbacks.ReadEntryCallback callback, Object ctx) {
-
-    }
-
-    @Override
-    public NavigableMap<Long, LedgerInfo> getLedgersInfo() {
-        return null;
-    }
-
-    @Override
-    public Position getNextValidPosition(Position position) {
-        return null;
-    }
-
-    @Override
-    public Position getPreviousPosition(Position position) {
-        return null;
-    }
-
-    @Override
-    public long getEstimatedBacklogSize(Position position) {
-        return 0;
-    }
-
-    @Override
-    public Position getPositionAfterN(Position startPosition, long n, PositionBound startRange) {
-        return null;
-    }
-
-    @Override
-    public int getPendingAddEntriesCount() {
-        return 0;
-    }
-
-    @Override
-    public long getCacheSize() {
-        return 0;
-    }
-
-    @Override
-    public Position getFirstPosition() {
-        return null;
-    }
-
-    @Override
-    public CompletableFuture<Position> asyncMigrate() {
-        // no-op
-        return null;
-    }
-
-    @Override
-    public CompletableFuture<Void> asyncAddLedgerProperty(long ledgerId, String key, String value) {
-        return null;
-    }
-
-    @Override
-    public CompletableFuture<Void> asyncRemoveLedgerProperty(long ledgerId, String key) {
-        return null;
-    }
-
-    @Override
-    public CompletableFuture<String> asyncGetLedgerProperty(long ledgerId, String key) {
-        return null;
-    }
-
-    @Override
-    public boolean isMigrated() {
-        // no-op
-        return false;
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import com.google.common.collect.Range;
+import io.netty.buffer.ByteBuf;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.Optional;
+import java.util.concurrent.CompletableFuture;
+import java.util.function.Predicate;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.bookkeeper.mledger.AsyncCallbacks;
+import org.apache.bookkeeper.mledger.Entry;
+import org.apache.bookkeeper.mledger.ManagedCursor;
+import org.apache.bookkeeper.mledger.ManagedLedger;
+import org.apache.bookkeeper.mledger.ManagedLedgerConfig;
+import org.apache.bookkeeper.mledger.ManagedLedgerException;
+import org.apache.bookkeeper.mledger.ManagedLedgerMXBean;
+import org.apache.bookkeeper.mledger.Position;
+import org.apache.bookkeeper.mledger.PositionBound;
+import org.apache.bookkeeper.mledger.intercept.ManagedLedgerInterceptor;
+import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;
+import org.apache.pulsar.common.api.proto.CommandSubscribe;
+import org.apache.pulsar.common.policies.data.ManagedLedgerInternalStats;
+
+@Slf4j
+public class MockManagedLedger implements ManagedLedger {
+    @Override
+    public String getName() {
+        return null;
+    }
+
+    @Override
+    public Position addEntry(byte[] data) {
+        return null;
+    }
+
+    @Override
+    public Position addEntry(byte[] data, int numberOfMessages) {
+        return null;
+    }
+
+    @Override
+    public void asyncAddEntry(byte[] data, AsyncCallbacks.AddEntryCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public Position addEntry(byte[] data, int offset, int length) {
+        return null;
+    }
+
+    @Override
+    public Position addEntry(byte[] data, int numberOfMessages, int offset, int length) throws InterruptedException,
+            ManagedLedgerException {
+        return null;
+    }
+
+    @Override
+    public void asyncAddEntry(byte[] data, int offset, int length, AsyncCallbacks.AddEntryCallback callback,
+                              Object ctx) {
+
+    }
+
+    @Override
+    public void asyncAddEntry(byte[] data, int numberOfMessages, int offset, int length,
+                              AsyncCallbacks.AddEntryCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public void asyncAddEntry(ByteBuf buffer, AsyncCallbacks.AddEntryCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public void asyncAddEntry(ByteBuf buffer, int numberOfMessages, AsyncCallbacks.AddEntryCallback callback,
+                              Object ctx) {
+
+    }
+
+    @Override
+    public ManagedCursor openCursor(String name) throws InterruptedException, ManagedLedgerException {
+        return null;
+    }
+
+    @Override
+    public ManagedCursor openCursor(String name, CommandSubscribe.InitialPosition initialPosition) throws
+            InterruptedException, ManagedLedgerException {
+        return null;
+    }
+
+    @Override
+    public ManagedCursor openCursor(String name, CommandSubscribe.InitialPosition initialPosition,
+                                    Map<String, Long> properties, Map<String, String> cursorProperties)
+            throws InterruptedException, ManagedLedgerException {
+        return null;
+    }
+
+    @Override
+    public ManagedCursor newNonDurableCursor(Position startCursorPosition) throws ManagedLedgerException {
+        return null;
+    }
+
+    @Override
+    public ManagedCursor newNonDurableCursor(Position startPosition, String subscriptionName) throws
+            ManagedLedgerException {
+        return null;
+    }
+
+    @Override
+    public ManagedCursor newNonDurableCursor(Position startPosition, String subscriptionName,
+                                             CommandSubscribe.InitialPosition initialPosition,
+                                             boolean isReadCompacted) throws ManagedLedgerException {
+        return null;
+    }
+
+    @Override
+    public void asyncDeleteCursor(String name, AsyncCallbacks.DeleteCursorCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public void deleteCursor(String name) throws InterruptedException, ManagedLedgerException {
+
+    }
+
+    @Override
+    public void removeWaitingCursor(ManagedCursor cursor) {
+
+    }
+
+    @Override
+    public void asyncOpenCursor(String name, AsyncCallbacks.OpenCursorCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public void asyncOpenCursor(String name, CommandSubscribe.InitialPosition initialPosition,
+                                AsyncCallbacks.OpenCursorCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public void asyncOpenCursor(String name, CommandSubscribe.InitialPosition initialPosition,
+                                Map<String, Long> properties, Map<String, String> cursorProperties,
+                                AsyncCallbacks.OpenCursorCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public Iterable<ManagedCursor> getCursors() {
+        return null;
+    }
+
+    @Override
+    public Iterable<ManagedCursor> getActiveCursors() {
+        return null;
+    }
+
+    @Override
+    public long getNumberOfEntries() {
+        return 0;
+    }
+
+    @Override
+    public long getNumberOfEntries(Range<Position> range) {
+        return 0;
+    }
+
+    @Override
+    public long getNumberOfActiveEntries() {
+        return 0;
+    }
+
+    @Override
+    public long getTotalSize() {
+        return 0;
+    }
+
+    @Override
+    public long getEstimatedBacklogSize() {
+        return 0;
+    }
+
+    @Override
+    public CompletableFuture<Long> getEarliestMessagePublishTimeInBacklog() {
+        return CompletableFuture.completedFuture(0L);
+    }
+
+    @Override
+    public long getOffloadedSize() {
+        return 0;
+    }
+
+    @Override
+    public long getLastOffloadedLedgerId() {
+        return 0;
+    }
+
+    @Override
+    public long getLastOffloadedSuccessTimestamp() {
+        return 0;
+    }
+
+    @Override
+    public long getLastOffloadedFailureTimestamp() {
+        return 0;
+    }
+
+    @Override
+    public void asyncTerminate(AsyncCallbacks.TerminateCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public Position terminate() throws InterruptedException, ManagedLedgerException {
+        return null;
+    }
+
+    @Override
+    public void close() throws InterruptedException, ManagedLedgerException {
+
+    }
+
+    @Override
+    public void asyncClose(AsyncCallbacks.CloseCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public ManagedLedgerMXBean getStats() {
+        return null;
+    }
+
+    @Override
+    public void delete() throws InterruptedException, ManagedLedgerException {
+
+    }
+
+    @Override
+    public void asyncDelete(AsyncCallbacks.DeleteLedgerCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public Position offloadPrefix(Position pos) throws InterruptedException, ManagedLedgerException {
+        return null;
+    }
+
+    @Override
+    public void asyncOffloadPrefix(Position pos, AsyncCallbacks.OffloadCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public ManagedCursor getSlowestConsumer() {
+        return null;
+    }
+
+    @Override
+    public boolean isTerminated() {
+        return false;
+    }
+
+    @Override
+    public ManagedLedgerConfig getConfig() {
+        return new ManagedLedgerConfig();
+    }
+
+    @Override
+    public void setConfig(ManagedLedgerConfig config) {
+
+    }
+
+    @Override
+    public Position getLastConfirmedEntry() {
+        return null;
+    }
+
+    @Override
+    public void readyToCreateNewLedger() {
+
+    }
+
+    @Override
+    public Map<String, String> getProperties() {
+        return null;
+    }
+
+    @Override
+    public void setProperty(String key, String value) throws InterruptedException, ManagedLedgerException {
+
+    }
+
+    @Override
+    public void asyncSetProperty(String key, String value, AsyncCallbacks.UpdatePropertiesCallback callback,
+                                 Object ctx) {
+
+    }
+
+    @Override
+    public void deleteProperty(String key) throws InterruptedException, ManagedLedgerException {
+
+    }
+
+    @Override
+    public void asyncDeleteProperty(String key, AsyncCallbacks.UpdatePropertiesCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public void setProperties(Map<String, String> properties) throws InterruptedException, ManagedLedgerException {
+
+    }
+
+    @Override
+    public void asyncSetProperties(Map<String, String> properties,
+                                   AsyncCallbacks.UpdatePropertiesCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public void trimConsumedLedgersInBackground(CompletableFuture<?> promise) {
+
+    }
+
+    @Override
+    public void rollCurrentLedgerIfFull() {
+
+    }
+
+    @Override
+    public CompletableFuture<Position> asyncFindPosition(Predicate<Entry> predicate) {
+        return CompletableFuture.completedFuture(null);
+    }
+
+    @Override
+    public ManagedLedgerInterceptor getManagedLedgerInterceptor() {
+        return null;
+    }
+
+    @Override
+    public CompletableFuture<LedgerInfo> getLedgerInfo(long ledgerId) {
+        final LedgerInfo build = LedgerInfo.newBuilder().setLedgerId(ledgerId).setSize(100).setEntries(20).build();
+        return CompletableFuture.completedFuture(build);
+    }
+
+    @Override
+    public Optional<LedgerInfo> getOptionalLedgerInfo(long ledgerId) {
+        final LedgerInfo build = LedgerInfo.newBuilder().setLedgerId(ledgerId).setSize(100).setEntries(20).build();
+        return Optional.of(build);
+    }
+
+    @Override
+    public CompletableFuture<Void> asyncTruncate() {
+        return CompletableFuture.completedFuture(null);
+    }
+
+    @Override
+    public CompletableFuture<ManagedLedgerInternalStats> getManagedLedgerInternalStats(boolean includeLedgerMetadata) {
+        return CompletableFuture.completedFuture(null);
+    }
+
+    @Override
+    public boolean checkInactiveLedgerAndRollOver() {
+        return false;
+    }
+
+    @Override
+    public void checkCursorsToCacheEntries() {
+        // no-op
+    }
+
+    @Override
+    public void asyncReadEntry(Position position, AsyncCallbacks.ReadEntryCallback callback, Object ctx) {
+
+    }
+
+    @Override
+    public NavigableMap<Long, LedgerInfo> getLedgersInfo() {
+        return null;
+    }
+
+    @Override
+    public Position getNextValidPosition(Position position) {
+        return null;
+    }
+
+    @Override
+    public Position getPreviousPosition(Position position) {
+        return null;
+    }
+
+    @Override
+    public long getEstimatedBacklogSize(Position position) {
+        return 0;
+    }
+
+    @Override
+    public Position getPositionAfterN(Position startPosition, long n, PositionBound startRange) {
+        return null;
+    }
+
+    @Override
+    public int getPendingAddEntriesCount() {
+        return 0;
+    }
+
+    @Override
+    public long getCacheSize() {
+        return 0;
+    }
+
+    @Override
+    public Position getFirstPosition() {
+        return null;
+    }
+
+    @Override
+    public CompletableFuture<Position> asyncMigrate() {
+        // no-op
+        return null;
+    }
+
+    @Override
+    public CompletableFuture<Void> asyncAddLedgerProperty(long ledgerId, String key, String value) {
+        return null;
+    }
+
+    @Override
+    public CompletableFuture<Void> asyncRemoveLedgerProperty(long ledgerId, String key) {
+        return null;
+    }
+
+    @Override
+    public CompletableFuture<String> asyncGetLedgerProperty(long ledgerId, String key) {
+        return null;
+    }
+
+    @Override
+    public boolean isMigrated() {
+        // no-op
+        return false;
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexTest.java
index 1f409598f3..afb6199161 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexTest.java
@@ -1,257 +1,257 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static com.google.common.base.Charsets.UTF_8;
-import static org.testng.Assert.assertEquals;
-import static org.testng.Assert.assertTrue;
-import static org.testng.Assert.fail;
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
-import io.netty.buffer.ByteBuf;
-import io.netty.buffer.Unpooled;
-import java.io.ByteArrayInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.ArrayList;
-import java.util.Map;
-import lombok.extern.slf4j.Slf4j;
-import org.apache.bookkeeper.client.LedgerMetadataBuilder;
-import org.apache.bookkeeper.client.api.DigestType;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockBuilder;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;
-import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;
-import org.apache.bookkeeper.net.BookieId;
-import org.apache.bookkeeper.net.BookieSocketAddress;
-import org.testng.annotations.Test;
-
-@Slf4j
-public class OffloadIndexTest {
-
-    @Test
-    public void offloadIndexEntryImplTest() {
-        // verify OffloadIndexEntryImpl builder
-        OffloadIndexEntryImpl entry1 = OffloadIndexEntryImpl.of(0, 2, 0, 20);
-        OffloadIndexEntryImpl entry2 = OffloadIndexEntryImpl.of(100, 3, 1234, 20);
-
-        // verify OffloadIndexEntryImpl get
-        assertEquals(entry1.getEntryId(), 0L);
-        assertEquals(entry1.getPartId(), 2);
-        assertEquals(entry1.getOffset(), 0L);
-        assertEquals(entry1.getDataOffset(), 20L);
-
-        assertEquals(entry2.getEntryId(), 100L);
-        assertEquals(entry2.getPartId(), 3);
-        assertEquals(entry2.getOffset(), 1234L);
-        assertEquals(entry2.getDataOffset(), 1254L);
-    }
-
-
-    // use mock to setLastEntryId
-//    public static class LedgerMetadataMock extends org.apache.bookkeeper.client.LedgerMetadata {
-//        long lastId = 0;
-//        public LedgerMetadataMock(int ensembleSize, int writeQuorumSize, int ackQuorumSize,
-//        org.apache.bookkeeper.client.BookKeeper.DigestType digestType, byte[] password, Map<String,
-//        byte[]> customMetadata, boolean storeSystemtimeAsLedgerCreationTime) {
-//            super(ensembleSize, writeQuorumSize, ackQuorumSize, digestType, password, customMetadata,
-//            storeSystemtimeAsLedgerCreationTime);
-//        }
-//
-//        @Override
-//        public long getLastEntryId(){
-//            return  lastId;
-//        }
-//
-//        public void setLastEntryId(long lastId) {
-//            this.lastId = lastId;
-//        }
-//    }
-
-    public static LedgerMetadata createLedgerMetadata(long id) throws Exception {
-
-        Map<String, byte[]> metadataCustom = Maps.newHashMap();
-        metadataCustom.put("key1", "value1".getBytes(UTF_8));
-        metadataCustom.put("key7", "value7".getBytes(UTF_8));
-
-        ArrayList<BookieId> bookies = Lists.newArrayList();
-        bookies.add(0, new BookieSocketAddress("127.0.0.1:3181").toBookieId());
-        bookies.add(1, new BookieSocketAddress("127.0.0.2:3181").toBookieId());
-        bookies.add(2, new BookieSocketAddress("127.0.0.3:3181").toBookieId());
-
-        return LedgerMetadataBuilder.create().withEnsembleSize(3).withWriteQuorumSize(3).withAckQuorumSize(2)
-                .withDigestType(DigestType.CRC32C).withPassword("password".getBytes(UTF_8))
-                .withCustomMetadata(metadataCustom).withClosedState().withLastEntryId(5000).withLength(100)
-                .newEnsembleEntry(0L, bookies).withId(id).build();
-
-    }
-
-    public static LedgerInfo createLedgerInfo(long id) throws Exception {
-
-        Map<String, byte[]> metadataCustom = Maps.newHashMap();
-        metadataCustom.put("key1", "value1".getBytes(UTF_8));
-        metadataCustom.put("key7", "value7".getBytes(UTF_8));
-
-        return LedgerInfo.newBuilder().setLedgerId(id).setEntries(5001).setSize(10000).build();
-    }
-
-    // prepare metadata, then use builder to build a OffloadIndexBlockImpl
-    // verify get methods, readout and fromStream methods.
-    @Test
-    public void offloadIndexBlockImplTest() throws Exception {
-        OffloadIndexBlockBuilder blockBuilder = OffloadIndexBlockBuilder.create();
-        LedgerMetadata metadata = createLedgerMetadata(1); // use dummy ledgerId, from BK 4.12 the ledger is is required
-        log.debug("created metadata: {}", metadata.toString());
-
-        blockBuilder.withLedgerMetadata(metadata).withDataObjectLength(1).withDataBlockHeaderLength(23455);
-
-        blockBuilder.addBlock(0, 2, 64 * 1024 * 1024);
-        blockBuilder.addBlock(1000, 3, 64 * 1024 * 1024);
-        blockBuilder.addBlock(2000, 4, 64 * 1024 * 1024);
-        OffloadIndexBlock indexBlock = blockBuilder.build();
-
-        // verify getEntryCount and getLedgerMetadata
-        assertEquals(indexBlock.getEntryCount(), 3);
-        assertEquals(indexBlock.getLedgerMetadata(), metadata);
-
-        // verify getIndexEntryForEntry
-        OffloadIndexEntry entry1 = indexBlock.getIndexEntryForEntry(0);
-        assertEquals(entry1.getEntryId(), 0);
-        assertEquals(entry1.getPartId(), 2);
-        assertEquals(entry1.getOffset(), 0);
-
-        OffloadIndexEntry entry11 = indexBlock.getIndexEntryForEntry(500);
-        assertEquals(entry11, entry1);
-
-        OffloadIndexEntry entry2 = indexBlock.getIndexEntryForEntry(1000);
-        assertEquals(entry2.getEntryId(), 1000);
-        assertEquals(entry2.getPartId(), 3);
-        assertEquals(entry2.getOffset(), 64 * 1024 * 1024);
-
-        OffloadIndexEntry entry22 = indexBlock.getIndexEntryForEntry(1300);
-        assertEquals(entry22, entry2);
-
-        OffloadIndexEntry entry3 = indexBlock.getIndexEntryForEntry(2000);
-
-        assertEquals(entry3.getEntryId(), 2000);
-        assertEquals(entry3.getPartId(), 4);
-        assertEquals(entry3.getOffset(), 2 * 64 * 1024 * 1024);
-
-        OffloadIndexEntry entry33 = indexBlock.getIndexEntryForEntry(3000);
-        assertEquals(entry33, entry3);
-
-        try {
-            OffloadIndexEntry entry4 = indexBlock.getIndexEntryForEntry(6000);
-            fail("Should throw IndexOutOfBoundsException.");
-        } catch (Exception e) {
-            assertTrue(e instanceof IndexOutOfBoundsException);
-            assertEquals(e.getMessage(), "Entry index: 6000 beyond lastEntryId: 5000");
-        }
-
-        // verify toStream
-        InputStream out = indexBlock.toStream();
-        byte b[] = new byte[1024];
-        int readoutLen = out.read(b);
-        out.close();
-        ByteBuf wrapper = Unpooled.wrappedBuffer(b);
-        int magic = wrapper.readInt();
-        int indexBlockLength = wrapper.readInt();
-        long dataObjectLength = wrapper.readLong();
-        long dataHeaderLength = wrapper.readLong();
-        int indexEntryCount = wrapper.readInt();
-        int segmentMetadataLength = wrapper.readInt();
-
-        // verify counter
-        assertEquals(magic, OffloadIndexBlockImpl.getIndexMagicWord());
-        assertEquals(indexBlockLength, readoutLen);
-        assertEquals(indexEntryCount, 3);
-        assertEquals(dataObjectLength, 1);
-        assertEquals(dataHeaderLength, 23455);
-
-        wrapper.readBytes(segmentMetadataLength);
-        log.debug("magic: {}, blockLength: {}, metadataLength: {}, indexCount: {}",
-            magic, indexBlockLength, segmentMetadataLength, indexEntryCount);
-
-        // verify entry
-        OffloadIndexEntry e1 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
-                                                        wrapper.readLong(), dataHeaderLength);
-        OffloadIndexEntry e2 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
-                                                        wrapper.readLong(), dataHeaderLength);
-        OffloadIndexEntry e3 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
-                                                        wrapper.readLong(), dataHeaderLength);
-
-        assertEquals(e1.getEntryId(), entry1.getEntryId());
-        assertEquals(e1.getPartId(), entry1.getPartId());
-        assertEquals(e1.getOffset(), entry1.getOffset());
-        assertEquals(e1.getDataOffset(), entry1.getDataOffset());
-        assertEquals(e2.getEntryId(), entry2.getEntryId());
-        assertEquals(e2.getPartId(), entry2.getPartId());
-        assertEquals(e2.getOffset(), entry2.getOffset());
-        assertEquals(e2.getDataOffset(), entry2.getDataOffset());
-        assertEquals(e3.getEntryId(), entry3.getEntryId());
-        assertEquals(e3.getPartId(), entry3.getPartId());
-        assertEquals(e3.getOffset(), entry3.getOffset());
-        assertEquals(e3.getDataOffset(), entry3.getDataOffset());
-        wrapper.release();
-
-        // verify build OffloadIndexBlock from InputStream
-        InputStream out2 = indexBlock.toStream();
-        int streamLength = out2.available();
-        out2.mark(0);
-        OffloadIndexBlock indexBlock2 = (OffloadIndexBlock) blockBuilder.fromStream(out2);
-        // 1. verify metadata that got from inputstream success.
-        LedgerMetadata metadata2 = indexBlock2.getLedgerMetadata();
-        log.debug("built metadata: {}", metadata2.toString());
-        assertEquals(metadata2.getAckQuorumSize(), metadata.getAckQuorumSize());
-        assertEquals(metadata2.getEnsembleSize(), metadata.getEnsembleSize());
-        assertEquals(metadata2.getDigestType(), metadata.getDigestType());
-        assertEquals(metadata2.getAllEnsembles().entrySet(), metadata.getAllEnsembles().entrySet());
-        // 2. verify set all the entries
-        assertEquals(indexBlock2.getEntryCount(), indexBlock.getEntryCount());
-        // 3. verify reach end
-        assertEquals(out2.read(), -1);
-
-
-        out2.reset();
-        byte streamContent[] = new byte[streamLength];
-        // stream with all 0, simulate junk data, should throw exception for header magic not match.
-        try (InputStream stream3 = new ByteArrayInputStream(streamContent, 0, streamLength)) {
-            OffloadIndexBlock indexBlock3 = (OffloadIndexBlock) blockBuilder.fromStream(stream3);
-            fail("Should throw IOException");
-        } catch (Exception e) {
-            assertTrue(e instanceof IOException);
-            assertTrue(e.getMessage().contains("Invalid MagicWord"));
-        }
-
-        // simulate read header too small, throw EOFException.
-        out2.read(streamContent);
-        try (InputStream stream4 =
-                new ByteArrayInputStream(streamContent, 0, streamLength - 1)) {
-            OffloadIndexBlock indexBlock4 = (OffloadIndexBlock) blockBuilder.fromStream(stream4);
-            fail("Should throw EOFException");
-        } catch (Exception e) {
-            assertTrue(e instanceof java.io.EOFException);
-        }
-
-        out2.close();
-        indexBlock.close();
-    }
-
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static com.google.common.base.Charsets.UTF_8;
+import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertTrue;
+import static org.testng.Assert.fail;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.Unpooled;
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.ArrayList;
+import java.util.Map;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.bookkeeper.client.LedgerMetadataBuilder;
+import org.apache.bookkeeper.client.api.DigestType;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockBuilder;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;
+import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;
+import org.apache.bookkeeper.net.BookieId;
+import org.apache.bookkeeper.net.BookieSocketAddress;
+import org.testng.annotations.Test;
+
+@Slf4j
+public class OffloadIndexTest {
+
+    @Test
+    public void offloadIndexEntryImplTest() {
+        // verify OffloadIndexEntryImpl builder
+        OffloadIndexEntryImpl entry1 = OffloadIndexEntryImpl.of(0, 2, 0, 20);
+        OffloadIndexEntryImpl entry2 = OffloadIndexEntryImpl.of(100, 3, 1234, 20);
+
+        // verify OffloadIndexEntryImpl get
+        assertEquals(entry1.getEntryId(), 0L);
+        assertEquals(entry1.getPartId(), 2);
+        assertEquals(entry1.getOffset(), 0L);
+        assertEquals(entry1.getDataOffset(), 20L);
+
+        assertEquals(entry2.getEntryId(), 100L);
+        assertEquals(entry2.getPartId(), 3);
+        assertEquals(entry2.getOffset(), 1234L);
+        assertEquals(entry2.getDataOffset(), 1254L);
+    }
+
+
+    // use mock to setLastEntryId
+//    public static class LedgerMetadataMock extends org.apache.bookkeeper.client.LedgerMetadata {
+//        long lastId = 0;
+//        public LedgerMetadataMock(int ensembleSize, int writeQuorumSize, int ackQuorumSize,
+//        org.apache.bookkeeper.client.BookKeeper.DigestType digestType, byte[] password, Map<String,
+//        byte[]> customMetadata, boolean storeSystemtimeAsLedgerCreationTime) {
+//            super(ensembleSize, writeQuorumSize, ackQuorumSize, digestType, password, customMetadata,
+//            storeSystemtimeAsLedgerCreationTime);
+//        }
+//
+//        @Override
+//        public long getLastEntryId(){
+//            return  lastId;
+//        }
+//
+//        public void setLastEntryId(long lastId) {
+//            this.lastId = lastId;
+//        }
+//    }
+
+    public static LedgerMetadata createLedgerMetadata(long id) throws Exception {
+
+        Map<String, byte[]> metadataCustom = Maps.newHashMap();
+        metadataCustom.put("key1", "value1".getBytes(UTF_8));
+        metadataCustom.put("key7", "value7".getBytes(UTF_8));
+
+        ArrayList<BookieId> bookies = Lists.newArrayList();
+        bookies.add(0, new BookieSocketAddress("127.0.0.1:3181").toBookieId());
+        bookies.add(1, new BookieSocketAddress("127.0.0.2:3181").toBookieId());
+        bookies.add(2, new BookieSocketAddress("127.0.0.3:3181").toBookieId());
+
+        return LedgerMetadataBuilder.create().withEnsembleSize(3).withWriteQuorumSize(3).withAckQuorumSize(2)
+                .withDigestType(DigestType.CRC32C).withPassword("password".getBytes(UTF_8))
+                .withCustomMetadata(metadataCustom).withClosedState().withLastEntryId(5000).withLength(100)
+                .newEnsembleEntry(0L, bookies).withId(id).build();
+
+    }
+
+    public static LedgerInfo createLedgerInfo(long id) throws Exception {
+
+        Map<String, byte[]> metadataCustom = Maps.newHashMap();
+        metadataCustom.put("key1", "value1".getBytes(UTF_8));
+        metadataCustom.put("key7", "value7".getBytes(UTF_8));
+
+        return LedgerInfo.newBuilder().setLedgerId(id).setEntries(5001).setSize(10000).build();
+    }
+
+    // prepare metadata, then use builder to build a OffloadIndexBlockImpl
+    // verify get methods, readout and fromStream methods.
+    @Test
+    public void offloadIndexBlockImplTest() throws Exception {
+        OffloadIndexBlockBuilder blockBuilder = OffloadIndexBlockBuilder.create();
+        LedgerMetadata metadata = createLedgerMetadata(1); // use dummy ledgerId, from BK 4.12 the ledger is is required
+        log.debug("created metadata: {}", metadata.toString());
+
+        blockBuilder.withLedgerMetadata(metadata).withDataObjectLength(1).withDataBlockHeaderLength(23455);
+
+        blockBuilder.addBlock(0, 2, 64 * 1024 * 1024);
+        blockBuilder.addBlock(1000, 3, 64 * 1024 * 1024);
+        blockBuilder.addBlock(2000, 4, 64 * 1024 * 1024);
+        OffloadIndexBlock indexBlock = blockBuilder.build();
+
+        // verify getEntryCount and getLedgerMetadata
+        assertEquals(indexBlock.getEntryCount(), 3);
+        assertEquals(indexBlock.getLedgerMetadata(), metadata);
+
+        // verify getIndexEntryForEntry
+        OffloadIndexEntry entry1 = indexBlock.getIndexEntryForEntry(0);
+        assertEquals(entry1.getEntryId(), 0);
+        assertEquals(entry1.getPartId(), 2);
+        assertEquals(entry1.getOffset(), 0);
+
+        OffloadIndexEntry entry11 = indexBlock.getIndexEntryForEntry(500);
+        assertEquals(entry11, entry1);
+
+        OffloadIndexEntry entry2 = indexBlock.getIndexEntryForEntry(1000);
+        assertEquals(entry2.getEntryId(), 1000);
+        assertEquals(entry2.getPartId(), 3);
+        assertEquals(entry2.getOffset(), 64 * 1024 * 1024);
+
+        OffloadIndexEntry entry22 = indexBlock.getIndexEntryForEntry(1300);
+        assertEquals(entry22, entry2);
+
+        OffloadIndexEntry entry3 = indexBlock.getIndexEntryForEntry(2000);
+
+        assertEquals(entry3.getEntryId(), 2000);
+        assertEquals(entry3.getPartId(), 4);
+        assertEquals(entry3.getOffset(), 2 * 64 * 1024 * 1024);
+
+        OffloadIndexEntry entry33 = indexBlock.getIndexEntryForEntry(3000);
+        assertEquals(entry33, entry3);
+
+        try {
+            OffloadIndexEntry entry4 = indexBlock.getIndexEntryForEntry(6000);
+            fail("Should throw IndexOutOfBoundsException.");
+        } catch (Exception e) {
+            assertTrue(e instanceof IndexOutOfBoundsException);
+            assertEquals(e.getMessage(), "Entry index: 6000 beyond lastEntryId: 5000");
+        }
+
+        // verify toStream
+        InputStream out = indexBlock.toStream();
+        byte b[] = new byte[1024];
+        int readoutLen = out.read(b);
+        out.close();
+        ByteBuf wrapper = Unpooled.wrappedBuffer(b);
+        int magic = wrapper.readInt();
+        int indexBlockLength = wrapper.readInt();
+        long dataObjectLength = wrapper.readLong();
+        long dataHeaderLength = wrapper.readLong();
+        int indexEntryCount = wrapper.readInt();
+        int segmentMetadataLength = wrapper.readInt();
+
+        // verify counter
+        assertEquals(magic, OffloadIndexBlockImpl.getIndexMagicWord());
+        assertEquals(indexBlockLength, readoutLen);
+        assertEquals(indexEntryCount, 3);
+        assertEquals(dataObjectLength, 1);
+        assertEquals(dataHeaderLength, 23455);
+
+        wrapper.readBytes(segmentMetadataLength);
+        log.debug("magic: {}, blockLength: {}, metadataLength: {}, indexCount: {}",
+            magic, indexBlockLength, segmentMetadataLength, indexEntryCount);
+
+        // verify entry
+        OffloadIndexEntry e1 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
+                                                        wrapper.readLong(), dataHeaderLength);
+        OffloadIndexEntry e2 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
+                                                        wrapper.readLong(), dataHeaderLength);
+        OffloadIndexEntry e3 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
+                                                        wrapper.readLong(), dataHeaderLength);
+
+        assertEquals(e1.getEntryId(), entry1.getEntryId());
+        assertEquals(e1.getPartId(), entry1.getPartId());
+        assertEquals(e1.getOffset(), entry1.getOffset());
+        assertEquals(e1.getDataOffset(), entry1.getDataOffset());
+        assertEquals(e2.getEntryId(), entry2.getEntryId());
+        assertEquals(e2.getPartId(), entry2.getPartId());
+        assertEquals(e2.getOffset(), entry2.getOffset());
+        assertEquals(e2.getDataOffset(), entry2.getDataOffset());
+        assertEquals(e3.getEntryId(), entry3.getEntryId());
+        assertEquals(e3.getPartId(), entry3.getPartId());
+        assertEquals(e3.getOffset(), entry3.getOffset());
+        assertEquals(e3.getDataOffset(), entry3.getDataOffset());
+        wrapper.release();
+
+        // verify build OffloadIndexBlock from InputStream
+        InputStream out2 = indexBlock.toStream();
+        int streamLength = out2.available();
+        out2.mark(0);
+        OffloadIndexBlock indexBlock2 = (OffloadIndexBlock) blockBuilder.fromStream(out2);
+        // 1. verify metadata that got from inputstream success.
+        LedgerMetadata metadata2 = indexBlock2.getLedgerMetadata();
+        log.debug("built metadata: {}", metadata2.toString());
+        assertEquals(metadata2.getAckQuorumSize(), metadata.getAckQuorumSize());
+        assertEquals(metadata2.getEnsembleSize(), metadata.getEnsembleSize());
+        assertEquals(metadata2.getDigestType(), metadata.getDigestType());
+        assertEquals(metadata2.getAllEnsembles().entrySet(), metadata.getAllEnsembles().entrySet());
+        // 2. verify set all the entries
+        assertEquals(indexBlock2.getEntryCount(), indexBlock.getEntryCount());
+        // 3. verify reach end
+        assertEquals(out2.read(), -1);
+
+
+        out2.reset();
+        byte streamContent[] = new byte[streamLength];
+        // stream with all 0, simulate junk data, should throw exception for header magic not match.
+        try (InputStream stream3 = new ByteArrayInputStream(streamContent, 0, streamLength)) {
+            OffloadIndexBlock indexBlock3 = (OffloadIndexBlock) blockBuilder.fromStream(stream3);
+            fail("Should throw IOException");
+        } catch (Exception e) {
+            assertTrue(e instanceof IOException);
+            assertTrue(e.getMessage().contains("Invalid MagicWord"));
+        }
+
+        // simulate read header too small, throw EOFException.
+        out2.read(streamContent);
+        try (InputStream stream4 =
+                new ByteArrayInputStream(streamContent, 0, streamLength - 1)) {
+            OffloadIndexBlock indexBlock4 = (OffloadIndexBlock) blockBuilder.fromStream(stream4);
+            fail("Should throw EOFException");
+        } catch (Exception e) {
+            assertTrue(e instanceof java.io.EOFException);
+        }
+
+        out2.close();
+        indexBlock.close();
+    }
+
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexV2Test.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexV2Test.java
index 9ec4585f14..03d5c09f38 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexV2Test.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffloadIndexV2Test.java
@@ -1,327 +1,327 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static org.testng.Assert.assertEquals;
-import static org.testng.Assert.assertTrue;
-import static org.testng.Assert.fail;
-import io.netty.buffer.ByteBuf;
-import io.netty.buffer.Unpooled;
-import java.io.ByteArrayInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import lombok.extern.slf4j.Slf4j;
-import org.apache.bookkeeper.client.api.LedgerMetadata;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2Builder;
-import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;
-import org.apache.bookkeeper.mledger.offload.jcloud.impl.OffloadIndexBlockV2Impl.CompatibleMetadata;
-import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;
-import org.testng.annotations.Test;
-
-@Slf4j
-public class OffloadIndexV2Test {
-
-    // prepare metadata, then use builder to build a StreamingOffloadIndexBlockImpl
-    // verify get methods, readout and fromStream methods.
-    @Test
-    public void streamingOffloadIndexBlockImplTest() throws Exception {
-        OffloadIndexBlockV2Builder blockBuilder = OffloadIndexBlockV2Builder.create();
-        final long ledgerId = 1; // use dummy ledgerId, from BK 4.12 the ledger is is required
-        LedgerInfo metadata = OffloadIndexTest.createLedgerInfo(ledgerId);
-        log.debug("created metadata: {}", metadata.toString());
-
-        blockBuilder.addLedgerMeta(ledgerId, metadata).withDataObjectLength(1).withDataBlockHeaderLength(23455);
-
-        blockBuilder.addBlock(ledgerId, 0, 2, 64 * 1024 * 1024);
-        blockBuilder.addBlock(ledgerId, 1000, 3, 64 * 1024 * 1024);
-        blockBuilder.addBlock(ledgerId, 2000, 4, 64 * 1024 * 1024);
-        OffloadIndexBlockV2 indexBlock = blockBuilder.buildV2();
-
-        // verify getEntryCount and getLedgerMetadata
-        assertEquals(indexBlock.getEntryCount(), 3);
-        assertEquals(indexBlock.getLedgerMetadata(ledgerId), new CompatibleMetadata(metadata));
-
-        // verify getIndexEntryForEntry
-        OffloadIndexEntry entry1 = indexBlock.getIndexEntryForEntry(ledgerId, 0);
-        assertEquals(entry1.getEntryId(), 0);
-        assertEquals(entry1.getPartId(), 2);
-        assertEquals(entry1.getOffset(), 0);
-
-        OffloadIndexEntry entry11 = indexBlock.getIndexEntryForEntry(ledgerId, 500);
-        assertEquals(entry11, entry1);
-
-        OffloadIndexEntry entry2 = indexBlock.getIndexEntryForEntry(ledgerId, 1000);
-        assertEquals(entry2.getEntryId(), 1000);
-        assertEquals(entry2.getPartId(), 3);
-        assertEquals(entry2.getOffset(), 64 * 1024 * 1024);
-
-        OffloadIndexEntry entry22 = indexBlock.getIndexEntryForEntry(ledgerId, 1300);
-        assertEquals(entry22, entry2);
-
-        OffloadIndexEntry entry3 = indexBlock.getIndexEntryForEntry(ledgerId, 2000);
-
-        assertEquals(entry3.getEntryId(), 2000);
-        assertEquals(entry3.getPartId(), 4);
-        assertEquals(entry3.getOffset(), 2 * 64 * 1024 * 1024);
-
-        OffloadIndexEntry entry33 = indexBlock.getIndexEntryForEntry(ledgerId, 3000);
-        assertEquals(entry33, entry3);
-
-        try {
-            OffloadIndexEntry entry4 = indexBlock.getIndexEntryForEntry(ledgerId, 6000);
-            fail("Should throw IndexOutOfBoundsException.");
-        } catch (Exception e) {
-            assertTrue(e instanceof IndexOutOfBoundsException);
-            assertEquals(e.getMessage(), "Entry index: 6000 beyond lastEntryId: 5000");
-        }
-
-        // verify toStream
-        InputStream out = indexBlock.toStream();
-        byte b[] = new byte[1024];
-        int readoutLen = out.read(b);
-        out.close();
-        ByteBuf wrapper = Unpooled.wrappedBuffer(b);
-        int magic = wrapper.readInt();
-        int indexBlockLength = wrapper.readInt();
-        long dataObjectLength = wrapper.readLong();
-        long dataHeaderLength = wrapper.readLong();
-        assertEquals(ledgerId, wrapper.readLong());
-        int indexEntryCount = wrapper.readInt();
-        int segmentMetadataLength = wrapper.readInt();
-
-        // verify counter
-        assertEquals(magic, OffloadIndexBlockV2Impl.getIndexMagicWord());
-        assertEquals(indexBlockLength, readoutLen);
-        assertEquals(indexEntryCount, 3);
-        assertEquals(dataObjectLength, 1);
-        assertEquals(dataHeaderLength, 23455);
-
-        wrapper.skipBytes(segmentMetadataLength);
-        log.debug("magic: {}, blockLength: {}, metadataLength: {}, indexCount: {}",
-                magic, indexBlockLength, segmentMetadataLength, indexEntryCount);
-
-        // verify entry
-        OffloadIndexEntry e1 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
-                wrapper.readLong(), dataHeaderLength);
-        OffloadIndexEntry e2 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
-                wrapper.readLong(), dataHeaderLength);
-        OffloadIndexEntry e3 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
-                wrapper.readLong(), dataHeaderLength);
-
-        assertEquals(e1.getEntryId(), entry1.getEntryId());
-        assertEquals(e1.getPartId(), entry1.getPartId());
-        assertEquals(e1.getOffset(), entry1.getOffset());
-        assertEquals(e1.getDataOffset(), entry1.getDataOffset());
-        assertEquals(e2.getEntryId(), entry2.getEntryId());
-        assertEquals(e2.getPartId(), entry2.getPartId());
-        assertEquals(e2.getOffset(), entry2.getOffset());
-        assertEquals(e2.getDataOffset(), entry2.getDataOffset());
-        assertEquals(e3.getEntryId(), entry3.getEntryId());
-        assertEquals(e3.getPartId(), entry3.getPartId());
-        assertEquals(e3.getOffset(), entry3.getOffset());
-        assertEquals(e3.getDataOffset(), entry3.getDataOffset());
-        wrapper.release();
-
-        // verify build StreamingOffloadIndexBlock from InputStream
-        InputStream out2 = indexBlock.toStream();
-        int streamLength = out2.available();
-        out2.mark(0);
-        OffloadIndexBlockV2 indexBlock2 = blockBuilder.fromStream(out2);
-        // 1. verify metadata that got from inputstream success.
-        LedgerMetadata metadata2 = indexBlock2.getLedgerMetadata(ledgerId);
-        log.debug("built metadata: {}", metadata2.toString());
-        assertEquals(metadata.getLedgerId(), metadata2.getLedgerId());
-        assertEquals(metadata.getEntries() - 1, metadata2.getLastEntryId());
-        assertEquals(metadata.getSize(), metadata2.getLength());
-        // 2. verify set all the entries
-        assertEquals(indexBlock2.getEntryCount(), indexBlock.getEntryCount());
-        // 3. verify reach end
-        assertEquals(out2.read(), -1);
-
-
-        out2.reset();
-        byte streamContent[] = new byte[streamLength];
-        // stream with all 0, simulate junk data, should throw exception for header magic not match.
-        try (InputStream stream3 = new ByteArrayInputStream(streamContent, 0, streamLength)) {
-            OffloadIndexBlockV2 indexBlock3 = blockBuilder.fromStream(stream3);
-            fail("Should throw IOException");
-        } catch (Exception e) {
-            assertTrue(e instanceof IOException);
-            assertTrue(e.getMessage().contains("Invalid MagicWord"));
-        }
-
-        // simulate read header too small, throw EOFException.
-        out2.read(streamContent);
-        try (InputStream stream4 =
-                     new ByteArrayInputStream(streamContent, 0, streamLength - 1)) {
-            OffloadIndexBlockV2 indexBlock4 = blockBuilder.fromStream(stream4);
-            fail("Should throw EOFException");
-        } catch (Exception e) {
-            assertTrue(e instanceof java.io.EOFException);
-        }
-
-        out2.close();
-        indexBlock.close();
-    }
-
-    @Test
-    public void streamingMultiLedgerOffloadIndexBlockImplTest() throws Exception {
-        OffloadIndexBlockV2Builder blockBuilder = OffloadIndexBlockV2Builder.create();
-        final long ledgerId1 = 1; // use dummy ledgerId, from BK 4.12 the ledger is is required
-        final long ledgerId2 = 2;
-        LedgerInfo metadata1 = OffloadIndexTest.createLedgerInfo(ledgerId1);
-        LedgerInfo metadata2 = OffloadIndexTest.createLedgerInfo(ledgerId2);
-        log.debug("created metadata: {}", metadata1.toString());
-        log.debug("created metadata: {}", metadata2.toString());
-
-        blockBuilder.addLedgerMeta(ledgerId1, metadata1)
-                .addLedgerMeta(ledgerId2, metadata2)
-                .withDataObjectLength(1)
-                .withDataBlockHeaderLength(23455);
-
-        blockBuilder.addBlock(ledgerId1, 1000, 2, 64 * 1024 * 1024);
-        blockBuilder.addBlock(ledgerId2, 0, 3, 64 * 1024 * 1024);
-        blockBuilder.addBlock(ledgerId2, 1000, 4, 64 * 1024 * 1024);
-        OffloadIndexBlockV2 indexBlock = blockBuilder.buildV2();
-
-        // verify getEntryCount and getLedgerMetadata
-        assertEquals(indexBlock.getEntryCount(), 3);
-        assertEquals(indexBlock.getLedgerMetadata(ledgerId1),
-                new CompatibleMetadata(metadata1));
-        assertEquals(indexBlock.getLedgerMetadata(ledgerId2), new CompatibleMetadata(metadata2));
-
-        // verify getIndexEntryForEntry
-        OffloadIndexEntry entry1 = indexBlock.getIndexEntryForEntry(ledgerId1, 1000);
-        assertEquals(entry1.getEntryId(), 1000);
-        assertEquals(entry1.getPartId(), 2);
-        assertEquals(entry1.getOffset(), 0);
-
-        OffloadIndexEntry entry11 = indexBlock.getIndexEntryForEntry(ledgerId1, 1500);
-        assertEquals(entry11, entry1);
-
-        OffloadIndexEntry entry2 = indexBlock.getIndexEntryForEntry(ledgerId2, 0);
-        assertEquals(entry2.getEntryId(), 0);
-        assertEquals(entry2.getPartId(), 3);
-        assertEquals(entry2.getOffset(), 64 * 1024 * 1024);
-
-        OffloadIndexEntry entry22 = indexBlock.getIndexEntryForEntry(ledgerId2, 300);
-        assertEquals(entry22, entry2);
-
-        OffloadIndexEntry entry3 = indexBlock.getIndexEntryForEntry(ledgerId2, 1000);
-
-        assertEquals(entry3.getEntryId(), 1000);
-        assertEquals(entry3.getPartId(), 4);
-        assertEquals(entry3.getOffset(), 2 * 64 * 1024 * 1024);
-
-        OffloadIndexEntry entry33 = indexBlock.getIndexEntryForEntry(ledgerId2, 2000);
-        assertEquals(entry33, entry3);
-
-        try {
-            OffloadIndexEntry entry4 = indexBlock.getIndexEntryForEntry(ledgerId2, 6000);
-            fail("Should throw IndexOutOfBoundsException.");
-        } catch (Exception e) {
-            assertTrue(e instanceof IndexOutOfBoundsException);
-            assertEquals(e.getMessage(), "Entry index: 6000 beyond lastEntryId: 5000");
-        }
-
-        // verify toStream
-        InputStream out = indexBlock.toStream();
-        byte b[] = new byte[1024];
-        int readoutLen = out.read(b);
-        out.close();
-        ByteBuf wrapper = Unpooled.wrappedBuffer(b);
-        int magic = wrapper.readInt();
-        int indexBlockLength = wrapper.readInt();
-        long dataObjectLength = wrapper.readLong();
-        long dataHeaderLength = wrapper.readLong();
-        assertEquals(ledgerId1, wrapper.readLong());
-        int indexEntryCount = wrapper.readInt();
-        int segmentMetadataLength = wrapper.readInt();
-
-        // verify counter
-        assertEquals(magic, OffloadIndexBlockV2Impl.getIndexMagicWord());
-        assertEquals(indexBlockLength, readoutLen);
-        assertEquals(indexEntryCount, 1);
-        assertEquals(dataObjectLength, 1);
-        assertEquals(dataHeaderLength, 23455);
-
-        wrapper.skipBytes(segmentMetadataLength);
-        log.debug("magic: {}, blockLength: {}, metadataLength: {}, indexCount: {}",
-                magic, indexBlockLength, segmentMetadataLength, indexEntryCount);
-
-        // verify entry
-        OffloadIndexEntry e1 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
-                wrapper.readLong(), dataHeaderLength);
-
-        assertEquals(e1.getEntryId(), entry1.getEntryId());
-        assertEquals(e1.getPartId(), entry1.getPartId());
-        assertEquals(e1.getOffset(), entry1.getOffset());
-        assertEquals(e1.getDataOffset(), entry1.getDataOffset());
-
-
-        assertEquals(ledgerId2, wrapper.readLong());
-        int indexEntryCount2 = wrapper.readInt();
-        assertEquals(indexEntryCount2, 2);
-        int segmentMetadataLength2 = wrapper.readInt();
-        wrapper.skipBytes(segmentMetadataLength2);
-
-        OffloadIndexEntry e2 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
-                wrapper.readLong(), dataHeaderLength);
-        OffloadIndexEntry e3 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
-                wrapper.readLong(), dataHeaderLength);
-
-        assertEquals(e2.getEntryId(), entry2.getEntryId());
-        assertEquals(e2.getPartId(), entry2.getPartId());
-        assertEquals(e2.getOffset(), entry2.getOffset());
-        assertEquals(e2.getDataOffset(), entry2.getDataOffset());
-        assertEquals(e3.getEntryId(), entry3.getEntryId());
-        assertEquals(e3.getPartId(), entry3.getPartId());
-        assertEquals(e3.getOffset(), entry3.getOffset());
-        assertEquals(e3.getDataOffset(), entry3.getDataOffset());
-        wrapper.release();
-
-        // verify build StreamingOffloadIndexBlock from InputStream
-        InputStream out2 = indexBlock.toStream();
-        int streamLength = out2.available();
-        out2.mark(0);
-        OffloadIndexBlockV2 indexBlock2 = blockBuilder.fromStream(out2);
-        // 1. verify metadata that got from inputstream success.
-        //TODO change to meaningful things
-//        LedgerMetadata metadata1back = indexBlock2.getLedgerMetadata(ledgerId1);
-//        log.debug("built metadata: {}", metadata1back.toString());
-//        assertEquals(metadata1back.getAckQuorumSize(), metadata1.getAckQuorumSize());
-//        assertEquals(metadata1back.getEnsembleSize(), metadata1.getEnsembleSize());
-//        assertEquals(metadata1back.getDigestType(), metadata1.getDigestType());
-//        assertEquals(metadata1back.getAllEnsembles().entrySet(), metadata1.getAllEnsembles().entrySet());
-//        LedgerMetadata metadata2back = indexBlock2.getLedgerMetadata(ledgerId2);
-//        log.debug("built metadata: {}", metadata2back.toString());
-//        assertEquals(metadata2back.getAckQuorumSize(), metadata1.getAckQuorumSize());
-//        assertEquals(metadata2back.getEnsembleSize(), metadata1.getEnsembleSize());
-//        assertEquals(metadata2back.getDigestType(), metadata1.getDigestType());
-//        assertEquals(metadata2back.getAllEnsembles().entrySet(), metadata1.getAllEnsembles().entrySet());
-        // 2. verify set all the entries
-        assertEquals(indexBlock2.getEntryCount(), indexBlock.getEntryCount());
-        // 3. verify reach end
-        assertEquals(out2.read(), -1);
-
-        out2.close();
-        indexBlock.close();
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertTrue;
+import static org.testng.Assert.fail;
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.Unpooled;
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.bookkeeper.client.api.LedgerMetadata;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2Builder;
+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;
+import org.apache.bookkeeper.mledger.offload.jcloud.impl.OffloadIndexBlockV2Impl.CompatibleMetadata;
+import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;
+import org.testng.annotations.Test;
+
+@Slf4j
+public class OffloadIndexV2Test {
+
+    // prepare metadata, then use builder to build a StreamingOffloadIndexBlockImpl
+    // verify get methods, readout and fromStream methods.
+    @Test
+    public void streamingOffloadIndexBlockImplTest() throws Exception {
+        OffloadIndexBlockV2Builder blockBuilder = OffloadIndexBlockV2Builder.create();
+        final long ledgerId = 1; // use dummy ledgerId, from BK 4.12 the ledger is is required
+        LedgerInfo metadata = OffloadIndexTest.createLedgerInfo(ledgerId);
+        log.debug("created metadata: {}", metadata.toString());
+
+        blockBuilder.addLedgerMeta(ledgerId, metadata).withDataObjectLength(1).withDataBlockHeaderLength(23455);
+
+        blockBuilder.addBlock(ledgerId, 0, 2, 64 * 1024 * 1024);
+        blockBuilder.addBlock(ledgerId, 1000, 3, 64 * 1024 * 1024);
+        blockBuilder.addBlock(ledgerId, 2000, 4, 64 * 1024 * 1024);
+        OffloadIndexBlockV2 indexBlock = blockBuilder.buildV2();
+
+        // verify getEntryCount and getLedgerMetadata
+        assertEquals(indexBlock.getEntryCount(), 3);
+        assertEquals(indexBlock.getLedgerMetadata(ledgerId), new CompatibleMetadata(metadata));
+
+        // verify getIndexEntryForEntry
+        OffloadIndexEntry entry1 = indexBlock.getIndexEntryForEntry(ledgerId, 0);
+        assertEquals(entry1.getEntryId(), 0);
+        assertEquals(entry1.getPartId(), 2);
+        assertEquals(entry1.getOffset(), 0);
+
+        OffloadIndexEntry entry11 = indexBlock.getIndexEntryForEntry(ledgerId, 500);
+        assertEquals(entry11, entry1);
+
+        OffloadIndexEntry entry2 = indexBlock.getIndexEntryForEntry(ledgerId, 1000);
+        assertEquals(entry2.getEntryId(), 1000);
+        assertEquals(entry2.getPartId(), 3);
+        assertEquals(entry2.getOffset(), 64 * 1024 * 1024);
+
+        OffloadIndexEntry entry22 = indexBlock.getIndexEntryForEntry(ledgerId, 1300);
+        assertEquals(entry22, entry2);
+
+        OffloadIndexEntry entry3 = indexBlock.getIndexEntryForEntry(ledgerId, 2000);
+
+        assertEquals(entry3.getEntryId(), 2000);
+        assertEquals(entry3.getPartId(), 4);
+        assertEquals(entry3.getOffset(), 2 * 64 * 1024 * 1024);
+
+        OffloadIndexEntry entry33 = indexBlock.getIndexEntryForEntry(ledgerId, 3000);
+        assertEquals(entry33, entry3);
+
+        try {
+            OffloadIndexEntry entry4 = indexBlock.getIndexEntryForEntry(ledgerId, 6000);
+            fail("Should throw IndexOutOfBoundsException.");
+        } catch (Exception e) {
+            assertTrue(e instanceof IndexOutOfBoundsException);
+            assertEquals(e.getMessage(), "Entry index: 6000 beyond lastEntryId: 5000");
+        }
+
+        // verify toStream
+        InputStream out = indexBlock.toStream();
+        byte b[] = new byte[1024];
+        int readoutLen = out.read(b);
+        out.close();
+        ByteBuf wrapper = Unpooled.wrappedBuffer(b);
+        int magic = wrapper.readInt();
+        int indexBlockLength = wrapper.readInt();
+        long dataObjectLength = wrapper.readLong();
+        long dataHeaderLength = wrapper.readLong();
+        assertEquals(ledgerId, wrapper.readLong());
+        int indexEntryCount = wrapper.readInt();
+        int segmentMetadataLength = wrapper.readInt();
+
+        // verify counter
+        assertEquals(magic, OffloadIndexBlockV2Impl.getIndexMagicWord());
+        assertEquals(indexBlockLength, readoutLen);
+        assertEquals(indexEntryCount, 3);
+        assertEquals(dataObjectLength, 1);
+        assertEquals(dataHeaderLength, 23455);
+
+        wrapper.skipBytes(segmentMetadataLength);
+        log.debug("magic: {}, blockLength: {}, metadataLength: {}, indexCount: {}",
+                magic, indexBlockLength, segmentMetadataLength, indexEntryCount);
+
+        // verify entry
+        OffloadIndexEntry e1 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
+                wrapper.readLong(), dataHeaderLength);
+        OffloadIndexEntry e2 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
+                wrapper.readLong(), dataHeaderLength);
+        OffloadIndexEntry e3 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
+                wrapper.readLong(), dataHeaderLength);
+
+        assertEquals(e1.getEntryId(), entry1.getEntryId());
+        assertEquals(e1.getPartId(), entry1.getPartId());
+        assertEquals(e1.getOffset(), entry1.getOffset());
+        assertEquals(e1.getDataOffset(), entry1.getDataOffset());
+        assertEquals(e2.getEntryId(), entry2.getEntryId());
+        assertEquals(e2.getPartId(), entry2.getPartId());
+        assertEquals(e2.getOffset(), entry2.getOffset());
+        assertEquals(e2.getDataOffset(), entry2.getDataOffset());
+        assertEquals(e3.getEntryId(), entry3.getEntryId());
+        assertEquals(e3.getPartId(), entry3.getPartId());
+        assertEquals(e3.getOffset(), entry3.getOffset());
+        assertEquals(e3.getDataOffset(), entry3.getDataOffset());
+        wrapper.release();
+
+        // verify build StreamingOffloadIndexBlock from InputStream
+        InputStream out2 = indexBlock.toStream();
+        int streamLength = out2.available();
+        out2.mark(0);
+        OffloadIndexBlockV2 indexBlock2 = blockBuilder.fromStream(out2);
+        // 1. verify metadata that got from inputstream success.
+        LedgerMetadata metadata2 = indexBlock2.getLedgerMetadata(ledgerId);
+        log.debug("built metadata: {}", metadata2.toString());
+        assertEquals(metadata.getLedgerId(), metadata2.getLedgerId());
+        assertEquals(metadata.getEntries() - 1, metadata2.getLastEntryId());
+        assertEquals(metadata.getSize(), metadata2.getLength());
+        // 2. verify set all the entries
+        assertEquals(indexBlock2.getEntryCount(), indexBlock.getEntryCount());
+        // 3. verify reach end
+        assertEquals(out2.read(), -1);
+
+
+        out2.reset();
+        byte streamContent[] = new byte[streamLength];
+        // stream with all 0, simulate junk data, should throw exception for header magic not match.
+        try (InputStream stream3 = new ByteArrayInputStream(streamContent, 0, streamLength)) {
+            OffloadIndexBlockV2 indexBlock3 = blockBuilder.fromStream(stream3);
+            fail("Should throw IOException");
+        } catch (Exception e) {
+            assertTrue(e instanceof IOException);
+            assertTrue(e.getMessage().contains("Invalid MagicWord"));
+        }
+
+        // simulate read header too small, throw EOFException.
+        out2.read(streamContent);
+        try (InputStream stream4 =
+                     new ByteArrayInputStream(streamContent, 0, streamLength - 1)) {
+            OffloadIndexBlockV2 indexBlock4 = blockBuilder.fromStream(stream4);
+            fail("Should throw EOFException");
+        } catch (Exception e) {
+            assertTrue(e instanceof java.io.EOFException);
+        }
+
+        out2.close();
+        indexBlock.close();
+    }
+
+    @Test
+    public void streamingMultiLedgerOffloadIndexBlockImplTest() throws Exception {
+        OffloadIndexBlockV2Builder blockBuilder = OffloadIndexBlockV2Builder.create();
+        final long ledgerId1 = 1; // use dummy ledgerId, from BK 4.12 the ledger is is required
+        final long ledgerId2 = 2;
+        LedgerInfo metadata1 = OffloadIndexTest.createLedgerInfo(ledgerId1);
+        LedgerInfo metadata2 = OffloadIndexTest.createLedgerInfo(ledgerId2);
+        log.debug("created metadata: {}", metadata1.toString());
+        log.debug("created metadata: {}", metadata2.toString());
+
+        blockBuilder.addLedgerMeta(ledgerId1, metadata1)
+                .addLedgerMeta(ledgerId2, metadata2)
+                .withDataObjectLength(1)
+                .withDataBlockHeaderLength(23455);
+
+        blockBuilder.addBlock(ledgerId1, 1000, 2, 64 * 1024 * 1024);
+        blockBuilder.addBlock(ledgerId2, 0, 3, 64 * 1024 * 1024);
+        blockBuilder.addBlock(ledgerId2, 1000, 4, 64 * 1024 * 1024);
+        OffloadIndexBlockV2 indexBlock = blockBuilder.buildV2();
+
+        // verify getEntryCount and getLedgerMetadata
+        assertEquals(indexBlock.getEntryCount(), 3);
+        assertEquals(indexBlock.getLedgerMetadata(ledgerId1),
+                new CompatibleMetadata(metadata1));
+        assertEquals(indexBlock.getLedgerMetadata(ledgerId2), new CompatibleMetadata(metadata2));
+
+        // verify getIndexEntryForEntry
+        OffloadIndexEntry entry1 = indexBlock.getIndexEntryForEntry(ledgerId1, 1000);
+        assertEquals(entry1.getEntryId(), 1000);
+        assertEquals(entry1.getPartId(), 2);
+        assertEquals(entry1.getOffset(), 0);
+
+        OffloadIndexEntry entry11 = indexBlock.getIndexEntryForEntry(ledgerId1, 1500);
+        assertEquals(entry11, entry1);
+
+        OffloadIndexEntry entry2 = indexBlock.getIndexEntryForEntry(ledgerId2, 0);
+        assertEquals(entry2.getEntryId(), 0);
+        assertEquals(entry2.getPartId(), 3);
+        assertEquals(entry2.getOffset(), 64 * 1024 * 1024);
+
+        OffloadIndexEntry entry22 = indexBlock.getIndexEntryForEntry(ledgerId2, 300);
+        assertEquals(entry22, entry2);
+
+        OffloadIndexEntry entry3 = indexBlock.getIndexEntryForEntry(ledgerId2, 1000);
+
+        assertEquals(entry3.getEntryId(), 1000);
+        assertEquals(entry3.getPartId(), 4);
+        assertEquals(entry3.getOffset(), 2 * 64 * 1024 * 1024);
+
+        OffloadIndexEntry entry33 = indexBlock.getIndexEntryForEntry(ledgerId2, 2000);
+        assertEquals(entry33, entry3);
+
+        try {
+            OffloadIndexEntry entry4 = indexBlock.getIndexEntryForEntry(ledgerId2, 6000);
+            fail("Should throw IndexOutOfBoundsException.");
+        } catch (Exception e) {
+            assertTrue(e instanceof IndexOutOfBoundsException);
+            assertEquals(e.getMessage(), "Entry index: 6000 beyond lastEntryId: 5000");
+        }
+
+        // verify toStream
+        InputStream out = indexBlock.toStream();
+        byte b[] = new byte[1024];
+        int readoutLen = out.read(b);
+        out.close();
+        ByteBuf wrapper = Unpooled.wrappedBuffer(b);
+        int magic = wrapper.readInt();
+        int indexBlockLength = wrapper.readInt();
+        long dataObjectLength = wrapper.readLong();
+        long dataHeaderLength = wrapper.readLong();
+        assertEquals(ledgerId1, wrapper.readLong());
+        int indexEntryCount = wrapper.readInt();
+        int segmentMetadataLength = wrapper.readInt();
+
+        // verify counter
+        assertEquals(magic, OffloadIndexBlockV2Impl.getIndexMagicWord());
+        assertEquals(indexBlockLength, readoutLen);
+        assertEquals(indexEntryCount, 1);
+        assertEquals(dataObjectLength, 1);
+        assertEquals(dataHeaderLength, 23455);
+
+        wrapper.skipBytes(segmentMetadataLength);
+        log.debug("magic: {}, blockLength: {}, metadataLength: {}, indexCount: {}",
+                magic, indexBlockLength, segmentMetadataLength, indexEntryCount);
+
+        // verify entry
+        OffloadIndexEntry e1 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
+                wrapper.readLong(), dataHeaderLength);
+
+        assertEquals(e1.getEntryId(), entry1.getEntryId());
+        assertEquals(e1.getPartId(), entry1.getPartId());
+        assertEquals(e1.getOffset(), entry1.getOffset());
+        assertEquals(e1.getDataOffset(), entry1.getDataOffset());
+
+
+        assertEquals(ledgerId2, wrapper.readLong());
+        int indexEntryCount2 = wrapper.readInt();
+        assertEquals(indexEntryCount2, 2);
+        int segmentMetadataLength2 = wrapper.readInt();
+        wrapper.skipBytes(segmentMetadataLength2);
+
+        OffloadIndexEntry e2 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
+                wrapper.readLong(), dataHeaderLength);
+        OffloadIndexEntry e3 = OffloadIndexEntryImpl.of(wrapper.readLong(), wrapper.readInt(),
+                wrapper.readLong(), dataHeaderLength);
+
+        assertEquals(e2.getEntryId(), entry2.getEntryId());
+        assertEquals(e2.getPartId(), entry2.getPartId());
+        assertEquals(e2.getOffset(), entry2.getOffset());
+        assertEquals(e2.getDataOffset(), entry2.getDataOffset());
+        assertEquals(e3.getEntryId(), entry3.getEntryId());
+        assertEquals(e3.getPartId(), entry3.getPartId());
+        assertEquals(e3.getOffset(), entry3.getOffset());
+        assertEquals(e3.getDataOffset(), entry3.getDataOffset());
+        wrapper.release();
+
+        // verify build StreamingOffloadIndexBlock from InputStream
+        InputStream out2 = indexBlock.toStream();
+        int streamLength = out2.available();
+        out2.mark(0);
+        OffloadIndexBlockV2 indexBlock2 = blockBuilder.fromStream(out2);
+        // 1. verify metadata that got from inputstream success.
+        //TODO change to meaningful things
+//        LedgerMetadata metadata1back = indexBlock2.getLedgerMetadata(ledgerId1);
+//        log.debug("built metadata: {}", metadata1back.toString());
+//        assertEquals(metadata1back.getAckQuorumSize(), metadata1.getAckQuorumSize());
+//        assertEquals(metadata1back.getEnsembleSize(), metadata1.getEnsembleSize());
+//        assertEquals(metadata1back.getDigestType(), metadata1.getDigestType());
+//        assertEquals(metadata1back.getAllEnsembles().entrySet(), metadata1.getAllEnsembles().entrySet());
+//        LedgerMetadata metadata2back = indexBlock2.getLedgerMetadata(ledgerId2);
+//        log.debug("built metadata: {}", metadata2back.toString());
+//        assertEquals(metadata2back.getAckQuorumSize(), metadata1.getAckQuorumSize());
+//        assertEquals(metadata2back.getEnsembleSize(), metadata1.getEnsembleSize());
+//        assertEquals(metadata2back.getDigestType(), metadata1.getDigestType());
+//        assertEquals(metadata2back.getAllEnsembles().entrySet(), metadata1.getAllEnsembles().entrySet());
+        // 2. verify set all the entries
+        assertEquals(indexBlock2.getEntryCount(), indexBlock.getEntryCount());
+        // 3. verify reach end
+        assertEquals(out2.read(), -1);
+
+        out2.close();
+        indexBlock.close();
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffsetsCacheTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffsetsCacheTest.java
index 5eb11edf95..ab5a98498e 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffsetsCacheTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/OffsetsCacheTest.java
@@ -1,45 +1,45 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.impl;
-
-import static org.testng.Assert.assertEquals;
-import static org.testng.Assert.assertNull;
-import lombok.extern.slf4j.Slf4j;
-import org.testng.annotations.Test;
-
-@Slf4j
-public class OffsetsCacheTest {
-
-    @Test
-    public void testCache() throws Exception {
-        System.setProperty("pulsar.jclouds.readhandleimpl.offsetsscache.ttl.seconds", "1");
-        OffsetsCache offsetsCache = new OffsetsCache();
-        assertNull(offsetsCache.getIfPresent(1, 2));
-        offsetsCache.put(1, 1, 1);
-        assertEquals(offsetsCache.getIfPresent(1, 1), 1);
-        offsetsCache.clear();
-        assertNull(offsetsCache.getIfPresent(1, 1));
-        // test ttl
-        offsetsCache.put(1, 2, 2);
-        assertEquals(offsetsCache.getIfPresent(1, 2), 2);
-        Thread.sleep(2000);
-        assertNull(offsetsCache.getIfPresent(1, 2));
-        offsetsCache.close();
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.impl;
+
+import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertNull;
+import lombok.extern.slf4j.Slf4j;
+import org.testng.annotations.Test;
+
+@Slf4j
+public class OffsetsCacheTest {
+
+    @Test
+    public void testCache() throws Exception {
+        System.setProperty("pulsar.jclouds.readhandleimpl.offsetsscache.ttl.seconds", "1");
+        OffsetsCache offsetsCache = new OffsetsCache();
+        assertNull(offsetsCache.getIfPresent(1, 2));
+        offsetsCache.put(1, 1, 1);
+        assertEquals(offsetsCache.getIfPresent(1, 1), 1);
+        offsetsCache.clear();
+        assertNull(offsetsCache.getIfPresent(1, 1));
+        // test ttl
+        offsetsCache.put(1, 2, 2);
+        assertEquals(offsetsCache.getIfPresent(1, 2), 2);
+        Thread.sleep(2000);
+        assertNull(offsetsCache.getIfPresent(1, 2));
+        offsetsCache.close();
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/AbstractJCloudBlobStoreFactoryTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/AbstractJCloudBlobStoreFactoryTest.java
index a32c592df6..6a71c4a1ac 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/AbstractJCloudBlobStoreFactoryTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/AbstractJCloudBlobStoreFactoryTest.java
@@ -1,114 +1,114 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.provider;
-
-import com.google.common.base.Charsets;
-import com.google.common.collect.Lists;
-import com.google.common.io.ByteSource;
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-import org.apache.commons.io.IOUtils;
-import org.jclouds.blobstore.BlobStore;
-import org.jclouds.blobstore.domain.Blob;
-import org.jclouds.blobstore.domain.BlobBuilder;
-import org.jclouds.blobstore.domain.MultipartPart;
-import org.jclouds.blobstore.domain.MultipartUpload;
-import org.jclouds.blobstore.options.PutOptions;
-import org.jclouds.io.Payload;
-import org.jclouds.io.Payloads;
-import org.testng.Assert;
-import org.testng.annotations.BeforeTest;
-
-public abstract class AbstractJCloudBlobStoreFactoryTest {
-
-    protected static final int DEFAULT_BLOCK_SIZE = 5 * 1024 * 1024;
-    protected static final int DEFAULT_READ_BUFFER_SIZE = 1 * 1024 * 1024;
-    protected static final ByteSource PAYLOAD = ByteSource.wrap("blob-content".getBytes(Charsets.UTF_8));
-
-    protected TieredStorageConfiguration config;
-    protected BlobStore blobStore;
-
-    protected abstract Map<String, String> getConfig();
-
-    @BeforeTest
-    protected final void init() throws Exception {
-        config =  TieredStorageConfiguration.create(getConfig());
-        config.getProvider().validate(config);
-        blobStore = config.getBlobStore();
-    }
-
-    protected void sendBlob(String containerName, String blobName, ByteSource payload) throws IOException {
-     // Create a Blob
-        Blob blob = createBlob(payload, blobName);
-
-        // Upload the Blob
-        blobStore.putBlob(containerName, blob);
-        Assert.assertTrue(blobStore.blobExists(containerName, blobName));
-    }
-
-    protected void verifyBlob(String containerName, String blobName, ByteSource payload) throws IOException {
-        Blob retrieved = blobStore.getBlob(containerName, blobName);
-        Assert.assertNotNull(retrieved);
-
-        Payload p = retrieved.getPayload();
-        Assert.assertEquals(IOUtils.toByteArray(p.openStream()), payload.read());
-    }
-
-    protected void sendAndVerifyBlob(String containerName, String blobName, ByteSource payload) throws IOException {
-        sendBlob(containerName, blobName, payload);
-        verifyBlob(containerName, blobName, payload);
-    }
-
-    protected void sendMultipartPayload(String containerName, String blobName, String[] lines) {
-        MultipartUpload mpu = null;
-        List<MultipartPart> parts = Lists.newArrayList();
-
-        BlobBuilder blobBuilder = blobStore.blobBuilder(blobName);
-        Blob blob = blobBuilder.build();
-        mpu = blobStore.initiateMultipartUpload(containerName, blob.getMetadata(), new PutOptions());
-
-        int partId = 1;
-        for (String line: lines) {
-            Payload partPayload = Payloads.newByteArrayPayload(line.getBytes());
-            partPayload.getContentMetadata().setContentLength((long) line.length());
-            partPayload.getContentMetadata().setContentType("application/octet-stream");
-            parts.add(blobStore.uploadMultipartPart(mpu, partId++, partPayload));
-        }
-
-        blobStore.completeMultipartUpload(mpu, parts);
-    }
-
-    protected void deleteBlobAndVerify(String containerName, String blobName) {
-        blobStore.removeBlob(containerName, blobName);
-        Assert.assertFalse(blobStore.blobExists(containerName, blobName));
-    }
-
-    protected void deleteContainerAndVerify(String containerName) {
-        blobStore.deleteContainer(containerName);
-        Assert.assertFalse(blobStore.containerExists(containerName));
-    }
-
-    protected Blob createBlob(ByteSource payload, String name) throws IOException {
-        return blobStore.blobBuilder(name) // The blob name?
-                .payload(payload.read())
-                .contentLength(payload.size())
-                .build();
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.provider;
+
+import com.google.common.base.Charsets;
+import com.google.common.collect.Lists;
+import com.google.common.io.ByteSource;
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import org.apache.commons.io.IOUtils;
+import org.jclouds.blobstore.BlobStore;
+import org.jclouds.blobstore.domain.Blob;
+import org.jclouds.blobstore.domain.BlobBuilder;
+import org.jclouds.blobstore.domain.MultipartPart;
+import org.jclouds.blobstore.domain.MultipartUpload;
+import org.jclouds.blobstore.options.PutOptions;
+import org.jclouds.io.Payload;
+import org.jclouds.io.Payloads;
+import org.testng.Assert;
+import org.testng.annotations.BeforeTest;
+
+public abstract class AbstractJCloudBlobStoreFactoryTest {
+
+    protected static final int DEFAULT_BLOCK_SIZE = 5 * 1024 * 1024;
+    protected static final int DEFAULT_READ_BUFFER_SIZE = 1 * 1024 * 1024;
+    protected static final ByteSource PAYLOAD = ByteSource.wrap("blob-content".getBytes(Charsets.UTF_8));
+
+    protected TieredStorageConfiguration config;
+    protected BlobStore blobStore;
+
+    protected abstract Map<String, String> getConfig();
+
+    @BeforeTest
+    protected final void init() throws Exception {
+        config =  TieredStorageConfiguration.create(getConfig());
+        config.getProvider().validate(config);
+        blobStore = config.getBlobStore();
+    }
+
+    protected void sendBlob(String containerName, String blobName, ByteSource payload) throws IOException {
+     // Create a Blob
+        Blob blob = createBlob(payload, blobName);
+
+        // Upload the Blob
+        blobStore.putBlob(containerName, blob);
+        Assert.assertTrue(blobStore.blobExists(containerName, blobName));
+    }
+
+    protected void verifyBlob(String containerName, String blobName, ByteSource payload) throws IOException {
+        Blob retrieved = blobStore.getBlob(containerName, blobName);
+        Assert.assertNotNull(retrieved);
+
+        Payload p = retrieved.getPayload();
+        Assert.assertEquals(IOUtils.toByteArray(p.openStream()), payload.read());
+    }
+
+    protected void sendAndVerifyBlob(String containerName, String blobName, ByteSource payload) throws IOException {
+        sendBlob(containerName, blobName, payload);
+        verifyBlob(containerName, blobName, payload);
+    }
+
+    protected void sendMultipartPayload(String containerName, String blobName, String[] lines) {
+        MultipartUpload mpu = null;
+        List<MultipartPart> parts = Lists.newArrayList();
+
+        BlobBuilder blobBuilder = blobStore.blobBuilder(blobName);
+        Blob blob = blobBuilder.build();
+        mpu = blobStore.initiateMultipartUpload(containerName, blob.getMetadata(), new PutOptions());
+
+        int partId = 1;
+        for (String line: lines) {
+            Payload partPayload = Payloads.newByteArrayPayload(line.getBytes());
+            partPayload.getContentMetadata().setContentLength((long) line.length());
+            partPayload.getContentMetadata().setContentType("application/octet-stream");
+            parts.add(blobStore.uploadMultipartPart(mpu, partId++, partPayload));
+        }
+
+        blobStore.completeMultipartUpload(mpu, parts);
+    }
+
+    protected void deleteBlobAndVerify(String containerName, String blobName) {
+        blobStore.removeBlob(containerName, blobName);
+        Assert.assertFalse(blobStore.blobExists(containerName, blobName));
+    }
+
+    protected void deleteContainerAndVerify(String containerName) {
+        blobStore.deleteContainer(containerName);
+        Assert.assertFalse(blobStore.containerExists(containerName));
+    }
+
+    protected Blob createBlob(ByteSource payload, String name) throws IOException {
+        return blobStore.blobBuilder(name) // The blob name?
+                .payload(payload.read())
+                .contentLength(payload.size())
+                .build();
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/JCloudBlobStoreProviderTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/JCloudBlobStoreProviderTest.java
index f45cb30ea4..12c2152eb0 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/JCloudBlobStoreProviderTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/JCloudBlobStoreProviderTest.java
@@ -1,133 +1,133 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.provider;
-
-import static org.testng.Assert.assertEquals;
-import java.util.HashMap;
-import java.util.Map;
-import org.testng.annotations.Test;
-
-public class JCloudBlobStoreProviderTest {
-
-    private TieredStorageConfiguration config;
-
-    @Test
-    public void awsValidationSuccessTest() {
-        Map<String, String> map = new HashMap<String, String>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
-        map.put("managedLedgerOffloadRegion", "us-east-1");
-        map.put("managedLedgerOffloadBucket", "test bucket");
-        map.put("managedLedgerOffloadMaxBlockSizeInBytes", "99999999");
-        config = new TieredStorageConfiguration(map);
-        JCloudBlobStoreProvider.AWS_S3.validate(config);
-    }
-
-    @Test
-    public void awsValidationDefaultBlockSizeTest() {
-        Map<String, String> map = new HashMap<String, String>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
-        map.put("managedLedgerOffloadRegion", "us-east-1");
-        map.put("managedLedgerOffloadBucket", "test bucket");
-        config = new TieredStorageConfiguration(map);
-        JCloudBlobStoreProvider.AWS_S3.validate(config);
-    }
-
-    @Test(expectedExceptions = IllegalArgumentException.class,
-            expectedExceptionsMessageRegExp = "Either Region or ServiceEndpoint must specified for aws-s3 offload")
-    public void awsValidationMissingRegionTest() {
-        Map<String, String> map = new HashMap<String, String>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
-        map.put("managedLedgerOffloadBucket", "my-bucket");
-        map.put("managedLedgerOffloadMaxBlockSizeInBytes", "999999");
-        config = new TieredStorageConfiguration(map);
-        JCloudBlobStoreProvider.AWS_S3.validate(config);
-    }
-
-    @Test(expectedExceptions = IllegalArgumentException.class,
-            expectedExceptionsMessageRegExp = "Bucket cannot be empty for aws-s3 offload")
-    public void awsValidationMissingBucketTest() {
-        Map<String, String> map = new HashMap<String, String>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
-        map.put("managedLedgerOffloadRegion", "us-east-1");
-        map.put("managedLedgerOffloadMaxBlockSizeInBytes", "99999999");
-        config = new TieredStorageConfiguration(map);
-        assertEquals(config.getRegion(), "us-east-1");
-        JCloudBlobStoreProvider.AWS_S3.validate(config);
-    }
-
-    @Test(expectedExceptions = IllegalArgumentException.class,
-            expectedExceptionsMessageRegExp = "ManagedLedgerOffloadMaxBlockSizeInBytes cannot "
-                    + "be less than 5MB for aws-s3 offload")
-    public void awsValidationBlockSizeTest() {
-        Map<String, String> map = new HashMap<String, String>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
-        map.put("managedLedgerOffloadRegion", "us-east-1");
-        map.put("managedLedgerOffloadBucket", "test bucket");
-        map.put("managedLedgerOffloadMaxBlockSizeInBytes", "1");
-        config = new TieredStorageConfiguration(map);
-        JCloudBlobStoreProvider.AWS_S3.validate(config);
-    }
-
-    @Test
-    public void transientValidationSuccessTest() {
-        Map<String, String> map = new HashMap<String, String>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, "transient");
-        map.put("managedLedgerOffloadBucket", "test bucket");
-        config = new TieredStorageConfiguration(map);
-        JCloudBlobStoreProvider.TRANSIENT.validate(config);
-    }
-
-    @Test(expectedExceptions = IllegalArgumentException.class,
-            expectedExceptionsMessageRegExp = "Bucket cannot be empty for Local offload")
-    public void transientValidationFailureTest() {
-        Map<String, String> map = new HashMap<String, String>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, "transient");
-        config = new TieredStorageConfiguration(map);
-        JCloudBlobStoreProvider.TRANSIENT.validate(config);
-    }
-
-    @Test()
-    public void s3ValidationTest() {
-        Map<String, String> map = new HashMap<>();
-        map.put("managedLedgerOffloadDriver", "S3");
-        map.put("managedLedgerOffloadServiceEndpoint", "http://s3.service");
-        map.put("managedLedgerOffloadBucket", "test-s3-bucket");
-        TieredStorageConfiguration configuration = new TieredStorageConfiguration(map);
-        configuration.getProvider().validate(configuration);
-    }
-
-    @Test(expectedExceptions = IllegalArgumentException.class,
-        expectedExceptionsMessageRegExp = "ServiceEndpoint must specified for S3 offload")
-    public void s3ValidationServiceEndpointMissed() {
-        Map<String, String> map = new HashMap<>();
-        map.put("managedLedgerOffloadDriver", "S3");
-        TieredStorageConfiguration configuration = new TieredStorageConfiguration(map);
-        configuration.getProvider().validate(configuration);
-    }
-
-    @Test(expectedExceptions = IllegalArgumentException.class,
-        expectedExceptionsMessageRegExp = "Bucket cannot be empty for S3 offload")
-    public void s3ValidationBucketMissed() {
-        Map<String, String> map = new HashMap<>();
-        map.put("managedLedgerOffloadDriver", "S3");
-        map.put("managedLedgerOffloadServiceEndpoint", "http://s3.service");
-        TieredStorageConfiguration configuration = new TieredStorageConfiguration(map);
-        configuration.getProvider().validate(configuration);
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.provider;
+
+import static org.testng.Assert.assertEquals;
+import java.util.HashMap;
+import java.util.Map;
+import org.testng.annotations.Test;
+
+public class JCloudBlobStoreProviderTest {
+
+    private TieredStorageConfiguration config;
+
+    @Test
+    public void awsValidationSuccessTest() {
+        Map<String, String> map = new HashMap<String, String>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
+        map.put("managedLedgerOffloadRegion", "us-east-1");
+        map.put("managedLedgerOffloadBucket", "test bucket");
+        map.put("managedLedgerOffloadMaxBlockSizeInBytes", "99999999");
+        config = new TieredStorageConfiguration(map);
+        JCloudBlobStoreProvider.AWS_S3.validate(config);
+    }
+
+    @Test
+    public void awsValidationDefaultBlockSizeTest() {
+        Map<String, String> map = new HashMap<String, String>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
+        map.put("managedLedgerOffloadRegion", "us-east-1");
+        map.put("managedLedgerOffloadBucket", "test bucket");
+        config = new TieredStorageConfiguration(map);
+        JCloudBlobStoreProvider.AWS_S3.validate(config);
+    }
+
+    @Test(expectedExceptions = IllegalArgumentException.class,
+            expectedExceptionsMessageRegExp = "Either Region or ServiceEndpoint must specified for aws-s3 offload")
+    public void awsValidationMissingRegionTest() {
+        Map<String, String> map = new HashMap<String, String>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
+        map.put("managedLedgerOffloadBucket", "my-bucket");
+        map.put("managedLedgerOffloadMaxBlockSizeInBytes", "999999");
+        config = new TieredStorageConfiguration(map);
+        JCloudBlobStoreProvider.AWS_S3.validate(config);
+    }
+
+    @Test(expectedExceptions = IllegalArgumentException.class,
+            expectedExceptionsMessageRegExp = "Bucket cannot be empty for aws-s3 offload")
+    public void awsValidationMissingBucketTest() {
+        Map<String, String> map = new HashMap<String, String>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
+        map.put("managedLedgerOffloadRegion", "us-east-1");
+        map.put("managedLedgerOffloadMaxBlockSizeInBytes", "99999999");
+        config = new TieredStorageConfiguration(map);
+        assertEquals(config.getRegion(), "us-east-1");
+        JCloudBlobStoreProvider.AWS_S3.validate(config);
+    }
+
+    @Test(expectedExceptions = IllegalArgumentException.class,
+            expectedExceptionsMessageRegExp = "ManagedLedgerOffloadMaxBlockSizeInBytes cannot "
+                    + "be less than 5MB for aws-s3 offload")
+    public void awsValidationBlockSizeTest() {
+        Map<String, String> map = new HashMap<String, String>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
+        map.put("managedLedgerOffloadRegion", "us-east-1");
+        map.put("managedLedgerOffloadBucket", "test bucket");
+        map.put("managedLedgerOffloadMaxBlockSizeInBytes", "1");
+        config = new TieredStorageConfiguration(map);
+        JCloudBlobStoreProvider.AWS_S3.validate(config);
+    }
+
+    @Test
+    public void transientValidationSuccessTest() {
+        Map<String, String> map = new HashMap<String, String>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, "transient");
+        map.put("managedLedgerOffloadBucket", "test bucket");
+        config = new TieredStorageConfiguration(map);
+        JCloudBlobStoreProvider.TRANSIENT.validate(config);
+    }
+
+    @Test(expectedExceptions = IllegalArgumentException.class,
+            expectedExceptionsMessageRegExp = "Bucket cannot be empty for Local offload")
+    public void transientValidationFailureTest() {
+        Map<String, String> map = new HashMap<String, String>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, "transient");
+        config = new TieredStorageConfiguration(map);
+        JCloudBlobStoreProvider.TRANSIENT.validate(config);
+    }
+
+    @Test()
+    public void s3ValidationTest() {
+        Map<String, String> map = new HashMap<>();
+        map.put("managedLedgerOffloadDriver", "S3");
+        map.put("managedLedgerOffloadServiceEndpoint", "http://s3.service");
+        map.put("managedLedgerOffloadBucket", "test-s3-bucket");
+        TieredStorageConfiguration configuration = new TieredStorageConfiguration(map);
+        configuration.getProvider().validate(configuration);
+    }
+
+    @Test(expectedExceptions = IllegalArgumentException.class,
+        expectedExceptionsMessageRegExp = "ServiceEndpoint must specified for S3 offload")
+    public void s3ValidationServiceEndpointMissed() {
+        Map<String, String> map = new HashMap<>();
+        map.put("managedLedgerOffloadDriver", "S3");
+        TieredStorageConfiguration configuration = new TieredStorageConfiguration(map);
+        configuration.getProvider().validate(configuration);
+    }
+
+    @Test(expectedExceptions = IllegalArgumentException.class,
+        expectedExceptionsMessageRegExp = "Bucket cannot be empty for S3 offload")
+    public void s3ValidationBucketMissed() {
+        Map<String, String> map = new HashMap<>();
+        map.put("managedLedgerOffloadDriver", "S3");
+        map.put("managedLedgerOffloadServiceEndpoint", "http://s3.service");
+        TieredStorageConfiguration configuration = new TieredStorageConfiguration(map);
+        configuration.getProvider().validate(configuration);
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TieredStorageConfigurationTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TieredStorageConfigurationTest.java
index 2c3fc70baa..2267cb6eb6 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TieredStorageConfigurationTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TieredStorageConfigurationTest.java
@@ -1,233 +1,233 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.provider;
-
-import static org.testng.Assert.assertEquals;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-import org.jclouds.domain.Credentials;
-import org.testng.annotations.Test;
-
-public class TieredStorageConfigurationTest {
-
-    /*
-     * Previous property names, for backwards-compatibility.
-     */
-    static final String BC_S3_REGION = "s3ManagedLedgerOffloadRegion";
-    static final String BC_S3_BUCKET = "s3ManagedLedgerOffloadBucket";
-    static final String BC_S3_ENDPOINT = "s3ManagedLedgerOffloadServiceEndpoint";
-    static final String BC_S3_MAX_BLOCK_SIZE = "s3ManagedLedgerOffloadMaxBlockSizeInBytes";
-    static final String BC_S3_READ_BUFFER_SIZE = "s3ManagedLedgerOffloadReadBufferSizeInBytes";
-
-    static final String BC_GCS_BUCKET = "gcsManagedLedgerOffloadBucket";
-    static final String BC_GCS_REGION = "gcsManagedLedgerOffloadRegion";
-    static final String BC_GCS_MAX_BLOCK_SIZE = "gcsManagedLedgerOffloadMaxBlockSizeInBytes";
-    static final String BC_GCS_READ_BUFFER_SIZE = "gcsManagedLedgerOffloadReadBufferSizeInBytes";
-
-
-    /**
-     * Confirm that both property options are available for AWS.
-     */
-    @Test
-    public final void awsS3KeysTest() {
-        Map<String, String> map = new HashMap<>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
-        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
-        List<String> keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_BUCKET);
-        assertEquals(keys.get(0), BC_S3_BUCKET);
-        assertEquals(keys.get(1), "managedLedgerOffloadBucket");
-
-        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_REGION);
-        assertEquals(keys.get(0), BC_S3_REGION);
-        assertEquals(keys.get(1), "managedLedgerOffloadRegion");
-
-        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_ENDPOINT);
-        assertEquals(keys.get(0), BC_S3_ENDPOINT);
-        assertEquals(keys.get(1), "managedLedgerOffloadServiceEndpoint");
-
-        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE);
-        assertEquals(keys.get(0), BC_S3_MAX_BLOCK_SIZE);
-        assertEquals(keys.get(1), "managedLedgerOffloadMaxBlockSizeInBytes");
-
-        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_READ_BUFFER_SIZE);
-        assertEquals(keys.get(0), BC_S3_READ_BUFFER_SIZE);
-        assertEquals(keys.get(1), "managedLedgerOffloadReadBufferSizeInBytes");
-    }
-
-    /**
-     * Confirm that we can configure AWS using the new properties.
-     */
-    @Test
-    public final void awsS3PropertiesTest() {
-        Map<String, String> map = new HashMap<>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
-        map.put("managedLedgerOffloadRegion", "us-east-1");
-        map.put("managedLedgerOffloadBucket", "test bucket");
-        map.put("managedLedgerOffloadMaxBlockSizeInBytes", "1");
-        map.put("managedLedgerOffloadReadBufferSizeInBytes", "500");
-        map.put("managedLedgerOffloadServiceEndpoint", "http://some-url:9093");
-        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
-
-        assertEquals(config.getRegion(), "us-east-1");
-        assertEquals(config.getBucket(), "test bucket");
-        assertEquals(config.getMaxBlockSizeInBytes(), Integer.valueOf(1));
-        assertEquals(config.getReadBufferSizeInBytes(), Integer.valueOf(500));
-        assertEquals(config.getServiceEndpoint(), "http://some-url:9093");
-    }
-
-    /**
-     * Confirm that we can configure AWS using the old properties.
-     */
-    @Test
-    public final void awsS3BackwardCompatiblePropertiesTest() {
-        Map<String, String> map = new HashMap<>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
-        map.put(BC_S3_BUCKET, "test bucket");
-        map.put(BC_S3_ENDPOINT, "http://some-url:9093");
-        map.put(BC_S3_MAX_BLOCK_SIZE, "12");
-        map.put(BC_S3_READ_BUFFER_SIZE, "500");
-        map.put(BC_S3_REGION, "test region");
-        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
-
-        assertEquals(config.getRegion(), "test region");
-        assertEquals(config.getBucket(), "test bucket");
-        assertEquals(config.getMaxBlockSizeInBytes(), Integer.valueOf(12));
-        assertEquals(config.getReadBufferSizeInBytes(), Integer.valueOf(500));
-        assertEquals(config.getServiceEndpoint(), "http://some-url:9093");
-    }
-
-    /**
-     * Confirm that with AWS we create different instances of the credentials
-     * object each time we call the supplier, this ensure that we get fresh credentials
-     * if the aws credential provider changes.
-     */
-    @Test
-    public final void awsS3CredsProviderTest() {
-        Map<String, String> map = new HashMap<>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
-        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
-
-        // set the aws properties with fake creds so the defaultProviderChain works
-        System.setProperty("aws.accessKeyId", "fakeid1");
-        System.setProperty("aws.secretKey", "fakekey1");
-        try {
-            Credentials creds1 = config.getProviderCredentials().get();
-            assertEquals(creds1.identity, "fakeid1");
-            assertEquals(creds1.credential, "fakekey1");
-
-            // reset the properties and ensure we get different values by re-evaluating the chain
-            System.setProperty("aws.accessKeyId", "fakeid2");
-            System.setProperty("aws.secretKey", "fakekey2");
-            Credentials creds2 = config.getProviderCredentials().get();
-            assertEquals(creds2.identity, "fakeid2");
-            assertEquals(creds2.credential, "fakekey2");
-        } finally {
-            System.clearProperty("aws.accessKeyId");
-            System.clearProperty("aws.secretKey");
-        }
-    }
-
-    /**
-     * Confirm that both property options are available for GCS.
-     */
-    @Test
-    public final void gcsKeysTest() {
-        Map<String, String> map = new HashMap<>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY,
-                JCloudBlobStoreProvider.GOOGLE_CLOUD_STORAGE.getDriver());
-        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
-        List<String> keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_BUCKET);
-        assertEquals(keys.get(0), BC_GCS_BUCKET);
-        assertEquals(keys.get(1), "managedLedgerOffloadBucket");
-
-        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_REGION);
-        assertEquals(keys.get(0), BC_GCS_REGION);
-        assertEquals(keys.get(1), "managedLedgerOffloadRegion");
-
-        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE);
-        assertEquals(keys.get(0), BC_GCS_MAX_BLOCK_SIZE);
-        assertEquals(keys.get(1), "managedLedgerOffloadMaxBlockSizeInBytes");
-
-        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_READ_BUFFER_SIZE);
-        assertEquals(keys.get(0), BC_GCS_READ_BUFFER_SIZE);
-        assertEquals(keys.get(1), "managedLedgerOffloadReadBufferSizeInBytes");
-    }
-
-    /**
-     * Confirm that we can configure GCS using the new properties.
-     */
-    @Test
-    public final void gcsPropertiesTest() {
-        Map<String, String> map = new HashMap<>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY,
-                JCloudBlobStoreProvider.GOOGLE_CLOUD_STORAGE.getDriver());
-        map.put("managedLedgerOffloadRegion", "us-east-1");
-        map.put("managedLedgerOffloadBucket", "test bucket");
-        map.put("managedLedgerOffloadMaxBlockSizeInBytes", "1");
-        map.put("managedLedgerOffloadReadBufferSizeInBytes", "500");
-        map.put("managedLedgerOffloadServiceEndpoint", "http://some-url:9093");
-        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
-
-        assertEquals(config.getRegion(), "us-east-1");
-        assertEquals(config.getBucket(), "test bucket");
-        assertEquals(config.getMaxBlockSizeInBytes(), Integer.valueOf(1));
-        assertEquals(config.getReadBufferSizeInBytes(), Integer.valueOf(500));
-    }
-
-    /**
-     * Confirm that we can configure GCS using the old properties.
-     */
-    @Test
-    public final void gcsBackwardCompatiblePropertiesTest() {
-        Map<String, String> map = new HashMap<>();
-        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY,
-                JCloudBlobStoreProvider.GOOGLE_CLOUD_STORAGE.getDriver());
-        map.put(BC_GCS_BUCKET, "test bucket");
-        map.put(BC_GCS_MAX_BLOCK_SIZE, "12");
-        map.put(BC_GCS_READ_BUFFER_SIZE, "500");
-        map.put(BC_GCS_REGION, "test region");
-        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
-
-        assertEquals(config.getRegion(), "test region");
-        assertEquals(config.getBucket(), "test bucket");
-        assertEquals(config.getMaxBlockSizeInBytes(), Integer.valueOf(12));
-        assertEquals(config.getReadBufferSizeInBytes(), Integer.valueOf(500));
-    }
-
-    @Test
-    public void overridePropertiesTest() {
-        Map<String, String> map = new HashMap<>();
-        map.put("s3ManagedLedgerOffloadServiceEndpoint", "http://localhost");
-        map.put("s3ManagedLedgerOffloadRegion", "my-region");
-        System.setProperty("jclouds.SystemPropertyA", "A");
-        System.setProperty("jclouds.region", "jclouds-region");
-        try {
-            TieredStorageConfiguration config = new TieredStorageConfiguration(map);
-            Properties properties = config.getOverrides();
-            assertEquals(properties.get("jclouds.region"), "jclouds-region");
-            assertEquals(config.getServiceEndpoint(), "http://localhost");
-            assertEquals(properties.get("jclouds.SystemPropertyA"), "A");
-        } finally {
-            System.clearProperty("jclouds.SystemPropertyA");
-            System.clearProperty("jclouds.region");
-        }
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.provider;
+
+import static org.testng.Assert.assertEquals;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+import org.jclouds.domain.Credentials;
+import org.testng.annotations.Test;
+
+public class TieredStorageConfigurationTest {
+
+    /*
+     * Previous property names, for backwards-compatibility.
+     */
+    static final String BC_S3_REGION = "s3ManagedLedgerOffloadRegion";
+    static final String BC_S3_BUCKET = "s3ManagedLedgerOffloadBucket";
+    static final String BC_S3_ENDPOINT = "s3ManagedLedgerOffloadServiceEndpoint";
+    static final String BC_S3_MAX_BLOCK_SIZE = "s3ManagedLedgerOffloadMaxBlockSizeInBytes";
+    static final String BC_S3_READ_BUFFER_SIZE = "s3ManagedLedgerOffloadReadBufferSizeInBytes";
+
+    static final String BC_GCS_BUCKET = "gcsManagedLedgerOffloadBucket";
+    static final String BC_GCS_REGION = "gcsManagedLedgerOffloadRegion";
+    static final String BC_GCS_MAX_BLOCK_SIZE = "gcsManagedLedgerOffloadMaxBlockSizeInBytes";
+    static final String BC_GCS_READ_BUFFER_SIZE = "gcsManagedLedgerOffloadReadBufferSizeInBytes";
+
+
+    /**
+     * Confirm that both property options are available for AWS.
+     */
+    @Test
+    public final void awsS3KeysTest() {
+        Map<String, String> map = new HashMap<>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
+        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
+        List<String> keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_BUCKET);
+        assertEquals(keys.get(0), BC_S3_BUCKET);
+        assertEquals(keys.get(1), "managedLedgerOffloadBucket");
+
+        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_REGION);
+        assertEquals(keys.get(0), BC_S3_REGION);
+        assertEquals(keys.get(1), "managedLedgerOffloadRegion");
+
+        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_ENDPOINT);
+        assertEquals(keys.get(0), BC_S3_ENDPOINT);
+        assertEquals(keys.get(1), "managedLedgerOffloadServiceEndpoint");
+
+        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE);
+        assertEquals(keys.get(0), BC_S3_MAX_BLOCK_SIZE);
+        assertEquals(keys.get(1), "managedLedgerOffloadMaxBlockSizeInBytes");
+
+        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_READ_BUFFER_SIZE);
+        assertEquals(keys.get(0), BC_S3_READ_BUFFER_SIZE);
+        assertEquals(keys.get(1), "managedLedgerOffloadReadBufferSizeInBytes");
+    }
+
+    /**
+     * Confirm that we can configure AWS using the new properties.
+     */
+    @Test
+    public final void awsS3PropertiesTest() {
+        Map<String, String> map = new HashMap<>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
+        map.put("managedLedgerOffloadRegion", "us-east-1");
+        map.put("managedLedgerOffloadBucket", "test bucket");
+        map.put("managedLedgerOffloadMaxBlockSizeInBytes", "1");
+        map.put("managedLedgerOffloadReadBufferSizeInBytes", "500");
+        map.put("managedLedgerOffloadServiceEndpoint", "http://some-url:9093");
+        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
+
+        assertEquals(config.getRegion(), "us-east-1");
+        assertEquals(config.getBucket(), "test bucket");
+        assertEquals(config.getMaxBlockSizeInBytes(), Integer.valueOf(1));
+        assertEquals(config.getReadBufferSizeInBytes(), Integer.valueOf(500));
+        assertEquals(config.getServiceEndpoint(), "http://some-url:9093");
+    }
+
+    /**
+     * Confirm that we can configure AWS using the old properties.
+     */
+    @Test
+    public final void awsS3BackwardCompatiblePropertiesTest() {
+        Map<String, String> map = new HashMap<>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
+        map.put(BC_S3_BUCKET, "test bucket");
+        map.put(BC_S3_ENDPOINT, "http://some-url:9093");
+        map.put(BC_S3_MAX_BLOCK_SIZE, "12");
+        map.put(BC_S3_READ_BUFFER_SIZE, "500");
+        map.put(BC_S3_REGION, "test region");
+        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
+
+        assertEquals(config.getRegion(), "test region");
+        assertEquals(config.getBucket(), "test bucket");
+        assertEquals(config.getMaxBlockSizeInBytes(), Integer.valueOf(12));
+        assertEquals(config.getReadBufferSizeInBytes(), Integer.valueOf(500));
+        assertEquals(config.getServiceEndpoint(), "http://some-url:9093");
+    }
+
+    /**
+     * Confirm that with AWS we create different instances of the credentials
+     * object each time we call the supplier, this ensure that we get fresh credentials
+     * if the aws credential provider changes.
+     */
+    @Test
+    public final void awsS3CredsProviderTest() {
+        Map<String, String> map = new HashMap<>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, JCloudBlobStoreProvider.AWS_S3.getDriver());
+        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
+
+        // set the aws properties with fake creds so the defaultProviderChain works
+        System.setProperty("aws.accessKeyId", "fakeid1");
+        System.setProperty("aws.secretKey", "fakekey1");
+        try {
+            Credentials creds1 = config.getProviderCredentials().get();
+            assertEquals(creds1.identity, "fakeid1");
+            assertEquals(creds1.credential, "fakekey1");
+
+            // reset the properties and ensure we get different values by re-evaluating the chain
+            System.setProperty("aws.accessKeyId", "fakeid2");
+            System.setProperty("aws.secretKey", "fakekey2");
+            Credentials creds2 = config.getProviderCredentials().get();
+            assertEquals(creds2.identity, "fakeid2");
+            assertEquals(creds2.credential, "fakekey2");
+        } finally {
+            System.clearProperty("aws.accessKeyId");
+            System.clearProperty("aws.secretKey");
+        }
+    }
+
+    /**
+     * Confirm that both property options are available for GCS.
+     */
+    @Test
+    public final void gcsKeysTest() {
+        Map<String, String> map = new HashMap<>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY,
+                JCloudBlobStoreProvider.GOOGLE_CLOUD_STORAGE.getDriver());
+        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
+        List<String> keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_BUCKET);
+        assertEquals(keys.get(0), BC_GCS_BUCKET);
+        assertEquals(keys.get(1), "managedLedgerOffloadBucket");
+
+        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_REGION);
+        assertEquals(keys.get(0), BC_GCS_REGION);
+        assertEquals(keys.get(1), "managedLedgerOffloadRegion");
+
+        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_MAX_BLOCK_SIZE);
+        assertEquals(keys.get(0), BC_GCS_MAX_BLOCK_SIZE);
+        assertEquals(keys.get(1), "managedLedgerOffloadMaxBlockSizeInBytes");
+
+        keys = config.getKeys(TieredStorageConfiguration.METADATA_FIELD_READ_BUFFER_SIZE);
+        assertEquals(keys.get(0), BC_GCS_READ_BUFFER_SIZE);
+        assertEquals(keys.get(1), "managedLedgerOffloadReadBufferSizeInBytes");
+    }
+
+    /**
+     * Confirm that we can configure GCS using the new properties.
+     */
+    @Test
+    public final void gcsPropertiesTest() {
+        Map<String, String> map = new HashMap<>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY,
+                JCloudBlobStoreProvider.GOOGLE_CLOUD_STORAGE.getDriver());
+        map.put("managedLedgerOffloadRegion", "us-east-1");
+        map.put("managedLedgerOffloadBucket", "test bucket");
+        map.put("managedLedgerOffloadMaxBlockSizeInBytes", "1");
+        map.put("managedLedgerOffloadReadBufferSizeInBytes", "500");
+        map.put("managedLedgerOffloadServiceEndpoint", "http://some-url:9093");
+        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
+
+        assertEquals(config.getRegion(), "us-east-1");
+        assertEquals(config.getBucket(), "test bucket");
+        assertEquals(config.getMaxBlockSizeInBytes(), Integer.valueOf(1));
+        assertEquals(config.getReadBufferSizeInBytes(), Integer.valueOf(500));
+    }
+
+    /**
+     * Confirm that we can configure GCS using the old properties.
+     */
+    @Test
+    public final void gcsBackwardCompatiblePropertiesTest() {
+        Map<String, String> map = new HashMap<>();
+        map.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY,
+                JCloudBlobStoreProvider.GOOGLE_CLOUD_STORAGE.getDriver());
+        map.put(BC_GCS_BUCKET, "test bucket");
+        map.put(BC_GCS_MAX_BLOCK_SIZE, "12");
+        map.put(BC_GCS_READ_BUFFER_SIZE, "500");
+        map.put(BC_GCS_REGION, "test region");
+        TieredStorageConfiguration config = new TieredStorageConfiguration(map);
+
+        assertEquals(config.getRegion(), "test region");
+        assertEquals(config.getBucket(), "test bucket");
+        assertEquals(config.getMaxBlockSizeInBytes(), Integer.valueOf(12));
+        assertEquals(config.getReadBufferSizeInBytes(), Integer.valueOf(500));
+    }
+
+    @Test
+    public void overridePropertiesTest() {
+        Map<String, String> map = new HashMap<>();
+        map.put("s3ManagedLedgerOffloadServiceEndpoint", "http://localhost");
+        map.put("s3ManagedLedgerOffloadRegion", "my-region");
+        System.setProperty("jclouds.SystemPropertyA", "A");
+        System.setProperty("jclouds.region", "jclouds-region");
+        try {
+            TieredStorageConfiguration config = new TieredStorageConfiguration(map);
+            Properties properties = config.getOverrides();
+            assertEquals(properties.get("jclouds.region"), "jclouds-region");
+            assertEquals(config.getServiceEndpoint(), "http://localhost");
+            assertEquals(properties.get("jclouds.SystemPropertyA"), "A");
+        } finally {
+            System.clearProperty("jclouds.SystemPropertyA");
+            System.clearProperty("jclouds.region");
+        }
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TransientBlobStoreFactoryTest.java b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TransientBlobStoreFactoryTest.java
index 9792aae80d..069cb7398a 100644
--- a/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TransientBlobStoreFactoryTest.java
+++ b/tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TransientBlobStoreFactoryTest.java
@@ -1,59 +1,59 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.bookkeeper.mledger.offload.jcloud.provider;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import org.apache.commons.io.IOUtils;
-import org.jclouds.blobstore.domain.Blob;
-import org.testng.Assert;
-import org.testng.annotations.Test;
-
-public class TransientBlobStoreFactoryTest extends AbstractJCloudBlobStoreFactoryTest {
-
-    private static final String TEST_CONTAINER_NAME = "test-container";
-    private static final String TEST_BLOB_NAME = "test-blob";
-
-    @Test
-    public final void smallBlobTest() throws IOException {
-        sendAndVerifyBlob(TEST_CONTAINER_NAME, TEST_BLOB_NAME, PAYLOAD);
-        deleteBlobAndVerify(TEST_CONTAINER_NAME, "test-blob");
-        deleteContainerAndVerify(TEST_CONTAINER_NAME);
-    }
-
-    @Test
-    public final void multipartUploadTest() throws IOException {
-        String[] lines = new String[] { "Line 1", "Line 2", "Line 3"};
-        sendMultipartPayload(TEST_CONTAINER_NAME, TEST_BLOB_NAME, lines);
-        Assert.assertTrue(blobStore.blobExists(TEST_CONTAINER_NAME, TEST_BLOB_NAME));
-
-        Blob retrieved = blobStore.getBlob(TEST_CONTAINER_NAME, TEST_BLOB_NAME);
-        Assert.assertNotNull(retrieved);
-        Assert.assertEquals(new String(IOUtils.toByteArray(retrieved.getPayload().openStream())), "Line 1Line 2Line 3");
-    }
-
-    @Override
-    protected Map<String, String> getConfig() {
-        Map<String, String> metadata = new HashMap<String, String>();
-        metadata.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, "transient");
-        metadata.put("managedLedgerOffloadBucket", TEST_CONTAINER_NAME);
-        return metadata;
-    }
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.bookkeeper.mledger.offload.jcloud.provider;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import org.apache.commons.io.IOUtils;
+import org.jclouds.blobstore.domain.Blob;
+import org.testng.Assert;
+import org.testng.annotations.Test;
+
+public class TransientBlobStoreFactoryTest extends AbstractJCloudBlobStoreFactoryTest {
+
+    private static final String TEST_CONTAINER_NAME = "test-container";
+    private static final String TEST_BLOB_NAME = "test-blob";
+
+    @Test
+    public final void smallBlobTest() throws IOException {
+        sendAndVerifyBlob(TEST_CONTAINER_NAME, TEST_BLOB_NAME, PAYLOAD);
+        deleteBlobAndVerify(TEST_CONTAINER_NAME, "test-blob");
+        deleteContainerAndVerify(TEST_CONTAINER_NAME);
+    }
+
+    @Test
+    public final void multipartUploadTest() throws IOException {
+        String[] lines = new String[] { "Line 1", "Line 2", "Line 3"};
+        sendMultipartPayload(TEST_CONTAINER_NAME, TEST_BLOB_NAME, lines);
+        Assert.assertTrue(blobStore.blobExists(TEST_CONTAINER_NAME, TEST_BLOB_NAME));
+
+        Blob retrieved = blobStore.getBlob(TEST_CONTAINER_NAME, TEST_BLOB_NAME);
+        Assert.assertNotNull(retrieved);
+        Assert.assertEquals(new String(IOUtils.toByteArray(retrieved.getPayload().openStream())), "Line 1Line 2Line 3");
+    }
+
+    @Override
+    protected Map<String, String> getConfig() {
+        Map<String, String> metadata = new HashMap<String, String>();
+        metadata.put(TieredStorageConfiguration.BLOB_STORE_PROVIDER_KEY, "transient");
+        metadata.put("managedLedgerOffloadBucket", TEST_CONTAINER_NAME);
+        return metadata;
+    }
+}
diff --git a/tiered-storage/jcloud/src/test/resources/log4j2-test.yml b/tiered-storage/jcloud/src/test/resources/log4j2-test.yml
index f5ee5c9a53..01f3cffb50 100644
--- a/tiered-storage/jcloud/src/test/resources/log4j2-test.yml
+++ b/tiered-storage/jcloud/src/test/resources/log4j2-test.yml
@@ -1,61 +1,61 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-#
-
-
-
-Configuration:
-  status: warn
-  name: YAMLConfigTest
-  properties:
-    property:
-      name: filename
-      value: target/test-yaml.log
-  thresholdFilter:
-    level: debug
-  appenders:
-    Console:
-      name: STDOUT
-      target: SYSTEM_OUT
-      PatternLayout:
-        Pattern: "%d{ISO8601_OFFSET_DATE_TIME_HHMM} [%t] %-5level %logger{36} - %msg%n"
-    File:
-      name: File
-      fileName: ${filename}
-      PatternLayout:
-        Pattern: "%d %p %c{1.} [%t] %m%n"
-      Filters:
-        ThresholdFilter:
-          level: error
-
-  Loggers:
-    logger:
-      - name: org.apache.bookkeeper.client.PulsarMockReadHandle
-        level: info
-        additivity: false
-        AppenderRef:
-          ref: STDOUT
-      - name: org.apache.logging.log4j.test2
-        level: debug
-        additivity: false
-        AppenderRef:
-          ref: File
-    Root:
-      level: debug
-      AppenderRef:
-        ref: STDOUT
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+
+
+Configuration:
+  status: warn
+  name: YAMLConfigTest
+  properties:
+    property:
+      name: filename
+      value: target/test-yaml.log
+  thresholdFilter:
+    level: debug
+  appenders:
+    Console:
+      name: STDOUT
+      target: SYSTEM_OUT
+      PatternLayout:
+        Pattern: "%d{ISO8601_OFFSET_DATE_TIME_HHMM} [%t] %-5level %logger{36} - %msg%n"
+    File:
+      name: File
+      fileName: ${filename}
+      PatternLayout:
+        Pattern: "%d %p %c{1.} [%t] %m%n"
+      Filters:
+        ThresholdFilter:
+          level: error
+
+  Loggers:
+    logger:
+      - name: org.apache.bookkeeper.client.PulsarMockReadHandle
+        level: info
+        additivity: false
+        AppenderRef:
+          ref: STDOUT
+      - name: org.apache.logging.log4j.test2
+        level: debug
+        additivity: false
+        AppenderRef:
+          ref: File
+    Root:
+      level: debug
+      AppenderRef:
+        ref: STDOUT
diff --git a/tiered-storage/pom.xml b/tiered-storage/pom.xml
index 9928a451e0..d93741ec2a 100644
--- a/tiered-storage/pom.xml
+++ b/tiered-storage/pom.xml
@@ -1,54 +1,54 @@
-<!--
-
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
-  <modelVersion>4.0.0</modelVersion>
-  <packaging>pom</packaging>
-  <parent>
-    <groupId>org.apache.pulsar</groupId>
-    <artifactId>pulsar</artifactId>
-    <version>4.1.2</version>
-  </parent>
-
-  <artifactId>tiered-storage-parent</artifactId>
-  <name>Apache Pulsar :: Tiered Storage :: Parent</name>
-
-  <properties>
-    <pulsar.jclouds.shaded.version>${project.version}</pulsar.jclouds.shaded.version>
-  </properties>
-
-  <modules>
-    <module>jcloud</module>
-    <module>file-system</module>
-  </modules>
-
-  <build>
-    <plugins>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-deploy-plugin</artifactId>
-        <configuration>
-          <skip>true</skip>
-        </configuration>
-      </plugin>
-    </plugins>
-  </build>
-</project>
+<!--
+
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+  <modelVersion>4.0.0</modelVersion>
+  <packaging>pom</packaging>
+  <parent>
+    <groupId>org.apache.pulsar</groupId>
+    <artifactId>pulsar</artifactId>
+    <version>4.1.2</version>
+  </parent>
+
+  <artifactId>tiered-storage-parent</artifactId>
+  <name>Apache Pulsar :: Tiered Storage :: Parent</name>
+
+  <properties>
+    <pulsar.jclouds.shaded.version>${project.version}</pulsar.jclouds.shaded.version>
+  </properties>
+
+  <modules>
+    <module>jcloud</module>
+    <module>file-system</module>
+  </modules>
+
+  <build>
+    <plugins>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-deploy-plugin</artifactId>
+        <configuration>
+          <skip>true</skip>
+        </configuration>
+      </plugin>
+    </plugins>
+  </build>
+</project>
