From: HelgiSaga <bglegb@gmail.com>
Date: Tue, 21 Jan 2026 12:30:00 +0300
Subject: [PATCH] debian: Add PIP documentation updates

Description: Add PIP documentation updates
Author: HelgiSaga <bglegb@gmail.com>
Forwarded: no
Last-Update: 2026-01-21

diff --git a/pip/pip-434.md b/pip/pip-434.md new file mode 100644 index 0000000000..6f6ee87298 --- /dev/null +++ b/pip/pip-434.md @@ -0,0 +1,95 @@ +# PIP-434: Expose Netty channel configuration WRITE_BUFFER_WATER_MARK to pulsar conf and pause receive requests when channel is unwritable + +# Background knowledge & Motivation + +As we discussed along the discussion: https://lists.apache.org/thread/6jfs02ovt13mnhn441txqy5m6knw6rr8 + +> Problem Statement: +> We've encountered a critical issue in our Apache Pulsar clusters where brokers experience Out-Of-Memory (OOM) errors and continuous restarts under specific load patterns. This occurs when Netty channel write buffers become full, leading to a buildup of unacknowledged responses in the broker's memory. + +> Background: +> Our clusters are configured with numerous namespaces, each containing approximately 8,000 to 10,000 topics. Our consumer applications are quite large, with each consumer using a regular expression (regex) pattern to subscribe to all topics within a namespace. + +> The problem manifests particularly during consumer application restarts. When a consumer restarts, it issues a getTopicsOfNamespace request. Due to the sheer number of topics, the response size is extremely large. This massive response overwhelms the socket output buffer, causing it to fill up rapidly. Consequently, the broker's responses get backlogged in memory, eventually leading to the broker's OOM and subsequent restart loop. + +> Solution we got: +> - Expose Netty channel configuration WRITE_BUFFER_WATER_MARK to pulsar conf +> - Stops receive requests continuously once the Netty channel is unwritable, users can use the new config to control the threshold that limits the max bytes that are pending write. + +# Goals + +## In Scope +- Expose Netty channel configuration WRITE_BUFFER_WATER_MARK to pulsar conf +- Stops receive requests continuously once the Netty channel is unwritable, users can use the new config to control the threshold that limits the max bytes that are pending write. + +## Out of Scope + +- This proposal is not in order to add a broker level memory limitation, it only focuses on addressing the OOM caused by the accumulation of a large number of responses in memory due to the channel granularity being unwritable.  + +# Detailed Design + +### Configuration + +```shell +# It relates to configuration "WriteBufferHighWaterMark" of Netty Channel Config. If the number of bytes queued in the write buffer exceeds this value, channel writable state will start to return "false". +pulsarChannelWriteBufferHighWaterMark=64k +# It relates to configuration "WriteBufferLowWaterMark" of Netty Channel Config. If the number of bytes queued in the write buffer is smaller than this value, channel writable state will start to return "true". +pulsarChannelWriteBufferLowWaterMark=32k +# Once the writer buffer is full, the channel stops dealing with new requests until it changes to writable +pulsarChannelPauseReceivingRequestsIfUnwritable=false +After the connection is recovered from an unreadable state, the channel will be rate-limited for a period of time to avoid overwhelming due to the backlog of requests. This parameter defines how many" requests should be allowed in the rate limiting period. +pulsarChannelRateLimitingRateAfterResumeFromUnreadable=1000 +After the connection is recovered from an unreadable state, the channel will be rate-limited for a period of time to avoid overwhelming due to the backlog of requests. This parameter defines how long the rate limiting should last, in seconds. Once the bytes that are waiting to be sent out reach the pulsarChannelWriteBufferHighWaterMark\"ÿ-Ì the timer will be reset. +pulsarChannelRateLimitingSecondsAfterResumeFromUnreadable=5 +``` + +### How it works +With the settings `pulsarChannelPauseReceivingRequestsIfUnwritable=false`, the behaviour is exactly the same as the previous. + +After setting `pulsarChannelPauseReceivingRequestsIfUnwritable` to `true`, the channel state will be changed as follows. +- Netty sets `channel.writable` to `false` when there is too much data that is waiting to be sent out(the size of the data cached in `ChannelOutboundBuffer` is larger than `{pulsarChannelWriteBufferHighWaterMark}`) +  - Netty will trigger an event `channelWritabilityChanged` +- Stops receiving requests that come into the channel, which relies on the attribute `autoRead` of the `Netty channel`. +- Netty sets `channel.writable` to `true` once the size of the data that is waiting to be sent out is less than `{pulsarChannelWriteBufferLowWaterMark}` +- Starts to receive requests(sets `channel.autoRead` to `true`). +  - Note: relies on `ServerCnxThrottleTracker`, which will track the "throttle count". When a throttling condition is present, the throttle count is increased and when it's no more present, the count is decreased. The autoread should be switched to false when the counter value goes from 0 to 1, and only when it goes back from 1 to 0 should it be set to true again. The autoread flag is no longer controlled directly from the rate limiters. Rate limiters are only responsible for their part, and it's ServerCnxThrottleTracker that decides when the autoread flag is toggled.  See more details [pip-322](https://github.com/apache/pulsar/blob/master/pip/pip-322.md) +  - To avoid handling a huge request in the backlog instantly, Pulsar will start a timed rate-limiter, which limits the rate of handling the request backlog("pulsarChannelRateLimitingRateAfterResumeFromUnreadable" requests per second). +  - After "{pulsarChannelRateLimitingSecondsAfterResumeFromUnreadable}" seconds, the rate-limiter will be removed automatically. Once the bytes that are waiting to be sent out reach the pulsarChannelWriteBufferHighWaterMark\"ÿ-Ì the timer will be reset. + +### CLI + +### Metrics +| Name                                                 | Description                                                                                 | Attributes   | Units| +|------------------------------------------------------|---------------------------------------------------------------------------------------------|--------------| --- | +| `pulsar_server_channel_write_buf_memory_used_bytes` | Counter. The memory amount that is occupied by netty write buffers                      | cluster | - | + + +# Monitoring + + +# Security Considerations +Nothing. + +# Backward & Forward Compatibility + +## Upgrade +Nothing. + +## Downgrade / Rollback +Nothing. + +## Pulsar Geo-Replication Upgrade & Downgrade/Rollback Considerations +Nothing. + +# Alternatives +Nothing. + +# General Notes + +# Links + +<!-- +Updated afterwards +--> +* Mailing List discussion thread: https://lists.apache.org/thread/hnbm9q3yvyf2wcbdggxmjzhr9boorqkn +* Mailing List voting thread: https://lists.apache.org/thread/vpvtf4jnbbrhsy9y5fg00mpz9qhb0cp5 diff --git a/pip/pip-437.md b/pip/pip-437.md new file mode 100644 index 0000000000..ae85360ccd --- /dev/null +++ b/pip/pip-437.md @@ -0,0 +1,142 @@ +# PIP-437: Granular and Fixed-Delay Policies for Message Delivery + +# Background +Pulsar's delayed delivery feature allows producers to schedule messages to be delivered at a future time. To provide administrative control and prevent abuse, **PIP-315** introduced a `maxDeliveryDelayInMillis` policy. +While this provides an important safeguard by setting an *upper bound*, it does not address all administrative needs. There is currently no way to enforce a *specific, non-overridable* delay for all messages on a topic. + +# Motivation + +### The Limitation: +Pulsar's delayed message delivery relies on tracking individual messages until their delivery time. This tracking state is persisted as part of the subscription's cursor metadata, +which records gaps or "holes" for unacknowledged messages. The Pulsar Managed Ledger has a hard limit on the number of disjoint unacknowledged ranges it can persist for a cursor,  +configured by `managedLedgerMaxUnackedRangesToPersist` (defaulting to 10,000). +A large volume of messages with widely varying delivery times can easily exhaust this capacity with each delayed message creating a separate unacknowledged hole.  + +### The Impact: Upon restart, the broker's only source of truth is the last persisted cursor position. +Once the `managedLedgerMaxUnackedRangesToPersist` limit is breached, the broker stops persisting the cursor's state and maintains it only in memory. This in-memory state is volatile and is completely lost if the broker restarts for any reason. +Upon restart, the broker's only source of truth is the last successfully persisted cursor position. This position does not account for the lost tracking information, forcing the broker to re-dispatch all messages that were being tracked in memory. +This results in significant and difficult-to-predict message duplication for downstream consumers. + +The lack of administrative controls on message delivery delays introduces critical risks to cluster stability and data integrity. +This proposal aims to provide granular control at the topic and namespace levels to add a new fixed delay delivery configuration:  + +1. **Prevent Message Duplication:**  By forcing all messages on a topic to have the exact same delay,it prevents the delayed message tracker from becoming overwhelmed and eliminates the risk of message duplication upon broker restart, leading to a more stable and predictable system. +2. **Enable High-Scale Scheduling:** When all messages in a topic have the same delay, the broker has significantly less state to manage. This allows topics to be used as highly efficient, high-scale schedulers. This pattern works exceptionally well with the `InMemoryDelayedDeliveryTracker` and its `delayedDeliveryFixedDelayDetectionLookahead` feature (introduced in #16609, #17907), as the broker can avoid building a large, complex index of individual delayed messages. +3. **Enforcing Compliance and Business Rules:** Certain workflows may require certain control that require the enforcement at the broker level +4. **Simplifying Producer Configuration:** By setting a fixed delay on the topic, the responsibility for managing the delay logic is shifted from the client to the broker, reducing the risk of client-side misconfiguration. + +# Goals +## In Scope +-   Introduce a new `fixed-delivery-delay` policy, configurable at the namespace and topic levels, to enforce a mandatory delivery delay. +-   Enhance the existing `/delayedDelivery` admin API endpoints and `pulsar-admin` commands to manage both the existing `maxDeliveryDelayInMillis` and the new `fixedDeliveryDelayInMillis` policies within the same policy group. +-   Ensure the `fixed-delivery-delay` policy takes precedence over any client-specified delay and the existing `max-delivery-delay` policy. + +# High Level Design +This proposal will enhance the existing `DelayedDeliveryPolicies` object to include a new `fixedDeliveryDelayInMillis` field. +This new policy will follow Pulsar's standard hierarchical model, allowing it to be set at the namespace level and overridden at the topic level. +The core logic will be implemented with the following precedence: + +1.  **If `fixed-delivery-delay` is set:** The broker will ignore any `deliverAt` time sent by the producer and will override it by calculating `publish_time + fixed_delay`. The `max-delivery-delay` policy is ignored. +2.  **If `fixed-delivery-delay` is NOT set, but `max-delivery-delay` is:** The broker will validate the producer's requested `deliverAt` time against the `max-delivery-delay` policy, rejecting the message if it exceeds the limit. +3.  **If neither policy is set:** The system behaves as it does today, honoring the client's requested `deliverAt` time, constrained only by the global broker-level setting. + +# Detailed Design +## Design & Implementation Details + +1.  **Data Model Changes**: +    *   Add a new `fixedDeliveryDelayInMillis` field to the `DelayedDeliveryPolicies.java` class. This class is already used for the existing `maxDeliveryDelayInMillis` policy. +    *   Update `HierarchyTopicPolicies.java` to resolve the effective `fixedDeliveryDelayInMillis` for a topic, respecting the topic-over-namespace hierarchy. + +2.  **Enforcement Logic**: +    *   The enforcement logic will be placed in `PersistentTopic.java` within the `publishMessage` and `publishTxnMessage` methods, right before the existing `isExceedMaximumDeliveryDelay` check. +    *   This logic will modify the `MessageMetadata` of the incoming message *before* it is passed to the managed ledger for persistence. + +## Public-facing Changes +### Public API +#### Topic-Level Policies +| Method   | Endpoint                                                                  | Description                                                                                                                              | +| :------- | :------------------------------------------------------------------------ |:-----------------------------------------------------------------------------------------------------------------------------------------| +| `POST`   | `/admin/v2/persistent/{tenant}/{namespace}/{topic}/delayedDelivery`       | Sets or updates the delayed delivery policies for the topic.To disable a policy, a field can be set to `0`.                              | +| `GET`    | `/admin/v2/persistent/{tenant}/{namespace}/{topic}/delayedDelivery`       | Gets the configured delayed delivery policies for the topic which optionally include the fixedDeliveryDelayInMillis if configured | + +#### Namespace-Level Policies +| Method   | Endpoint                                                          | Description                                                      | +| :------- | :---------------------------------------------------------------- |:-----------------------------------------------------------------| +| `POST`   | `/admin/v2/namespaces/{tenant}/{namespace}/delayedDelivery`       | Sets or updates the delayed delivery policies for the namespace. To disable a policy, a field can be set to `0`.| +| `GET`    | `/admin/v2/namespaces/{tenant}/{namespace}/delayedDelivery`       | Gets the configured delayed delivery policies for the namespace which optionally include the fixedDeliveryDelayInMillis if configured  | + +### Binary protocol + +### Configuration + +### CLI +*   The existing `set-delayed-delivery` command in `CmdTopicPolicies.java` and `CmdNamespaces.java` will be updated with a new optional parameter: `--fixed-delay` (or `-fd`). +*   The REST endpoints under `/delayedDelivery` will be updated to accept and return the new `fixedDeliveryDelayInMillis` field in their JSON payload. + +### Metrics + +To provide visibility into the enforcement of the new message delay policies,we will introduce a new counter metric for maxDeliveryDelayInMillis  +*   **Full Name**: `pulsar.broker.topic.messages.delayed.rejected` +*   **Description**: A counter for the total number of messages rejected because their delivery delay exceeded the configured maximum (`max-delivery-delay`). This helps administrators monitor when the policy is being enforced and identify misconfigured producers. +*   **Attributes (Labels)**: +    *   `pulsar.cluster`: The cluster where the broker is running. +    *   `pulsar.namespace`: The namespace of the topic. +    *   `pulsar.topic`: The specific topic where the message was rejected. +*   **Unit**: `{message}` (A standard unit for a count of messages). + +To provide visibility into the enforcement of the fix message delay policies, a new counter metric will be introduced for fixedDeliveryDelayInMillis +* **Full Name**: `pulsar.broker.topic.messages.fixed.delay.overridden` +    *   **Description**: A counter for the total number of messages where a client-specified `deliverAt` time was overridden by the topic's `fixed-delivery-delay` policy. This provides direct observability into how often the policy is being enforced against client-side settings. +    *   **Attributes (Labels)**: +        *   `pulsar.cluster`: The cluster where the broker is running. +        *   `pulsar.namespace`: The namespace of the topic. +        *   `pulsar.topic`: The specific topic where the message was overridden. +    *   **Unit**: `{message}`. + +# Monitoring +Administrators can use the new `pulsar.broker.topic.messages.delayed.rejected` metric to monitor the health and usage of the delayed delivery feature. +A sudden spike in this metric could indicate: +1.  A producer application has been deployed with a misconfiguration, attempting to schedule messages with an overly long delay. +2.  A recent change to the `max-delivery-delay` policy at the namespace or topic level is now affecting existing producers. + +A high or unexpected count for `pulsar.broker.topic.messages.fixed.delay.overridden` indicates that producer applications are sending messages with a `deliverAt` time to a topic that has a `fixed-delivery-delay` policy.  +While the policy is being enforced correctly, this metric helps operators identify clients that may be misconfigured or unaware of the topic's enforced behavior. + +# Security Considerations + +# Backward & Forward Compatibility + +## Upgrade + +<!-- +Specify the list of instructions, if there are such, needed to perform before/after upgrading to Pulsar version containing this feature. +--> + +## Downgrade / Rollback +Downgrading to a version without this feature is supported, but the new `fixed-delivery-delay` policy will no longer be enforced. +-   **Behavior Change**:  It will revert to the previous behavior of either honoring the client's `deliverAt` time or enforcing the `max-delivery-delay` policy. +-   **Clean Rollback**: Disable the `fixed-delivery-delay` policy on all relevant topics and namespaces *before* starting the rollback process. This can be done by setting the fixed delay value to `0` using the `pulsar-admin ... set-delayed-delivery` command. + +## Pulsar Geo-Replication Upgrade & Downgrade/Rollback Considerations +The system shall guarantee that changes to delayed delivery policies are applied atomically across all active brokers. +This ensures that the cluster does not operate in a mixed-policy state during either an upgrade or a downgrade procedure. +During a geo-replication upgrade, ensure that all clusters are upgraded before relying on the consistency of the new +delayed delivery policies. +If downgrading a geo-replicated cluster, remove the new topic-level configurations *before* downgrading to prevent +inconsistencies between clusters. + +# Alternatives + +<!-- +If there are alternatives that were already considered by the authors or, after the discussion, by the community, and were rejected, please list them here along with the reason why they were rejected. +--> + +# General Notes + +# Links + +<!-- +Updated afterwards +--> +* Mailing List discussion thread: https://lists.apache.org/thread/23t4zzbfjrm5r4pbzrofn3d85c0171yn +* Mailing List voting thread: https://lists.apache.org/thread/m8y5brc0o8fgcby1sop3hybjpm46xgrr diff --git a/pip/pip-442.md b/pip/pip-442.md new file mode 100644 index 0000000000..59a0cdebc0 --- /dev/null +++ b/pip/pip-442.md @@ -0,0 +1,739 @@ +# PIP-442: Add memory limits for CommandGetTopicsOfNamespace and CommandWatchTopicList on Broker and Proxy + +## Background Knowledge + +Apache Pulsar brokers provide commands for clients to discover topics within a namespace and watch for topic updates. +These commands are critical for client operations but currently lack memory limits and flow control mechanisms, +creating potential memory and stability issues at scale. + +### Existing Broker Memory Management + +Pulsar brokers already implement comprehensive memory management for most operations through several key configurations: + +**Message Publishing Memory Limits:** +- `maxMessagePublishBufferSizeInMB` (default: 50% direct memory): Limits memory used for buffering messages during publishing, providing backpressure when producers exceed broker capacity + +**Managed Ledger Memory Limits:** +- `managedLedgerMaxReadsInFlightSizeInMB` (default: 0, disabled): Controls memory allocation for concurrent read operations from BookKeeper, preventing excessive memory usage during high read loads. This limit extends to cover buffers that were read from BookKeeper and are waiting in channel outbound buffers to be written to client sockets. +- `managedLedgerCacheSizeMB` (default: 20% of direct memory): Limits cache memory for recently read ledger entries, ensuring predictable memory usage for read caching. This limit extends to cover buffers that were read from the cache and are waiting in channel outbound buffers to be written to client sockets. + +**Additional Memory Controls:** +- `maxConcurrentLookupRequest` (default: 50000): Limits concurrent topic lookup requests. The unit of this limit is the number of requests; it is not expressed in memory size. +- `maxConcurrentTopicLoadRequest` (default: 5000): Controls concurrent topic loading operations. The unit of this limit is the number of requests; it is not expressed in memory size. + +These existing limits effectively bound memory usage for message handling, storage operations, and most broker functions. However, there is a significant gap in memory management for topic discovery operations. + +### The Memory Management Gap + +Major unbounded memory allocation in Pulsar brokers occurs during topic listing operations: + +- `CommandGetTopicsOfNamespace` / `CommandGetTopicsOfNamespaceResponse` +- `CommandWatchTopicList` / `CommandWatchTopicListSuccess` & `CommandWatchTopicUpdate` + +These operations can allocate arbitrary amounts of memory based on namespace size, with no limits or backpressure mechanisms. + +### Key Components + +**Topic Discovery Commands:** +- **`CommandGetTopicsOfNamespace`**: Binary protocol command that retrieves all topics within a namespace +- **`CommandGetTopicsOfNamespaceResponse`**: Response containing the list of topics +- **`CommandWatchTopicList`**: Command to establish a watch for topic list changes +- **`CommandWatchTopicListSuccess`**: Initial response confirming watch establishment and returning current topic list +- **`CommandWatchTopicUpdate`**: Notifications sent when topics are added or removed + +**Current Implementation Flow:** + +The `getTopicsOfNamespace` request follows this path: + +1. **Client Request**: Sends `CommandGetTopicsOfNamespace` via binary protocol +2. **Request Handling**: +    - Broker: `ServerCnx.handleGetTopicsOfNamespace()` +    - Proxy: `LookupProxyHandler.handleGetTopicsOfNamespace()` +3. **Topic Retrieval**: `NamespaceService.getListOfUserTopics()` orchestrates: +    - Fetches persistent topics from `TopicResources` +    - Retrieves non-persistent topics from local cache or peer clusters +    - Filters system topics using `TopicList.filterSystemTopic()` +    - Implements caching via `inProgressQueryUserTopics` to prevent duplicate queries +4. **Response Construction**: Packages results with hash calculation and filtering metadata +5. **Response Transmission**: Sends complete response back to client + +### The Unbounded Memory Problem + +Unlike other broker operations that have memory limits, topic listing operations create unbounded memory allocation scenarios: + +**Memory Allocation Points:** +1. **Topic List Assembly**: When retrieving topics from metadata store, the entire list is materialized in heap memory +2. **Response Object Creation**: The complete topic list is serialized into a response object +3. **Network Buffers**: Netty allocates direct memory for the serialized response +4. **Proxy Buffering**: Proxy deserializes broker response then re-serializes for client + +**Scale Impact:** +- Namespace with 10,000 topics +× 100 bytes average topic name = ~1MB per response +- With 1000 concurrent requests: ~1GB memory pressure + +## Motivation + +The lack of memory limits for topic listing commands creates the final significant gap in Pulsar's otherwise comprehensive memory management system: + +1. **Memory Management Consistency**: While all other broker operations have memory limits and backpressure mechanisms, topic listing operations remain unbounded, creating an inconsistent and unpredictable memory profile. + +2. **Broker Memory Exhaustion Risk**: Large clusters can trigger OutOfMemoryErrors when multiple clients simultaneously request topic lists, causing broker crashes and service disruption despite other memory controls being in place. + +3. **Proxy Memory Exhaustion Risk**: Proxies are also impacted for `CommandGetTopicsOfNamespace` since the request is forwarded to a broker and the response is deserialized and reserialized without limits. + +4. **Unpredictable Resource Usage**: Operators cannot reliably predict or limit total broker or proxy memory consumption due to this unbounded allocation path, undermining capacity planning and resource management. + +5. **Performance Degradation**: Even without OOM, large topic list operations cause GC pressure and latency spikes affecting all broker operations, counteracting the stability provided by other memory limits. + +## Goals + +### In Scope + +- Close the memory management gap by implementing configurable memory limits for topic listing operations +- Add memory-based flow control and backpressure for both `CommandGetTopicsOfNamespace` and `CommandWatchTopicList` commands +- Provide separate limits for heap and direct memory consumption, consistent with existing broker memory management patterns +- Ensure fairness through queueing mechanisms when memory limits are reached +- Add comprehensive metrics for monitoring and alerting, similar to existing memory limit metrics +- Maintain backward compatibility with existing clients + +### Out of Scope + +- Pagination or streaming of topic lists (requires protocol changes) +- Compression of topic list responses (separate optimization) +- Changes to topic discovery semantics or filtering capabilities +- Memory limits for other broker commands (already covered by existing configurations) + +## High-Level Design + +The solution introduces an `AsyncDualMemoryLimiter` that acts as a memory-aware semaphore for topic listing operations, completing Pulsar's memory management framework: + +1. **Memory Tracking**: Before processing requests or sending responses, the system estimates memory requirements and acquires permits from the limiter. An initial permit is acquired with a fixed estimate (1KB), then updated to reflect the actual memory usage after the topic list is retrieved. + +2. **Dual Memory Pools**: Separate tracking for heap memory (topic list assembly) and direct memory (network buffers) with independent limits, since topic listing operations use both types of memory. + +3. **Asynchronous Backpressure**: When memory limits are reached, requests queue with configurable timeouts rather than failing immediately, providing graceful degradation similar to `managedLedgerMaxReadsInFlightSizeInMB` behavior. When the queue is completely full, requests are rejected. + +4. **Graceful Degradation**: The system continues processing within memory limits rather than crashing, with clear metrics indicating when memory-based throttling occurs. + +5. **Release Guarantees**: Memory permits are released after response transmission completes or on request failure, preventing memory leaks and ensuring accurate memory tracking. + +6. **Cancellation Support**: The implementation supports cancellation of permit requests when the client connection is closed, preventing unnecessary queueing and resource allocation. + +## Detailed Design + +### Design & Implementation Details + +#### AsyncSemaphore Interface + +This is an abstraction for a generic asynchronous semaphore. The memory limiter implementation uses this abstraction to implement separate limiters for heap and direct memory. + +```java +public interface AsyncSemaphore { +    /** +     * Acquire permits from the semaphore. +     * Returned future completes when permits are available. +     * It will complete exceptionally with AsyncSemaphore.PermitAcquireTimeoutException on timeout +     * and exceptionally with AsyncSemaphore.PermitAcquireQueueFullException when queue full +     * +     * @param permits     number of permits to acquire +     * @param isCancelled supplier that returns true if acquisition should be cancelled +     * @return CompletableFuture that completes with permit when available +     */ +    CompletableFuture<AsyncSemaphorePermit> acquire(long permits, BooleanSupplier isCancelled); + +    /** +     * Acquire or release permits for previously acquired permits by updating the permits. +     * Returns a future that completes when permits are available. +     * It will complete exceptionally with AsyncSemaphore.PermitAcquireTimeoutException on timeout +     * and exceptionally with AsyncSemaphore.PermitAcquireQueueFullException when queue full +     * +     * @param permit      previously acquired permit to update +     * @param newPermits  new number of permits to update to +     * @param isCancelled supplier that returns true if update should be cancelled +     * @return CompletableFuture that completes with permit when available +     */ +    CompletableFuture<AsyncSemaphorePermit> update(AsyncSemaphorePermit permit, long newPermits, +                                                   BooleanSupplier isCancelled); + +    /** +     * Release previously acquired permit. +     * Must be called to prevent permit leaks. +     * +     * @param permit permit to release +     */ +    void release(AsyncSemaphorePermit permit); +    /** +     * Get the number of available permits. +     */ +    long getAvailablePermits(); + +    /** +     * Get the number of acquired permits. +     */ +    long getAcquiredPermits(); + +    /** +     * Get the current size of queued requests. +     */ +    int getQueueSize(); +} +``` + +#### AsyncDualMemoryLimiter Interface + +This is an abstraction for an asynchronous memory semaphore that tracks separate limits for heap and direct memory. + +```java +public interface AsyncDualMemoryLimiter { +    enum LimitType { +        HEAP_MEMORY,    // For heap memory allocation +        DIRECT_MEMORY   // For direct memory allocation +    } + +    /** +     * Acquire permits for the specified memory size. +     * Returned future completes when memory permits are available. +     * It will complete exceptionally with AsyncSemaphore.PermitAcquireTimeoutException on timeout +     * and exceptionally with AsyncSemaphore.PermitAcquireQueueFullException when queue full +     * +     * @param memorySize  the size of memory to acquire permits for +     * @param limitType   the type of memory limit (HEAP_MEMORY or DIRECT_MEMORY) +     * @param isCancelled supplier that returns true if acquisition should be cancelled +     * @return CompletableFuture that completes with permit when available +     */ +    CompletableFuture<AsyncDualMemoryLimiterPermit> acquire(long memorySize, LimitType limitType, +                                                            BooleanSupplier isCancelled); + +    /** +     * Acquire or release permits for previously acquired permits by updating the requested memory size. +     * Returns a future that completes when permits are available. +     * It will complete exceptionally with AsyncSemaphore.PermitAcquireTimeoutException on timeout +     * and exceptionally with AsyncSemaphore.PermitAcquireQueueFullException when queue full +     * The provided permit is released when the permits are successfully acquired and the returned updated +     * permit replaces the old instance. +     * +     * @param permit        the previously acquired permit to update +     * @param newMemorySize the new memory size to update to +     * @param isCancelled   supplier that returns true if update should be cancelled +     * @return CompletableFuture that completes with permit when available +     */ +    CompletableFuture<AsyncDualMemoryLimiterPermit> update(AsyncDualMemoryLimiterPermit permit, long newMemorySize, +                                                           BooleanSupplier isCancelled); + +    /** +     * Release previously acquired permit. +     * Must be called to prevent memory permit leaks. +     * +     * @param permit the permit to release +     */ +    void release(AsyncDualMemoryLimiterPermit permit); +    /** +     * Execute the specified function with acquired permits and release the permits after the returned future completes. +     * @param memorySize memory size to acquire permits for +     * @param limitType memory limit type to acquire permits for +     * @param function function to execute with acquired permits +     * @return result of the function +     * @param <T> type of the CompletableFuture returned by the function +     */ +    default <T> CompletableFuture<T> withAcquiredPermits(long memorySize, LimitType limitType, +                                                         BooleanSupplier isCancelled, +                                                         Function<AsyncDualMemoryLimiterPermit, +                                                                 CompletableFuture<T>> function, +                                                         Function<Throwable, CompletableFuture<T>> +                                                                 permitAcquireErrorHandler) { +        return AsyncDualMemoryLimiterUtil.withPermitsFuture(acquire(memorySize, limitType, isCancelled), function, +                permitAcquireErrorHandler, this::release); +    } + +    /** +     * Executed the specified function with updated permits and release the permits after the returned future completes. +     * @param initialPermit initial permit to update +     * @param newMemorySize new memory size to update to +     * @param function function to execute with updated permits +     * @return result of the function +     * @param <T> type of the CompletableFuture returned by the function +     */ +    default <T> CompletableFuture<T> withUpdatedPermits(AsyncDualMemoryLimiterPermit initialPermit, long newMemorySize, +                                                        BooleanSupplier isCancelled, +                                                        Function<AsyncDualMemoryLimiterPermit, +                                                                CompletableFuture<T>> function, +                                                        Function<Throwable, CompletableFuture<T>> +                                                                permitAcquireErrorHandler) { +        return AsyncDualMemoryLimiterUtil.withPermitsFuture(update(initialPermit, newMemorySize, isCancelled), function, +                permitAcquireErrorHandler, this::release); +    } +} +``` + +#### AsyncDualMemoryLimiterUtil Helper + +A utility class provides helper methods for common patterns: + +```java +public class AsyncDualMemoryLimiterUtil { +    /** +     * Execute a function with acquired permits and ensure permits are released after completion. +     * This method handles the lifecycle of permits - acquisition, usage, and release, including error cases. +     * +     * @param permitsFuture             Future that will complete with the required permits +     * @param function                  Function to execute once permits are acquired that returns a CompletableFuture +     * @param permitAcquireErrorHandler Handler for permit acquisition errors that returns a CompletableFuture +     * @param releaser                  Consumer that handles releasing the permits +     * @param <T>                       The type of result returned by the function +     * @return CompletableFuture that completes with the result of the function execution +     */ +    public static <T> CompletableFuture<T> withPermitsFuture( +            CompletableFuture<AsyncDualMemoryLimiterPermit> +                    permitsFuture, +            Function<AsyncDualMemoryLimiterPermit, +                    CompletableFuture<T>> function, +            Function<Throwable, CompletableFuture<T>> +                    permitAcquireErrorHandler, +            Consumer<AsyncDualMemoryLimiterPermit> releaser) { +        // implementation omitted from PIP document +    } + +    /** +     * Acquires permits and writes the command as a response to the channel. +     * Releases the permits after the response has been written to the socket or if the write fails. +     * +     * @param ctx the channel handler context used for writing the response +     * @param dualMemoryLimiter the memory limiter used to acquire and release memory permits +     * @param isCancelled supplier that indicates if the permit acquisition should be cancelled +     * @param command the base command to serialize and write to the channel +     * @param permitAcquireErrorHandler handler for errors that occur during permit acquisition +     * @return a future that completes when the command has been written to the channel's outbound buffer +     */ +    public static CompletableFuture<Void> acquireDirectMemoryPermitsAndWriteAndFlush(ChannelHandlerContext ctx, +                                                                                     AsyncDualMemoryLimiter +                                                                                             dualMemoryLimiter, +                                                                                     BooleanSupplier isCancelled, +                                                                                     BaseCommand command, +                                                                                     Consumer<Throwable> +                                                                                             permitAcquireErrorHandler +    ) { +        // implementation omitted from PIP document +    } +} +``` + +#### Integration Points + +**1. Heap Memory Limiting (Post-Retrieval) - Broker** + +In `ServerCnx.handleGetTopicsOfNamespace`, the implementation uses the helper methods: + +```java +    private void internalHandleGetTopicsOfNamespace(String namespace, NamespaceName namespaceName, long requestId, +                                                    CommandGetTopicsOfNamespace.Mode mode, +                                                    Optional<String> topicsPattern, Optional<String> topicsHash, +                                                    Semaphore lookupSemaphore) { +    BooleanSupplier isPermitRequestCancelled = () -> !ctx().channel().isActive(); +    maxTopicListInFlightLimiter.withAcquiredPermits(INITIAL_TOPIC_LIST_HEAP_PERMITS_SIZE, +            AsyncDualMemoryLimiter.LimitType.HEAP_MEMORY, isPermitRequestCancelled, initialPermits -> { +                return getBrokerService().pulsar().getNamespaceService().getListOfUserTopics(namespaceName, mode) +                        .thenAccept(topics -> { +                            long actualSize = topics.stream().mapToInt(String::length).sum(); +                            maxTopicListInFlightLimiter.withUpdatedPermits(initialPermits, actualSize, +                                    isPermitRequestCancelled, permits -> { +                                        boolean filterTopics = false; +                                        // filter system topic +                                        List<String> filteredTopics = topics; + +                                        if (enableSubscriptionPatternEvaluation && topicsPattern.isPresent()) { +                                            if (topicsPattern.get().length() <= maxSubscriptionPatternLength) { +                                                filterTopics = true; +                                                filteredTopics = TopicList.filterTopics(filteredTopics, topicsPattern.get(), +                                                        topicsPatternImplementation); +                                            } else { +                                                log.info("[{}] Subscription pattern provided [{}] was longer than " +                                                                + "maximum {}.", remoteAddress, topicsPattern.get(), +                                                        maxSubscriptionPatternLength); +                                            } +                                        } +                                        String hash = TopicList.calculateHash(filteredTopics); +                                        boolean hashUnchanged = topicsHash.isPresent() && topicsHash.get().equals(hash); +                                        if (hashUnchanged) { +                                            filteredTopics = Collections.emptyList(); +                                        } +                                        if (log.isDebugEnabled()) { +                                            log.debug("[{}] Received CommandGetTopicsOfNamespace for namespace " +                                                            + "[//{}] by {}, size:{}", remoteAddress, namespace, +                                                    requestId, +                                                    topics.size()); +                                        } +                                        commandSender.sendGetTopicsOfNamespaceResponse(filteredTopics, hash, filterTopics, +                                                !hashUnchanged, requestId, ex -> { +                                                    log.warn("[{}] Failed to acquire direct memory permits for " +                                                            + "GetTopicsOfNamespace: {}", remoteAddress, ex.getMessage()); +                                                    commandSender.sendErrorResponse(requestId, ServerError.TooManyRequests, +                                                            "Cannot acquire permits for direct memory"); +                                                }); +                                        return CompletableFuture.completedFuture(null); +                                    }, t -> { +                                        log.warn("[{}] Failed to acquire heap memory permits for " +                                                + "GetTopicsOfNamespace: {}", remoteAddress, t.getMessage()); +                                        writeAndFlush(Commands.newError(requestId, ServerError.TooManyRequests, +                                                "Failed due to heap memory limit exceeded")); +                                        return CompletableFuture.completedFuture(null); +                                    }); +                        }).whenComplete((__, ___) -> { +                            lookupSemaphore.release(); +                        }).exceptionally(ex -> { +                            log.warn("[{}] Error GetTopicsOfNamespace for namespace [//{}] by {}", remoteAddress, +                                    namespace, requestId); +                            commandSender.sendErrorResponse(requestId, +                                    BrokerServiceException.getClientErrorCode(new ServerMetadataException(ex)), +                                    ex.getMessage()); +                            return null; +                        }); +            }, t -> { +                log.warn("[{}] Failed to acquire initial heap memory permits for GetTopicsOfNamespace: {}", +                        remoteAddress, t.getMessage()); +                writeAndFlush(Commands.newError(requestId, ServerError.TooManyRequests, +                        "Failed due to heap memory limit exceeded")); +                lookupSemaphore.release(); +                return CompletableFuture.completedFuture(null); +            }); +} +``` + +**2. Direct Memory Limiting (Pre-Serialization) - Broker** + +Modified `PulsarCommandSenderImpl`: + +```java +@Override +public void sendGetTopicsOfNamespaceResponse(List<String> topics, String topicsHash, +                                             boolean filtered, boolean changed, long requestId, +                                             Consumer<Throwable> permitAcquireErrorHandler) { +    BaseCommand command = Commands.newGetTopicsOfNamespaceResponseCommand(topics, topicsHash, +            filtered, changed, requestId); +    safeIntercept(command, cnx); +    acquireDirectMemoryPermitsAndWriteAndFlush(cnx.ctx(), maxTopicListInFlightLimiter, () -> !cnx.isActive(), +            command, permitAcquireErrorHandler); +} +``` + +The utility method implementation: + +```java +public static CompletableFuture<Void> acquireDirectMemoryPermitsAndWriteAndFlush(ChannelHandlerContext ctx, +                                                                                     AsyncDualMemoryLimiter +                                                                                             dualMemoryLimiter, +                                                                                     BooleanSupplier isCancelled, +                                                                                     BaseCommand command, +                                                                                     Consumer<Throwable> +                                                                                             permitAcquireErrorHandler +) { +    // Calculate serialized size before acquiring permits +    int serializedSize = command.getSerializedSize(); +    // Acquire permits +    return dualMemoryLimiter.acquire(serializedSize, AsyncDualMemoryLimiter.LimitType.DIRECT_MEMORY, isCancelled) +            .whenComplete((permits, t) -> { +                if (t != null) { +                    permitAcquireErrorHandler.accept(t); +                    return; +                } +                try { +                    // Serialize the response +                    ByteBuf outBuf = Commands.serializeWithPrecalculatedSerializedSize(command, serializedSize); +                    // Write the response +                    ctx.writeAndFlush(outBuf).addListener(future -> { +                        // Release permits after the response has been written to the socket +                        dualMemoryLimiter.release(permits); +                    }); +                } catch (Throwable e) { +                    // Return permits if an exception occurs before writeAndFlush is called successfully +                    dualMemoryLimiter.release(permits); +                    throw e; +                } +            }).thenApply(__ -> null); +} +``` + +**3. Watch Command Memory Control - Broker** + +Similar memory limiting patterns apply to watch commands in `TopicListService`: + +```java +public void sendTopicListUpdate(long watcherId, String topicsHash,  +                               List<String> deletedTopics, List<String> newTopics) { +    connection.getCommandSender().sendWatchTopicListUpdate( +        watcherId, newTopics, deletedTopics, topicsHash, +        t -> { +            log.warn("[{}] Cannot acquire direct memory tokens for sending topic list update", +                connection.toString(), t); +        }); +} +``` + +**4. Proxy Memory Control** + +On the Pulsar Proxy side in `LookupProxyHandler`: + +```java +private void internalPerformGetTopicsOfNamespace(long clientRequestId, String namespaceName, ClientCnx clientCnx, +                                                     ByteBuf command, long requestId) { +    BooleanSupplier isPermitRequestCancelled = () -> !proxyConnection.ctx().channel().isActive(); +    maxTopicListInFlightLimiter.withAcquiredPermits(INITIAL_TOPIC_LIST_HEAP_PERMITS_SIZE, +            AsyncDualMemoryLimiter.LimitType.HEAP_MEMORY, isPermitRequestCancelled, initialPermits -> { +                return clientCnx.newGetTopicsOfNamespace(command, requestId).whenComplete((r, t) -> { +                    if (t != null) { +                        log.warn("[{}] Failed to get TopicsOfNamespace {}: {}", clientAddress, namespaceName, +                                t.getMessage()); +                        writeAndFlush(Commands.newError(clientRequestId, getServerError(t), t.getMessage())); +                    } else { +                        long actualSize = +                                r.getNonPartitionedOrPartitionTopics().stream().mapToInt(String::length).sum(); +                        maxTopicListInFlightLimiter.withUpdatedPermits(initialPermits, actualSize, +                                isPermitRequestCancelled, permits -> { +                                    return handleWritingGetTopicsResponse(clientRequestId, r, isPermitRequestCancelled); +                                }, t2 -> { +                                    log.warn("[{}] Failed to acquire actual heap memory permits for " +                                            + "GetTopicsOfNamespace: {}", clientAddress, t2.getMessage()); +                                    writeAndFlush(Commands.newError(clientRequestId, ServerError.TooManyRequests, +                                            "Failed due to heap memory limit exceeded")); + +                                    return CompletableFuture.completedFuture(null); +                                }); +                    } +                }).thenApply(__ -> null); +            }, t -> { +                log.warn("[{}] Failed to acquire initial heap memory permits for GetTopicsOfNamespace: {}", +                        clientAddress, t.getMessage()); +                writeAndFlush(Commands.newError(clientRequestId, ServerError.TooManyRequests, +                        "Failed due to heap memory limit exceeded")); + +                return CompletableFuture.completedFuture(null); +            }).exceptionally(ex -> { +        writeAndFlush(Commands.newError(clientRequestId, getServerError(ex), ex.getMessage())); +        return null; +    }); +} + +private CompletableFuture<Void> handleWritingGetTopicsResponse(long clientRequestId, GetTopicsResult r, +                                                               BooleanSupplier isCancelled) { +    BaseCommand responseCommand = Commands.newGetTopicsOfNamespaceResponseCommand( +            r.getNonPartitionedOrPartitionTopics(), r.getTopicsHash(), r.isFiltered(), +            r.isChanged(), clientRequestId); +    return acquireDirectMemoryPermitsAndWriteAndFlush(proxyConnection.ctx(), maxTopicListInFlightLimiter, +            isCancelled, responseCommand, t -> { +                log.warn("[{}] Failed to acquire actual direct memory permits for GetTopicsOfNamespace: {}", +                        clientAddress, t.getMessage()); +                writeAndFlush(Commands.newError(clientRequestId, ServerError.TooManyRequests, +                        "Failed due to direct memory limit exceeded")); +            }); +} +``` + +#### TopicListMemoryLimiter - Metrics Integration + +The `TopicListMemoryLimiter` class extends `AsyncDualMemoryLimiterImpl` and adds Prometheus and OpenTelemetry metrics: + +```java +public class TopicListMemoryLimiter extends AsyncDualMemoryLimiterImpl { +    private final CollectorRegistry collectorRegistry; +    private final Gauge heapMemoryUsedBytes; +    private final Gauge heapMemoryLimitBytes; +    // ... other Prometheus metrics +    private final ObservableDoubleGauge otelHeapMemoryUsedGauge; +    // ... other OpenTelemetry metrics +     +    public TopicListMemoryLimiter(CollectorRegistry collectorRegistry,  +                                  String prometheusPrefix, +                                  Meter openTelemetryMeter, +                                  long maxHeapMemory, int maxHeapQueueSize, +                                  long heapTimeoutMillis, long maxDirectMemory,  +                                  int maxDirectQueueSize, long directTimeoutMillis) { +        super(maxHeapMemory, maxHeapQueueSize, heapTimeoutMillis,  +              maxDirectMemory, maxDirectQueueSize, directTimeoutMillis); +         +        // Initialize Prometheus metrics +        this.heapMemoryUsedBytes = Gauge.build( +            prometheusPrefix + "topic_list_heap_memory_used_bytes", +            "Current heap memory used by topic listings") +            .create() +            .setChild(new Gauge.Child() { +                @Override +                public double get() { +                    return getLimiter(LimitType.HEAP_MEMORY).getAcquiredPermits(); +                } +            }) +            .register(collectorRegistry); +         +        // Initialize OpenTelemetry metrics +        this.otelHeapMemoryUsedGauge = openTelemetryMeter +            .gaugeBuilder("topic.list.heap.memory.used") +            .setUnit("By") +            .setDescription("Current heap memory used by topic listings") +            .buildWithCallback(measurement -> { +                measurement.record(getLimiter(LimitType.HEAP_MEMORY).getAcquiredPermits()); +            }); +         +        // ... initialize other metrics +    } +     +    @Override +    protected void recordHeapWaitTime(long waitTimeNanos) { +        if (waitTimeNanos == Long.MAX_VALUE) { +            heapTimeoutTotal.inc(); +            otelHeapTimeoutTotal.add(1); +        } else { +            heapWaitTimeMs.observe(TimeUnit.NANOSECONDS.toMillis(waitTimeNanos)); +            otelHeapWaitTime.record(waitTimeNanos / 1_000_000_000.0d); +        } +    } +     +    @Override +    protected void recordDirectWaitTime(long waitTimeNanos) { +        // Similar implementation for direct memory +    } +} +``` + +### Public-facing Changes + +#### Configuration + +**broker.conf**/**proxy.conf** additions to complete the memory management configuration set: + +```properties +# Maximum heap memory for inflight topic list operations (MB) +# Default: 100 MB (supports ~1M topic names assuming 100 bytes each) +maxTopicListInFlightHeapMemSizeMB=100 + +# Maximum direct memory for inflight topic list responses (MB)   +# Default: 100 MB (network buffers for serialized responses) +maxTopicListInFlightDirectMemSizeMB=100 + +# Timeout for acquiring heap memory permits (milliseconds) +# Default: 25000 (25 seconds) +maxTopicListInFlightHeapMemSizePermitsAcquireTimeoutMillis=25000 + +# Maximum queue size for heap memory permit requests +# Default: 10000 (prevent unbounded queueing) +maxTopicListInFlightHeapMemSizePermitsAcquireQueueSize=10000 + +# Timeout for acquiring direct memory permits (milliseconds) +# Default: 25000 (25 seconds)   +maxTopicListInFlightDirectMemSizePermitsAcquireTimeoutMillis=25000 + +# Maximum queue size for direct memory permit requests +# Default: 10000 (prevent unbounded queueing) +maxTopicListInFlightDirectMemSizePermitsAcquireQueueSize=10000 +``` + +#### Metrics + +New metrics under `pulsar_broker_topic_list_`/`pulsar_proxy_topic_list_` prefix, complementing existing memory metrics: + +| Metric Name | Type | Description | Labels | +|------------|------|-------------|--------| +| `heap_memory_used_bytes` | Gauge | Current heap memory used by topic listings | `cluster` | +| `heap_memory_limit_bytes` | Gauge | Configured heap memory limit | `cluster` | +| `direct_memory_used_bytes` | Gauge | Current direct memory used by topic listings | `cluster` | +| `direct_memory_limit_bytes` | Gauge | Configured direct memory limit | `cluster` | +| `heap_queue_size` | Gauge | Current heap memory limiter queue size | `cluster` | +| `heap_queue_max_size` | Gauge | Maximum heap memory limiter queue size | `cluster` | +| `direct_queue_size` | Gauge | Current direct memory limiter queue size | `cluster` | +| `direct_queue_max_size` | Gauge | Maximum direct memory limiter queue size | `cluster` | +| `heap_wait_time_ms` | Summary | Wait time for heap memory permits (quantiles: 0.5, 0.95, 0.99, 1.0) | `cluster` | +| `direct_wait_time_ms` | Summary | Wait time for direct memory permits (quantiles: 0.5, 0.95, 0.99, 1.0) | `cluster` | +| `heap_timeout_total` | Counter | Total heap memory permit timeouts | `cluster` | +| `direct_timeout_total` | Counter | Total direct memory permit timeouts | `cluster` | + +OpenTelemetry equivalents are also provided with similar naming under the `topic.list.*` namespace. + +#### Public API + +No changes to REST API. + +#### Binary Protocol + +No protocol changes. Existing commands continue to work with added server-side memory limits and backpressure. + +## Monitoring + +Operators should monitor the following metrics alongside existing memory management metrics and set up alerts: + +1. **Memory Utilization Alert**: +    - Alert when `heap_memory_used_bytes / heap_memory_limit_bytes > 0.8` +    - Indicates the need to increase limits or investigate namespace growth + +2. **Queue Saturation Alert**: +    - Alert when `heap_queue_size / heap_queue_max_size > 0.9` +    - Indicates sustained memory pressure requiring capacity adjustment + +3. **Timeout Rate Alert**: +    - Alert when `rate(heap_timeout_total[5m]) > 1` +    - Indicates clients experiencing failures due to memory-based flow control + +4. **P99 Wait Time Alert**: +    - Alert when `heap_wait_time_ms{quantile="0.99"} > 10000` +    - Indicates degraded client experience due to memory pressure + +These alerts should be configured alongside existing memory alerts for `managedLedgerCacheSizeMB`, `maxMessagePublishBufferSizeInMB`, and other memory limits to provide comprehensive memory management visibility. + +## Security Considerations + +The memory limiting mechanism introduces new denial-of-service protections: + +1. **Resource Exhaustion Protection**: The limits prevent bad clients from triggering OOM by requesting large topic lists repeatedly, completing the broker's defense against memory-based attacks. + +2. **Fair Queueing**: The queue size limits prevent bad clients from monopolizing memory permits and blocking legitimate requests. + +3. **Cancellation Support**: The implementation includes cancellation support to prevent resource waste when clients disconnect, preventing malicious clients from queueing many requests and then disconnecting. + +4. **Multi-tenancy Isolation**: Consider per-tenant memory limits in future iterations to prevent one tenant from consuming all available topic listing memory permits, similar to how other memory limits could benefit from tenant isolation. + +## Backward & Forward Compatibility + +### Upgrade + +1. The feature is enabled by default with reasonable limits (100MB each for heap and direct memory) +2. After upgrade monitor metrics +3. No client changes are required + +### Downgrade / Rollback + +- No changes required +- If downgrading, the memory limiting behavior will be lost but no functional issues will occur + +### Pulsar Geo-Replication Upgrade & Downgrade/Rollback Considerations + +- No impact on replication protocol + +## Alternatives + +### Alternative 1: Pagination Protocol +- **Approach**: Modify protocol to support paginated topic listing +- **Rejected because**: Requires breaking protocol changes and client updates + +### Alternative 2: Response Streaming +- **Approach**: Stream topics as they're retrieved rather than buffering +- **Rejected because**: Streaming in smaller "chunks" doesn't solve the memory issue since the Pulsar client could have multiple outstanding requests. The topic list watcher is already designed to handle this scenario to reduce the load on the broker. + +### Alternative 3: Hard Memory Limits with Immediate Failure +- **Approach**: Fail requests immediately when memory threshold reached +- **Rejected because**: Client retries would add more load and wouldn't provide graceful degradation under peak load + +### Alternative 4: Extend Existing Memory Limits +- **Approach**: Include topic listing memory in `managedLedgerCacheSizeMB` or similar +- **Rejected because**: Topic listing memory has different characteristics and usage patterns, requiring separate tuning and monitoring + +## General Notes + +- Memory estimates are conservative and may overestimate actual usage to ensure safety +- The solution prioritizes memory management consistency and stability over perfect memory accuracy +- This completes Pulsar's comprehensive memory management framework by addressing the final unbounded allocation path +- Future enhancements could include: +  - Per-tenant memory limits for topic listing operations +  - Per-namespace memory limits +  - Per-connection memory limits to prevent single clients from queueing up many topic listing requests +  - Integration with overall broker memory management policies + +## Links + +* Mailing List discussion thread: https://lists.apache.org/thread/16fz747yqcr5kjkw9p5r6sc09rmcsyxr +* Mailing List voting thread: https://lists.apache.org/thread/pwptwg3h6y4nn3whmc7y172cpopd36gd \ No newline at end of file diff --git a/pip/pip-443.md b/pip/pip-443.md new file mode 100644 index 0000000000..1c90809324 --- /dev/null +++ b/pip/pip-443.md @@ -0,0 +1,237 @@ +# PIP-443: Stop using Netty Recycler in new code + +# Background knowledge + +Netty [`Recycler`](https://github.com/netty/netty/blob/netty-4.1.127.Final/common/src/main/java/io/netty/util/Recycler.java) is widely used in many places to reduce GC pressure by reusing objects. From the outputs of the following command: + +```bash +find . -name "*.java" | grep src/main | xargs grep -n "new .*Recycler" +``` + +You can find: +- 5 usages in `pulsar-common` +- 10 usages in `pulsar-broker` +- 10 usages in `pulsar-client` + +However, many of them are only the result of cargo cult programming, which blindly believes objects allocated from the `Recycler` is better than allocated from heap memory. Hence, we should understand how it works first. + +Recycler is basically a thread-local object pool that provides +- a `get` method to allocate an object from the pool or call the user-provided `newObject` method to create a new one if the pool is empty or full. +- a `recycle` method to return an object back to the pool. + +Here is a typical implementation that wraps a `Recycler` to manage a triple: + +```java +@Data // support getter and setter for all fields +public class RecyclablePair { + +    private final Recycler.Handle<RecyclablePair> handle; +    private Object x; +    private Object y; + +    private RecyclablePair(Handle<RecyclablePair> handle) { +        this.handle = handle; +    } + +    public void recycle() { +        this.x = null; +        this.y = null; +        handle.recycle(this); +    } + +    public static RecyclablePair create(Object x, Object y) { +        final RecyclablePair s = RECYCLER.get(); +        s.x = x; +        s.y = y; +        return s; +    } + +    private static final Recycler<RecyclablePair> RECYCLER = new Recycler<RecyclablePair>() { +        @Override +        protected RecyclablePair newObject(Handle<RecyclablePair> handle) { +            return new RecyclablePair(handle); +        } +    }; +} +``` + +It's typically used with a "recycle-after-use" pattern like: + +```java +var triple = RecyclablePair.create(a, b); +process(triple); +triple.recycle(); +``` + +or + +```java +var triple = RecyclablePair.create(a, b); +executor.execute(() -> { +    process(triple); +    triple.recycle(); +}); +``` + +Unlike a simple thread local pool, recycling in a different thread is supported because in this case, `recycle()` will push the object (`triple` in the code above) into a queue that implements [MessagePassingQueue](https://github.com/JCTools/JCTools/blob/v4.0.5/jctools-core/src/main/java/org/jctools/queues/MessagePassingQueue.java). + +The following code snippet of the thread local pool used by `Recycler` includes some key parts: + +```java +private static final class LocalPool<T> implements MessagePassingQueue.Consumer<DefaultHandle<T>> { +    private final ArrayDeque<DefaultHandle<T>> batch; +    private volatile Thread owner; +    private volatile MessagePassingQueue<DefaultHandle<T>> pooledHandles; +``` + +When `get` is called, it firsts get the thread local `LocalPool` and apply the following logic: +1. Drain some handles from `pooledHandles` to `batch` if `batch` is empty. +2. Poll a handle from `batch` and mark it as "claimed" (used by the current thread). +3. If `handle` is available, call the `get` method to return the object.  In the example above, `RecyclablePair#create` calls this `get` method to get a pooled object and initializes some fields. +4. Otherwise, call `newObject` to create a new object from heap memory and associated with a new handle. With the default config, the handle is a "no-op handle" in 7/8 cases that won't push the object into `pooledHandles` when `recycle` is called. In the rest 1/8 cases, the handle type is a `DefaultHandle` that is associated with the `LocalPool` in this thread and can be pushed to `pooledHandles` and `batch` + +When `recycle` is called if the handle is a `DefaultHandle`: +1. **Good Case**: if the object is allocated by `get()` in a `FastThreadLocalThread` and `recycle()` is called in the same thread, the handle is pushed to `batch` directly. +2. **Bad Case**: otherwise, the handle is pushed to `pooledHandles` queue, which is less efficient. + +# Motivation + +There are mainly three issues of using `Recycler` in Pulsar. + +## Issue 1: performance overhead + +As explained in the previous section, we can see that if the recyclable object like `RecyclablePair` is allocated not in a `FastThreadLocalThread`, it's not efficient. Here is an existing benchmark result running on a GitHub Actions Ubuntu 24.04 runner: + +``` +recycler.RecyclerBenchmark.testRecord                             thrpt   25  468034078.305 T- 1453202.080  ops/s +recycler.RecyclerBenchmark.testThreadLocalSingletonRecycler       thrpt   25  137434124.938 T-  173674.317  ops/s +recycler.RecyclerBenchmark.testThreadLocalQueueRecycler           thrpt   25   70585776.893 T-  418944.351  ops/s +recycler.RecyclerBenchmark.testRecycler                           thrpt   25   36061222.443 T-  368182.450  ops/s +``` + +- `testRecord` allocates a simple tuple `record` object from heap memory +- `testThreadLocalSingletonRecycler` allocates a tuple object from a thread local object +- `testThreadLocalQueueRecycler` allocates a recyclable tuple object from a thread local queue (`ArrayDeque`) and add it to the queue (it simulates the case that `recycle` is called in a `FastThreadLocalThread` and recycled in the same thread) +- `testRecycler` allocates a recyclable tuple object using `Recycler` and calling `recycle` then + +The full benchmark code is [here](https://github.com/BewareMyPower/JavaBenchmark/blob/v0.1.0/src/main/java/io/bewaremypower/recycler/RecyclerBenchmark.java). + +From the results above, the performance is about 7x worse than an object from heap memory in the good case and 13x worse in the bad case. + +## Issue 2: error-prone usage + +Compared to the performance overhead, the error-prone usage is a more serious issue. Users have to call `recycle()` explicitly for each object like the error-prone `malloc/free` in C language. + +```java +var triple = RecyclablePair.create(a, b); +process(triple); +triple.recycle(); +``` + +For example, if `process` in the code above throws an exception, `recycle()` will be skipped. [#13233](https://github.com/apache/pulsar/pull/13233) is a real world fix for such issue. Though the issue is not serious because the worst case is that the pool is full and then the object will be allocated from heap buffer. + +A more serious issue is that the object is shared among different threads and then `recycle()` are called twice, which will throw an exception for the 2nd recycle call. [#24697](https://github.com/apache/pulsar/issues/24697) suffers this issue and was not fixed when this proposal is written. Since the exception is thrown by `recycle()` during the read path, it affects the client side that the consumer cannot receive messages anymore. [#24725](https://github.com/apache/pulsar/pull/24725) fixes an issue that a recyclable object is unexpected shared between two threads, while if the object is not recyclable, the worst case is that the same batched message is acknowledged twice. + +The most serious issue is that if the object is shared by two threads and one thread has recycled it, the second thread could access: +- the object whose fields are reset with initial values by `recycle()` +- or the object that has been reused in another task by `get()` + +The 1st case might lead to NPE, which might be relatively fine because it's exposed in logs. The 2nd case is really dangerous because the object is in an inconsistent state and might not be exposed with any exception. + +Since the ownership of a recyclable object must be unique, the move semantics can be implemented like: + +```java +@RequiredArgsConstructor +@Getter +static class MovableRecyableObject { + +    private static final Recycler<MovableRecyableObject> RECYCLER = new Recycler<MovableRecyableObject>() { +        @Override +        protected MovableRecyableObject newObject(Handle<MovableRecyableObject> handle) { +            return new MovableRecyableObject(handle); +        } +    }; + +    private final Recycler.Handle<MovableRecyableObject> handle; +    private Object object; + +    public static MovableRecyableObject create(Object object) { +        final var result = RECYCLER.get(); +        result.object = object; +        return result; +    } + +    public MovableRecyableObject move() { +        final var result = MovableRecyableObject.create(this.object); +        this.recycle(); +        return result; +    } + +    public void recycle() { +        this.object = null; +        handle.recycle(this); +    } +} +``` + +Before publishing the recyclable object to a different thread, `move()` must be called, so that if the object is accessed by a third thread later, it will fail fast with NPE. However, this makes code harder to write that developers have to follow this pattern. Not calling `move()` won't lead to a compile-time issue or runtime issue. + +Without following the move-before-publish pattern, we have to make `recycle` synchronized, which will have more performance overhead: + +```java +    public synchronized void recycle() { +        if (this.object != null) { +            this.object = null; +            handle.recycle(this); +        } +    } +``` + +## Issue 3: questionable benefits + +How much GC pressure can be reduced by sacrificing the memory allocation latency is hard to measure and never proved by any benchmark. When `Recycler` was introduced in Netty, Java 8 was even not released. The main use case of `Recycler` in Netty is for `ByteBuf` to avoid the time-consuming buffer memory allocation from direct memory, where the buffer is used as a request for users to process in the `FastThreadLocalThread`. + +However, Java GC has involved a lot in these years. For example, Generational ZGC, which is introduced since Java 21, performs much better on tail latencies than the previous garbage collectors. Here are some blogs that verify the improvement: +- https://www.morling.dev/blog/lower-java-tail-latencies-with-zgc/ +- https://netflixtechblog.com/bending-pause-times-to-your-will-with-generational-zgc-256629c9386b + +It's questionable if `Recycler` is still useful in modern Java versions. Even if assuming a usage of `Recycler` can reduce the GC pressure significantly in a scenario, the benefit can be easily broken by a slight change of the code, which is hard to be noticed and verified. + +In Pulsar, Netty `Recycler` is typically used to wrap a list of objects to avoid passing them as method parameters, which makes methods too long with duplicated parameters. For example, [`ManagedLedgerImpl#asyncReadOpEntry`](https://github.com/apache/pulsar/blob/d833b8bef21cb9e85f0f313eb9d49c7ca550fbbd/managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/ManagedLedgerImpl.java#L2064) accepts an `OpReadEntry` instance, which is recycable and includes many fields. This method is called for each read operation, which should happen frequently, so technically it can save heap memory allocations. + +However, as the code becomes complicated, after being passed to the entry cache to process, the `OpReadEntry` is wrapped as another callback object: https://github.com/apache/pulsar/blob/d833b8bef21cb9e85f0f313eb9d49c7ca550fbbd/managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/cache/RangeEntryCacheImpl.java#L349-L365 + +Now, if the recycler design on `OpReadEntry` saved 1 of 1 heap memory allocation for each read operation, now it just saves 1 of 2 heap memory allocations. What's more, this wrapped callback is passed to `PendingReadsManager` and eventually used as a field of a new record type `ReadEntriesCallbackWithContext` to an `ArrayList` in https://github.com/apache/pulsar/blob/d833b8bef21cb9e85f0f313eb9d49c7ca550fbbd/managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/cache/PendingReadsManager.java#L256 + +Now, each read operation has: +- 1 object allocated by Netty recycler (`OpReadEntry`) +- 1 object allocated from heap memory (the wrapped callback in `RangeEntryCacheImpl`) +- 1 object allocated from heap memory (the `ReadEntriesCallbackWithContext` object that has a field of the wrapped callback) +  - This object is added to an `ArrayList` allocated from heap memory with at least 1 element + +Eventually, only saving the 1st heap memory allocation should not make a difference. Actually, many optimizations in garbage collectors can prevent short-lived objects from being allocated from heap memory. + +# Goals + +Make a rule for whether to use Netty `Recycler` to avoid debates when reviewing code like "the recycler is good that you should not remove it". + +# High Level Design + +For new contributions, don't use Netty `Recycler`. + +If you need to save heap memory usage, consider using `FastThreadLocal` to cache a single object or an object queue like What Netty `Recycler` does for the **Good Case** mentioned in the **Background knowledge** section. This would also be more effective for normal threads than Netty `Recycler`. + +For existing code, generally we should not pay efforts to remove recyclable classes. But it's acceptable to remove them especially when: +- A known recycler issue is reported but the reason is hard to analyze. (example: [#24741](https://github.com/apache/pulsar/pull/24741)) +- The existing recyclable object makes code refactoring hard + +# Alternatives + +## Add an option to allow whether to use Netty recycler + +This would be helpful to figure out if the GC pressure can be reduced once it's enabled. However, the code would be much more complicated. Additionally, measuring the GC pressure is not easy. In many dashboards, the GC time is mistakenly called as "GC pause time", where there is no actual pause. When objects aren't recycled, it's expected for some use cases that more CPU will be spent in GC. However the Netty Recycler consumes a significant share of CPU too and there's no metric, so just looking at GC metrics increasing wouldn't be a way how to see if getting rid of Netty Recyclers is useful. + +# Links + +* Mailing List discussion thread: http://lists.apache.org/thread/bvllwld260g39v5hfsyr8gl2bwb1kxqz +* Mailing List voting thread: https://lists.apache.org/thread/bxonjp38p7yzdw8ytk6bkp344nd9vw45 diff --git a/pip/pip-444.md b/pip/pip-444.md new file mode 100644 index 0000000000..74eb86ae5e --- /dev/null +++ b/pip/pip-444.md @@ -0,0 +1,55 @@ + +# PIP-444: Rate limit for deleting ledger to alleviate the zk pressure. + +# Background knowledge + +Apache Pulsar uses Apache BookKeeper as its storage layer.  +Each topic consists of multiple ledgers, which are created periodically based on the configuration. +Each ledger corresponds to a ZooKeeper (ZK) node for metadata storage. When a ledger is deleted, the corresponding ZK node will also be deleted. +Retention policies determine how long messages are retained in a topic. When the retention period is reached, the corresponding ledgers will be deleted. + +# Motivation + +When the retention of a large topic is reduced, a significant number of ledgers need to be deleted. +Since this deletion operation is not rate-limited, it could results in ZooKeeper (ZK) latency of several minutes, +which threatens the stability of the entire Pulsar cluster. + +# Goals + +Add rate limit feature for deleting ledgers. + +# Detailed Design + +Introduce a new configuration parameter: +``` +    @FieldContext( +            category = CATEGORY_STORAGE_ML, +            doc = "Max number of concurrent requests for deleting ledgers at broker level" +    ) +    private int managedLedgerDeleteMaxConcurrentRequests = 1000; +``` +These parameters can be configured in the `broker.conf` file. + +When the `managedLedgerDeleteMaxConcurrentRequests` parameter is set to a value greater than 0, +the concurrency of ledger deletions will be limited to the specified value. + +# Backward & Forward Compatibility + +Fully backward compatible and forward compatible. + +## Upgrade + +New feature is enabled by default with a rate limit of 1000 deletions per second and concurrency of 1. + +## Downgrade / Rollback + +To downgrade or rollback, simply revert the configuration changes in the `broker.conf` file and restart the broker. + + +# Links + +<!-- +Updated afterwards +--> +* Mailing List discussion thread: https://lists.apache.org/thread/ppmvm4noowlrhr229zf65mc44xqhj8k1 +* Mailing List voting thread: https://lists.apache.org/thread/2dop6kyvrpt6ztz9pvylyl8o2syvh9kw